{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVPbdOzHwFvoCM95rb5wTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henryfuentesbenito/TFM/blob/main/dNRI_particle_interaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4R7JLBmMwkh",
        "outputId": "38a32b9b-c95f-4106-f16e-0811b24114b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necesarias\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adtFpKLUVz93",
        "outputId": "dfb609af-4b41-4c7e-d101-9a71cf5e0b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clonar el repositorio\n",
        "!git clone https://github.com/cgraber/cvpr_dNRI.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aro6Vd_iV3Zs",
        "outputId": "89bec2e7-8805-4569-fa6a-4612849fdc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cvpr_dNRI'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 116 (delta 18), reused 12 (delta 12), pack-reused 90 (from 1)\u001b[K\n",
            "Receiving objects: 100% (116/116), 457.07 KiB | 9.52 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar al directorio del repositorio clonado\n",
        "%cd cvpr_dNRI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyiaeG9XWCVr",
        "outputId": "de15ed4a-77e9-4e09-bdbb-66a1eb93d251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cvpr_dNRI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar cualquier otro requerimiento si es necesario\n",
        "!pip install -e ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLGwq3y9WE0t",
        "outputId": "a304415e-168a-44fa-c98d-c665f2e136e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/cvpr_dNRI\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: dnri\n",
            "  Running setup.py develop for dnri\n",
            "Successfully installed dnri-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Definir las rutas\n",
        "DATA_PATH = '/content/drive/MyDrive/datos/'\n",
        "BASE_RESULTS_DIR = '/content/drive/MyDrive/results/'"
      ],
      "metadata": {
        "id": "iehJ2D0WWIBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED=1"
      ],
      "metadata": {
        "id": "nWe-GQR8JrUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_DIR=f\"{BASE_RESULTS_DIR}nri/seed_{SEED}/\"\n",
        "ENCODER_ARGS='--num_edge_types 2 --encoder_hidden 256 --skip_first --encoder_mlp_hidden 256 --encoder_mlp_num_layers 3'\n",
        "DECODER_ARGS=''\n",
        "MODEL_ARGS=f\"--model_type nri --graph_type static {ENCODER_ARGS} {DECODER_ARGS} --seed {SEED}\"\n",
        "TRAINING_ARGS='--no_edge_prior 0.9 --batch_size 16 --lr 5e-4 --use_adam --num_epochs 200 --lr_decay_factor 0.5 --lr_decay_steps 200 --normalize_kl --normalize_nll --tune_on_nll --val_teacher_forcing --teacher_forcing_steps -1'"
      ],
      "metadata": {
        "id": "FZUwe7w_JvuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "En4MwKKJKFsk",
        "outputId": "a65e967b-becd-4826-dbc9-9bc4d14cac5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/results/nri/seed_1/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p $WORKING_DIR"
      ],
      "metadata": {
        "id": "PMPUqLT1KQSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dnri.utils.flags import build_flags\n",
        "import dnri.models.model_builder as model_builder\n",
        "from dnri.datasets.small_synth_data import SmallSynthData\n",
        "import dnri.training.train as train\n",
        "import dnri.training.train_utils as train_utils\n",
        "import dnri.training.evaluate as evaluate\n",
        "import dnri.utils.misc as misc\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt, matplotlib.animation as animation, matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "def eval_edges(model, dataset, params):\n",
        "\n",
        "    gpu = params.get('gpu', False)\n",
        "    batch_size = params.get('batch_size', 1000)\n",
        "    eval_metric = params.get('eval_metric')\n",
        "    num_edge_types = params['num_edge_types']\n",
        "    skip_first = params['skip_first']\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, pin_memory=gpu)\n",
        "    full_edge_count = 0.\n",
        "    model.eval()\n",
        "    correct_edges = 0.\n",
        "    edge_count = 0.\n",
        "    correct_0_edges = 0.\n",
        "    edge_0_count = 0.\n",
        "    correct_1_edges = 0.\n",
        "    edge_1_count = 0.\n",
        "\n",
        "    correct = num_predicted = num_gt = 0\n",
        "    all_edges = []\n",
        "    for batch_ind, batch in enumerate(data_loader):\n",
        "        inputs = batch['inputs']\n",
        "        gt_edges = batch['edges'].long()\n",
        "        with torch.no_grad():\n",
        "            if gpu:\n",
        "                inputs = inputs.cuda(non_blocking=True)\n",
        "                gt_edges = gt_edges.cuda(non_blocking=True)\n",
        "\n",
        "            _, _, _, edges, _ = model.calculate_loss(inputs, is_train=False, return_logits=True)\n",
        "            edges = edges.argmax(dim=-1)\n",
        "            all_edges.append(edges.cpu())\n",
        "            if len(edges.shape) == 3 and len(gt_edges.shape) == 2:\n",
        "                gt_edges = gt_edges.unsqueeze(1).expand(gt_edges.size(0), edges.size(1), gt_edges.size(1))\n",
        "            elif len(gt_edges.shape) == 3 and len(edges.shape) == 2:\n",
        "                edges = edges.unsqueeze(1).expand(edges.size(0), gt_edges.size(1), edges.size(1))\n",
        "            if edges.size(1) == gt_edges.size(1) - 1:\n",
        "                gt_edges = gt_edges[:, :-1]\n",
        "            edge_count += edges.numel()\n",
        "            full_edge_count += gt_edges.numel()\n",
        "            correct_edges += ((edges == gt_edges)).sum().item()\n",
        "            edge_0_count += (gt_edges == 0).sum().item()\n",
        "            edge_1_count += (gt_edges == 1).sum().item()\n",
        "            correct_0_edges += ((edges == gt_edges)*(gt_edges == 0)).sum().item()\n",
        "            correct_1_edges += ((edges == gt_edges)*(gt_edges == 1)).sum().item()\n",
        "            correct += (edges*gt_edges).sum().item()\n",
        "            num_predicted += edges.sum().item()\n",
        "            num_gt += gt_edges.sum().item()\n",
        "    prec = correct / (num_predicted + 1e-8)\n",
        "    rec = correct / (num_gt + 1e-8)\n",
        "    f1 = 2*prec*rec / (prec+rec+1e-6)\n",
        "    all_edges = torch.cat(all_edges)\n",
        "    return f1, correct_edges / (full_edge_count + 1e-8), correct_0_edges / (edge_0_count + 1e-8), correct_1_edges / (edge_1_count + 1e-8), all_edges\n",
        "\n",
        "def plot_sample(model, dataset, num_samples, params):\n",
        "    gpu = params.get('gpu', False)\n",
        "    batch_size = params.get('batch_size', 1)\n",
        "    use_gt_edges = params.get('use_gt_edges')\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    batch_count = 0\n",
        "    all_errors = []\n",
        "    burn_in_steps = 10\n",
        "    forward_pred_steps = 40\n",
        "    for batch_ind, batch in enumerate(data_loader):\n",
        "        inputs = batch['inputs']\n",
        "        gt_edges = batch.get('edges', None)\n",
        "        with torch.no_grad():\n",
        "            model_inputs = inputs[:, :burn_in_steps]\n",
        "            gt_predictions = inputs[:, burn_in_steps:burn_in_steps+forward_pred_steps]\n",
        "            if gpu:\n",
        "                model_inputs = model_inputs.cuda(non_blocking=True)\n",
        "                if gt_edges is not None and use_gt_edges:\n",
        "                    gt_edges = gt_edges.cuda(non_blocking=True)\n",
        "            if not use_gt_edges:\n",
        "                gt_edges=None\n",
        "            model_preds = model.predict_future(model_inputs, forward_pred_steps).cpu()\n",
        "            #total_se += F.mse_loss(model_preds, gt_predictions).item()\n",
        "            print(\"MSE: \", torch.nn.functional.mse_loss(model_preds, gt_predictions).item())\n",
        "            batch_count += 1\n",
        "        fig, ax = plt.subplots()\n",
        "        unnormalized_preds = dataset.unnormalize(model_preds)\n",
        "        unnormalized_gt = dataset.unnormalize(inputs)\n",
        "        def update(frame):\n",
        "            ax.clear()\n",
        "            ax.plot(unnormalized_gt[0, frame, 0, 0], unnormalized_gt[0, frame, 0, 1], 'bo')\n",
        "            ax.plot(unnormalized_gt[0, frame, 1, 0], unnormalized_gt[0, frame, 1, 1], 'ro')\n",
        "            ax.plot(unnormalized_gt[0, frame, 2, 0], unnormalized_gt[0, frame, 2, 1], 'go')\n",
        "            if frame >= burn_in_steps:\n",
        "                tmp_fr = frame - burn_in_steps\n",
        "                ax.plot(unnormalized_preds[0, tmp_fr, 0, 0], unnormalized_preds[0, tmp_fr, 0, 1], 'bo', alpha=0.5)\n",
        "                ax.plot(unnormalized_preds[0, tmp_fr, 1, 0], unnormalized_preds[0, tmp_fr, 1, 1], 'ro', alpha=0.5)\n",
        "                ax.plot(unnormalized_preds[0, tmp_fr, 2, 0], unnormalized_preds[0, tmp_fr, 2, 1], 'go', alpha=0.5)\n",
        "            ax.set_xlim(-6, 6)\n",
        "            ax.set_ylim(-6, 6)\n",
        "        ani = animation.FuncAnimation(fig, update, interval=100, frames=burn_in_steps+forward_pred_steps)\n",
        "        path = os.path.join(params['working_dir'], 'pred_trajectory_%d.mp4'%batch_ind)\n",
        "        ani.save(path, codec='mpeg4')\n",
        "        if batch_count >= num_samples:\n",
        "            break"
      ],
      "metadata": {
        "id": "e5N-HsZyKWq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse"
      ],
      "metadata": {
        "id": "dMnrrkGkKfd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser('')\n",
        "parser.add_argument('--working_dir', required=True)\n",
        "parser.add_argument('--gpu', action='store_true')\n",
        "parser.add_argument('--seed', type=int, default=1)\n",
        "parser.add_argument('--mode', choices=['train', 'eval', 'eval_fixedwindow'], required=True)\n",
        "parser.add_argument('--load_model')\n",
        "parser.add_argument('--load_best_model', action='store_true')\n",
        "parser.add_argument('--continue_training', action='store_true')\n",
        "parser.add_argument('--model_type', choices=['nri', 'dnri', 'fc_baseline'])\n",
        "\n",
        "# Training Params\n",
        "parser.add_argument('--num_epochs', type=int)\n",
        "parser.add_argument('--lr', type=float, default=1e-3)\n",
        "parser.add_argument('--mom', type=float, default=0)\n",
        "parser.add_argument('--batch_size', type=int, default=32)\n",
        "parser.add_argument('--sub_batch_size', type=int)\n",
        "parser.add_argument('--val_batch_size', type=int)\n",
        "parser.add_argument('--val_interval', type=int, default=5)\n",
        "parser.add_argument('--test', action='store_true')\n",
        "parser.add_argument('--use_adam', action='store_true')\n",
        "parser.add_argument('--lr_decay_factor', type=float)\n",
        "parser.add_argument('--lr_decay_steps', type=int)\n",
        "parser.add_argument('--clip_grad_norm', type=float)\n",
        "parser.add_argument('--verbose', action='store_true')\n",
        "parser.add_argument('--tune_on_nll', action='store_true')\n",
        "parser.add_argument('--val_teacher_forcing', action='store_true')\n",
        "parser.add_argument('--accumulate_steps', type=int, default=1)\n",
        "parser.add_argument('--max_burn_in_count', type=int, default=-1)\n",
        "\n",
        "# Model Params\n",
        "parser.add_argument('--no_prior', action='store_true')\n",
        "parser.add_argument('--avg_prior', action='store_true')\n",
        "parser.add_argument('--add_uniform_prior', action='store_true')\n",
        "parser.add_argument('--prior_num_layers', type=int, default=1)\n",
        "parser.add_argument('--prior_hidden_size', type=int, default=256)\n",
        "parser.add_argument('--use_learned_prior', action='store_true')\n",
        "parser.add_argument('--graph_type', choices=['static', 'dynamic'])\n",
        "parser.add_argument('--avg_encoder_inputs', action='store_true')\n",
        "parser.add_argument('--use_dynamic_graph', action='store_true')\n",
        "parser.add_argument('--use_static_encoder', action='store_true')\n",
        "parser.add_argument('--decoder_type')\n",
        "parser.add_argument('--encoder_rnn_type', choices=['lstm', 'gru'], default='lstm')\n",
        "parser.add_argument('--decoder_rnn_type', choices=['lstm', 'gru'], default='gru')\n",
        "parser.add_argument('--encoder_hidden', type=int, default=256)\n",
        "parser.add_argument('--encoder_rnn_hidden', type=int)\n",
        "parser.add_argument('--num_edge_types', type=int, default=2)\n",
        "parser.add_argument('--encoder_dropout', type=float, default=0.0)\n",
        "parser.add_argument('--encoder_unidirectional', action='store_true')\n",
        "parser.add_argument('--encoder_bidirectional', action='store_true')\n",
        "parser.add_argument('--encoder_no_factor', action='store_true', default=False)\n",
        "parser.add_argument('--decoder_hidden', type=int, default=256)\n",
        "parser.add_argument('--decoder_msg_hidden', type=int, default=256)\n",
        "parser.add_argument('--decoder_dropout', type=float, default=0.0)\n",
        "parser.add_argument('--skip_first', action='store_true', default=False)\n",
        "parser.add_argument('--uniform_prior', action='store_true')\n",
        "parser.add_argument('--no_edge_prior', type=float)\n",
        "parser.add_argument('--teacher_forcing_steps', type=int, default=10)\n",
        "parser.add_argument('--gumbel_temp', type=float, default=0.5)\n",
        "parser.add_argument('--train_hard_sample', action='store_true')\n",
        "parser.add_argument('--normalize_kl', action='store_true')\n",
        "parser.add_argument('--normalize_kl_per_var', action='store_true')\n",
        "parser.add_argument('--normalize_nll', action='store_true')\n",
        "parser.add_argument('--normalize_nll_per_var', action='store_true')\n",
        "parser.add_argument('--kl_coef', type=float, default=1.)\n",
        "parser.add_argument('--no_encoder_bn', action='store_true')\n",
        "parser.add_argument('--encoder_mlp_hidden', type=int, default=256)\n",
        "parser.add_argument('--encoder_mlp_num_layers', type=int, default=1)\n",
        "parser.add_argument('--rnn_hidden', type=int, default=64)\n",
        "parser.add_argument('--teacher_forcing_prior', action='store_true')\n",
        "parser.add_argument('--decoder_rnn_hidden', type=int)\n",
        "parser.add_argument('--encoder_save_eval_memory', action='store_true')\n",
        "parser.add_argument('--encoder_normalize_mode', choices=[None, 'normalize_inp', 'normalize_all'])\n",
        "parser.add_argument('--normalize_inputs', action='store_true')\n",
        "parser.add_argument('--data_path')\n",
        "parser.add_argument('--same_data_norm', action='store_true')\n",
        "parser.add_argument('--no_data_norm', action='store_true')\n",
        "parser.add_argument('--error_out_name', default='prediction_errors_%dstep.npy')\n",
        "parser.add_argument('--prior_variance', type=float, default=5e-5)\n",
        "parser.add_argument('--test_burn_in_steps', type=int, default=10)\n",
        "parser.add_argument('--error_suffix')\n",
        "parser.add_argument('--subject_ind', type=int, default=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTietuh5KjNW",
        "outputId": "4482a233-eac6-46a4-a9e2-631f7da1bbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--subject_ind'], dest='subject_ind', nargs=None, const=None, default=-1, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args(f'--mode train --data_path {DATA_PATH} --working_dir {WORKING_DIR} {MODEL_ARGS} {TRAINING_ARGS}'.split()); args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffMsj--lKnJT",
        "outputId": "4b602bda-d11d-42b4-e1a7-172773023f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(working_dir='/content/drive/MyDrive/results/nri/seed_1/', gpu=False, seed=1, mode='train', load_model=None, load_best_model=False, continue_training=False, model_type='nri', num_epochs=200, lr=0.0005, mom=0, batch_size=16, sub_batch_size=None, val_batch_size=None, val_interval=5, test=False, use_adam=True, lr_decay_factor=0.5, lr_decay_steps=200, clip_grad_norm=None, verbose=False, tune_on_nll=True, val_teacher_forcing=True, accumulate_steps=1, max_burn_in_count=-1, no_prior=False, avg_prior=False, add_uniform_prior=False, prior_num_layers=1, prior_hidden_size=256, use_learned_prior=False, graph_type='static', avg_encoder_inputs=False, use_dynamic_graph=False, use_static_encoder=False, decoder_type=None, encoder_rnn_type='lstm', decoder_rnn_type='gru', encoder_hidden=256, encoder_rnn_hidden=None, num_edge_types=2, encoder_dropout=0.0, encoder_unidirectional=False, encoder_bidirectional=False, encoder_no_factor=False, decoder_hidden=256, decoder_msg_hidden=256, decoder_dropout=0.0, skip_first=True, uniform_prior=False, no_edge_prior=0.9, teacher_forcing_steps=-1, gumbel_temp=0.5, train_hard_sample=False, normalize_kl=True, normalize_kl_per_var=False, normalize_nll=True, normalize_nll_per_var=False, kl_coef=1.0, no_encoder_bn=False, encoder_mlp_hidden=256, encoder_mlp_num_layers=3, rnn_hidden=64, teacher_forcing_prior=False, decoder_rnn_hidden=None, encoder_save_eval_memory=False, encoder_normalize_mode=None, normalize_inputs=False, data_path='/content/drive/MyDrive/datos/', same_data_norm=False, no_data_norm=False, error_out_name='prediction_errors_%dstep.npy', prior_variance=5e-05, test_burn_in_steps=10, error_suffix=None, subject_ind=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = vars(args);params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTRAytndKqDb",
        "outputId": "8460ce92-5118-4fa7-ce10-a6524914da16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'working_dir': '/content/drive/MyDrive/results/nri/seed_1/',\n",
              " 'gpu': False,\n",
              " 'seed': 1,\n",
              " 'mode': 'train',\n",
              " 'load_model': None,\n",
              " 'load_best_model': False,\n",
              " 'continue_training': False,\n",
              " 'model_type': 'nri',\n",
              " 'num_epochs': 200,\n",
              " 'lr': 0.0005,\n",
              " 'mom': 0,\n",
              " 'batch_size': 16,\n",
              " 'sub_batch_size': None,\n",
              " 'val_batch_size': None,\n",
              " 'val_interval': 5,\n",
              " 'test': False,\n",
              " 'use_adam': True,\n",
              " 'lr_decay_factor': 0.5,\n",
              " 'lr_decay_steps': 200,\n",
              " 'clip_grad_norm': None,\n",
              " 'verbose': False,\n",
              " 'tune_on_nll': True,\n",
              " 'val_teacher_forcing': True,\n",
              " 'accumulate_steps': 1,\n",
              " 'max_burn_in_count': -1,\n",
              " 'no_prior': False,\n",
              " 'avg_prior': False,\n",
              " 'add_uniform_prior': False,\n",
              " 'prior_num_layers': 1,\n",
              " 'prior_hidden_size': 256,\n",
              " 'use_learned_prior': False,\n",
              " 'graph_type': 'static',\n",
              " 'avg_encoder_inputs': False,\n",
              " 'use_dynamic_graph': False,\n",
              " 'use_static_encoder': False,\n",
              " 'decoder_type': None,\n",
              " 'encoder_rnn_type': 'lstm',\n",
              " 'decoder_rnn_type': 'gru',\n",
              " 'encoder_hidden': 256,\n",
              " 'encoder_rnn_hidden': None,\n",
              " 'num_edge_types': 2,\n",
              " 'encoder_dropout': 0.0,\n",
              " 'encoder_unidirectional': False,\n",
              " 'encoder_bidirectional': False,\n",
              " 'encoder_no_factor': False,\n",
              " 'decoder_hidden': 256,\n",
              " 'decoder_msg_hidden': 256,\n",
              " 'decoder_dropout': 0.0,\n",
              " 'skip_first': True,\n",
              " 'uniform_prior': False,\n",
              " 'no_edge_prior': 0.9,\n",
              " 'teacher_forcing_steps': -1,\n",
              " 'gumbel_temp': 0.5,\n",
              " 'train_hard_sample': False,\n",
              " 'normalize_kl': True,\n",
              " 'normalize_kl_per_var': False,\n",
              " 'normalize_nll': True,\n",
              " 'normalize_nll_per_var': False,\n",
              " 'kl_coef': 1.0,\n",
              " 'no_encoder_bn': False,\n",
              " 'encoder_mlp_hidden': 256,\n",
              " 'encoder_mlp_num_layers': 3,\n",
              " 'rnn_hidden': 64,\n",
              " 'teacher_forcing_prior': False,\n",
              " 'decoder_rnn_hidden': None,\n",
              " 'encoder_save_eval_memory': False,\n",
              " 'encoder_normalize_mode': None,\n",
              " 'normalize_inputs': False,\n",
              " 'data_path': '/content/drive/MyDrive/datos/',\n",
              " 'same_data_norm': False,\n",
              " 'no_data_norm': False,\n",
              " 'error_out_name': 'prediction_errors_%dstep.npy',\n",
              " 'prior_variance': 5e-05,\n",
              " 'test_burn_in_steps': 10,\n",
              " 'error_suffix': None,\n",
              " 'subject_ind': -1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misc.seed(args.seed) # set seed for numpy torch cuda and random libs"
      ],
      "metadata": {
        "id": "4wnVgi7LK1C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params['num_vars'] = 3\n",
        "params['input_size'] = 4\n",
        "params['input_time_steps'] = 50\n",
        "params['nll_loss_type'] = 'gaussian'\n",
        "train_data = SmallSynthData(args.data_path, 'train', params)\n",
        "val_data   = SmallSynthData(args.data_path, 'val', params)"
      ],
      "metadata": {
        "id": "cb-DZEQcK4lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_builder.build_model(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I18V5g2pK7iw",
        "outputId": "e175bf5e-a5c3-425c-c0fa-9c69c362a69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using factor graph MLP encoder.\n",
            "ENCODER:  RefMLPEncoder(\n",
            "  (mlp1): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=200, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp2): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp3): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp4): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0, inplace=True)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0, inplace=True)\n",
            "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Using learned recurrent interaction net decoder.\n",
            "DECODER:  GraphRNNDecoder(\n",
            "  (msg_fc1): ModuleList(\n",
            "    (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n",
            "  )\n",
            "  (msg_fc2): ModuleList(\n",
            "    (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (hidden_r): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (hidden_i): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (hidden_h): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (input_r): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (input_i): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (input_n): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (out_fc1): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (out_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (out_fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "USING NO EDGE PRIOR:  tensor([[[-0.1054, -2.3026]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params['gpu']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-oDuItOK9d0",
        "outputId": "d0f9b5b3-747b-4562-aaaf-f6c34bf30676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.mode == 'train':\n",
        "    with train_utils.build_writers(args.working_dir) as (train_writer, val_writer):\n",
        "        train.train(model, train_data, val_data, params, train_writer, val_writer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64ygrYnFLBAq",
        "outputId": "3fe5c731-20bd-408d-c53f-9c8db1f4abbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1 0\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 1 EVAL: \n",
            "\tCURRENT VAL LOSS: 2.109163\n",
            "\tBEST VAL LOSS:    2.109163\n",
            "\tBEST VAL EPOCH:   1\n",
            "EPOCH 2 5.906940698623657\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 2 EVAL: \n",
            "\tCURRENT VAL LOSS: 0.809939\n",
            "\tBEST VAL LOSS:    0.809939\n",
            "\tBEST VAL EPOCH:   2\n",
            "EPOCH 3 3.8602466583251953\n",
            "EPOCH 3 EVAL: \n",
            "\tCURRENT VAL LOSS: 1.362719\n",
            "\tBEST VAL LOSS:    0.809939\n",
            "\tBEST VAL EPOCH:   2\n",
            "EPOCH 4 3.7527668476104736\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 4 EVAL: \n",
            "\tCURRENT VAL LOSS: 0.753977\n",
            "\tBEST VAL LOSS:    0.753977\n",
            "\tBEST VAL EPOCH:   4\n",
            "EPOCH 5 4.864334344863892\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 5 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.006269\n",
            "\tBEST VAL LOSS:    -0.006269\n",
            "\tBEST VAL EPOCH:   5\n",
            "EPOCH 6 4.691205739974976\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 6 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.412187\n",
            "\tBEST VAL LOSS:    -0.412187\n",
            "\tBEST VAL EPOCH:   6\n",
            "EPOCH 7 3.8450722694396973\n",
            "EPOCH 7 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.366136\n",
            "\tBEST VAL LOSS:    -0.412187\n",
            "\tBEST VAL EPOCH:   6\n",
            "EPOCH 8 3.7825326919555664\n",
            "EPOCH 8 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.380406\n",
            "\tBEST VAL LOSS:    -0.412187\n",
            "\tBEST VAL EPOCH:   6\n",
            "EPOCH 9 5.563120365142822\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 9 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.553842\n",
            "\tBEST VAL LOSS:    -0.553842\n",
            "\tBEST VAL EPOCH:   9\n",
            "EPOCH 10 4.178292274475098\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 10 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.566784\n",
            "\tBEST VAL LOSS:    -0.566784\n",
            "\tBEST VAL EPOCH:   10\n",
            "EPOCH 11 3.7469379901885986\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 11 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.690519\n",
            "\tBEST VAL LOSS:    -0.690519\n",
            "\tBEST VAL EPOCH:   11\n",
            "EPOCH 12 5.0280396938323975\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 12 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.722787\n",
            "\tBEST VAL LOSS:    -0.722787\n",
            "\tBEST VAL EPOCH:   12\n",
            "EPOCH 13 4.248025178909302\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 13 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.736986\n",
            "\tBEST VAL LOSS:    -0.736986\n",
            "\tBEST VAL EPOCH:   13\n",
            "EPOCH 14 5.351915121078491\n",
            "EPOCH 14 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.673192\n",
            "\tBEST VAL LOSS:    -0.736986\n",
            "\tBEST VAL EPOCH:   13\n",
            "EPOCH 15 6.142168283462524\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 15 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.892124\n",
            "\tBEST VAL LOSS:    -0.892124\n",
            "\tBEST VAL EPOCH:   15\n",
            "EPOCH 16 4.2889018058776855\n",
            "EPOCH 16 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.824621\n",
            "\tBEST VAL LOSS:    -0.892124\n",
            "\tBEST VAL EPOCH:   15\n",
            "EPOCH 17 3.7302000522613525\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 17 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.902756\n",
            "\tBEST VAL LOSS:    -0.902756\n",
            "\tBEST VAL EPOCH:   17\n",
            "EPOCH 18 4.114597797393799\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 18 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.919312\n",
            "\tBEST VAL LOSS:    -0.919312\n",
            "\tBEST VAL EPOCH:   18\n",
            "EPOCH 19 5.331649541854858\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 19 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.973833\n",
            "\tBEST VAL LOSS:    -0.973833\n",
            "\tBEST VAL EPOCH:   19\n",
            "EPOCH 20 3.81373929977417\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 20 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.987063\n",
            "\tBEST VAL LOSS:    -0.987063\n",
            "\tBEST VAL EPOCH:   20\n",
            "EPOCH 21 3.8121535778045654\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 21 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.989329\n",
            "\tBEST VAL LOSS:    -0.989329\n",
            "\tBEST VAL EPOCH:   21\n",
            "EPOCH 22 5.567514419555664\n",
            "EPOCH 22 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.897844\n",
            "\tBEST VAL LOSS:    -0.989329\n",
            "\tBEST VAL EPOCH:   21\n",
            "EPOCH 23 4.247218608856201\n",
            "EPOCH 23 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.811250\n",
            "\tBEST VAL LOSS:    -0.989329\n",
            "\tBEST VAL EPOCH:   21\n",
            "EPOCH 24 4.0247509479522705\n",
            "EPOCH 24 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.968128\n",
            "\tBEST VAL LOSS:    -0.989329\n",
            "\tBEST VAL EPOCH:   21\n",
            "EPOCH 25 4.89554500579834\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 25 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.992696\n",
            "\tBEST VAL LOSS:    -0.992696\n",
            "\tBEST VAL EPOCH:   25\n",
            "EPOCH 26 4.865478038787842\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 26 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.034310\n",
            "\tBEST VAL LOSS:    -1.034310\n",
            "\tBEST VAL EPOCH:   26\n",
            "EPOCH 27 3.955341100692749\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 27 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.058628\n",
            "\tBEST VAL LOSS:    -1.058628\n",
            "\tBEST VAL EPOCH:   27\n",
            "EPOCH 28 3.8435003757476807\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 28 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.119032\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 29 5.552350282669067\n",
            "EPOCH 29 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.011735\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 30 3.8123443126678467\n",
            "EPOCH 30 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.062488\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 31 3.7883715629577637\n",
            "EPOCH 31 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.091646\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 32 5.045607089996338\n",
            "EPOCH 32 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.066991\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 33 4.372751474380493\n",
            "EPOCH 33 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.103972\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 34 3.7796430587768555\n",
            "EPOCH 34 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.090839\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 35 3.879106283187866\n",
            "EPOCH 35 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.016834\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 36 5.399521112442017\n",
            "EPOCH 36 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.097417\n",
            "\tBEST VAL LOSS:    -1.119032\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 37 3.769650459289551\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 37 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.139108\n",
            "\tBEST VAL LOSS:    -1.139108\n",
            "\tBEST VAL EPOCH:   37\n",
            "EPOCH 38 3.811183452606201\n",
            "EPOCH 38 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.028748\n",
            "\tBEST VAL LOSS:    -1.139108\n",
            "\tBEST VAL EPOCH:   37\n",
            "EPOCH 39 5.078332901000977\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 39 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.226448\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 40 4.396071672439575\n",
            "EPOCH 40 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.218205\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 41 4.019618272781372\n",
            "EPOCH 41 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.200667\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 42 4.361706256866455\n",
            "EPOCH 42 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.209194\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 43 5.076996088027954\n",
            "EPOCH 43 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.021493\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 44 3.7739479541778564\n",
            "EPOCH 44 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.151515\n",
            "\tBEST VAL LOSS:    -1.226448\n",
            "\tBEST VAL EPOCH:   39\n",
            "EPOCH 45 3.7628235816955566\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 45 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.250935\n",
            "\tBEST VAL LOSS:    -1.250935\n",
            "\tBEST VAL EPOCH:   45\n",
            "EPOCH 46 5.626220464706421\n",
            "EPOCH 46 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.152475\n",
            "\tBEST VAL LOSS:    -1.250935\n",
            "\tBEST VAL EPOCH:   45\n",
            "EPOCH 47 3.819105386734009\n",
            "EPOCH 47 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.239206\n",
            "\tBEST VAL LOSS:    -1.250935\n",
            "\tBEST VAL EPOCH:   45\n",
            "EPOCH 48 3.8213610649108887\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 48 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.255223\n",
            "\tBEST VAL LOSS:    -1.255223\n",
            "\tBEST VAL EPOCH:   48\n",
            "EPOCH 49 6.650286912918091\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 49 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.259330\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 50 4.706277847290039\n",
            "EPOCH 50 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.176444\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 51 3.7867767810821533\n",
            "EPOCH 51 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.186707\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 52 3.7781670093536377\n",
            "EPOCH 52 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.223180\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 53 5.591885566711426\n",
            "EPOCH 53 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.179612\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 54 3.870363235473633\n",
            "EPOCH 54 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.200283\n",
            "\tBEST VAL LOSS:    -1.259330\n",
            "\tBEST VAL EPOCH:   49\n",
            "EPOCH 55 3.829249620437622\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 55 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.345715\n",
            "\tBEST VAL LOSS:    -1.345715\n",
            "\tBEST VAL EPOCH:   55\n",
            "EPOCH 56 5.086120128631592\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 56 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.352849\n",
            "\tBEST VAL LOSS:    -1.352849\n",
            "\tBEST VAL EPOCH:   56\n",
            "EPOCH 57 4.458436727523804\n",
            "EPOCH 57 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.310790\n",
            "\tBEST VAL LOSS:    -1.352849\n",
            "\tBEST VAL EPOCH:   56\n",
            "EPOCH 58 3.824620246887207\n",
            "EPOCH 58 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.307118\n",
            "\tBEST VAL LOSS:    -1.352849\n",
            "\tBEST VAL EPOCH:   56\n",
            "EPOCH 59 4.278106927871704\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 59 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.374550\n",
            "\tBEST VAL LOSS:    -1.374550\n",
            "\tBEST VAL EPOCH:   59\n",
            "EPOCH 60 5.192800998687744\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 60 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.383460\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 61 3.751258134841919\n",
            "EPOCH 61 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.262973\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 62 3.7543351650238037\n",
            "EPOCH 62 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.307224\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 63 5.300875663757324\n",
            "EPOCH 63 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.331622\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 64 4.040004730224609\n",
            "EPOCH 64 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.374487\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 65 3.7447452545166016\n",
            "EPOCH 65 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.290774\n",
            "\tBEST VAL LOSS:    -1.383460\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 66 4.437183141708374\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 66 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.408557\n",
            "\tBEST VAL LOSS:    -1.408557\n",
            "\tBEST VAL EPOCH:   66\n",
            "EPOCH 67 5.019940137863159\n",
            "EPOCH 67 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.323243\n",
            "\tBEST VAL LOSS:    -1.408557\n",
            "\tBEST VAL EPOCH:   66\n",
            "EPOCH 68 3.8120577335357666\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 68 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.432092\n",
            "\tBEST VAL LOSS:    -1.432092\n",
            "\tBEST VAL EPOCH:   68\n",
            "EPOCH 69 4.044956922531128\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 69 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.448175\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 70 5.663530349731445\n",
            "EPOCH 70 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.438258\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 71 3.8304035663604736\n",
            "EPOCH 71 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.441901\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 72 3.8564510345458984\n",
            "EPOCH 72 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.327879\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 73 4.928923606872559\n",
            "EPOCH 73 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.313510\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 74 4.688581943511963\n",
            "EPOCH 74 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.307691\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 75 3.800971746444702\n",
            "EPOCH 75 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.363054\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 76 3.8004050254821777\n",
            "EPOCH 76 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.365248\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 77 5.525157690048218\n",
            "EPOCH 77 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.351179\n",
            "\tBEST VAL LOSS:    -1.448175\n",
            "\tBEST VAL EPOCH:   69\n",
            "EPOCH 78 3.880117893218994\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 78 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.524187\n",
            "\tBEST VAL LOSS:    -1.524187\n",
            "\tBEST VAL EPOCH:   78\n",
            "EPOCH 79 3.8465442657470703\n",
            "EPOCH 79 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.355453\n",
            "\tBEST VAL LOSS:    -1.524187\n",
            "\tBEST VAL EPOCH:   78\n",
            "EPOCH 80 5.05846643447876\n",
            "EPOCH 80 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.509855\n",
            "\tBEST VAL LOSS:    -1.524187\n",
            "\tBEST VAL EPOCH:   78\n",
            "EPOCH 81 4.363816738128662\n",
            "EPOCH 81 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.409017\n",
            "\tBEST VAL LOSS:    -1.524187\n",
            "\tBEST VAL EPOCH:   78\n",
            "EPOCH 82 3.810154676437378\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 82 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.548442\n",
            "\tBEST VAL LOSS:    -1.548442\n",
            "\tBEST VAL EPOCH:   82\n",
            "EPOCH 83 5.945255279541016\n",
            "EPOCH 83 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.519902\n",
            "\tBEST VAL LOSS:    -1.548442\n",
            "\tBEST VAL EPOCH:   82\n",
            "EPOCH 84 5.703553199768066\n",
            "EPOCH 84 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.540517\n",
            "\tBEST VAL LOSS:    -1.548442\n",
            "\tBEST VAL EPOCH:   82\n",
            "EPOCH 85 3.889634609222412\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 85 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.578910\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 86 3.884425640106201\n",
            "EPOCH 86 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.565427\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 87 5.39919114112854\n",
            "EPOCH 87 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.556347\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 88 4.031976222991943\n",
            "EPOCH 88 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.516065\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 89 3.7895162105560303\n",
            "EPOCH 89 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.314911\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 90 4.389546155929565\n",
            "EPOCH 90 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.511758\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 91 5.107747554779053\n",
            "EPOCH 91 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.483443\n",
            "\tBEST VAL LOSS:    -1.578910\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 92 3.9443624019622803\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 92 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.611597\n",
            "\tBEST VAL LOSS:    -1.611597\n",
            "\tBEST VAL EPOCH:   92\n",
            "EPOCH 93 3.977403163909912\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 93 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.676776\n",
            "\tBEST VAL LOSS:    -1.676776\n",
            "\tBEST VAL EPOCH:   93\n",
            "EPOCH 94 5.769325256347656\n",
            "EPOCH 94 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.616467\n",
            "\tBEST VAL LOSS:    -1.676776\n",
            "\tBEST VAL EPOCH:   93\n",
            "EPOCH 95 3.8537168502807617\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 95 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.718429\n",
            "\tBEST VAL LOSS:    -1.718429\n",
            "\tBEST VAL EPOCH:   95\n",
            "EPOCH 96 3.8786911964416504\n",
            "EPOCH 96 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.590524\n",
            "\tBEST VAL LOSS:    -1.718429\n",
            "\tBEST VAL EPOCH:   95\n",
            "EPOCH 97 4.91547155380249\n",
            "EPOCH 97 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.712388\n",
            "\tBEST VAL LOSS:    -1.718429\n",
            "\tBEST VAL EPOCH:   95\n",
            "EPOCH 98 4.459673166275024\n",
            "EPOCH 98 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.488858\n",
            "\tBEST VAL LOSS:    -1.718429\n",
            "\tBEST VAL EPOCH:   95\n",
            "EPOCH 99 3.828557014465332\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 99 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.762049\n",
            "\tBEST VAL LOSS:    -1.762049\n",
            "\tBEST VAL EPOCH:   99\n",
            "EPOCH 100 4.156313419342041\n",
            "EPOCH 100 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.722379\n",
            "\tBEST VAL LOSS:    -1.762049\n",
            "\tBEST VAL EPOCH:   99\n",
            "EPOCH 101 5.14020848274231\n",
            "EPOCH 101 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.727517\n",
            "\tBEST VAL LOSS:    -1.762049\n",
            "\tBEST VAL EPOCH:   99\n",
            "EPOCH 102 3.8365824222564697\n",
            "EPOCH 102 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.676816\n",
            "\tBEST VAL LOSS:    -1.762049\n",
            "\tBEST VAL EPOCH:   99\n",
            "EPOCH 103 3.7734858989715576\n",
            "EPOCH 103 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.607049\n",
            "\tBEST VAL LOSS:    -1.762049\n",
            "\tBEST VAL EPOCH:   99\n",
            "EPOCH 104 5.572514295578003\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 104 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.862792\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 105 4.040400505065918\n",
            "EPOCH 105 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.694580\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 106 3.865286350250244\n",
            "EPOCH 106 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.777951\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 107 5.142366170883179\n",
            "EPOCH 107 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.619423\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 108 4.971002101898193\n",
            "EPOCH 108 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.727368\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 109 3.927963972091675\n",
            "EPOCH 109 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.628941\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 110 6.6674485206604\n",
            "EPOCH 110 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.710442\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 111 5.085887670516968\n",
            "EPOCH 111 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.754515\n",
            "\tBEST VAL LOSS:    -1.862792\n",
            "\tBEST VAL EPOCH:   104\n",
            "EPOCH 112 4.2536232471466064\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 112 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.965177\n",
            "\tBEST VAL LOSS:    -1.965177\n",
            "\tBEST VAL EPOCH:   112\n",
            "EPOCH 113 3.9409823417663574\n",
            "EPOCH 113 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.934221\n",
            "\tBEST VAL LOSS:    -1.965177\n",
            "\tBEST VAL EPOCH:   112\n",
            "EPOCH 114 5.7591094970703125\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 114 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.046231\n",
            "\tBEST VAL LOSS:    -2.046231\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 115 3.9392969608306885\n",
            "EPOCH 115 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.953736\n",
            "\tBEST VAL LOSS:    -2.046231\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 116 5.352305889129639\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 116 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.072985\n",
            "\tBEST VAL LOSS:    -2.072985\n",
            "\tBEST VAL EPOCH:   116\n",
            "EPOCH 117 6.141019105911255\n",
            "EPOCH 117 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.071999\n",
            "\tBEST VAL LOSS:    -2.072985\n",
            "\tBEST VAL EPOCH:   116\n",
            "EPOCH 118 3.84424090385437\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 118 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.104864\n",
            "\tBEST VAL LOSS:    -2.104864\n",
            "\tBEST VAL EPOCH:   118\n",
            "EPOCH 119 3.8587796688079834\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 119 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.121750\n",
            "\tBEST VAL LOSS:    -2.121750\n",
            "\tBEST VAL EPOCH:   119\n",
            "EPOCH 120 5.008834362030029\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 120 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.187534\n",
            "\tBEST VAL LOSS:    -2.187534\n",
            "\tBEST VAL EPOCH:   120\n",
            "EPOCH 121 4.482788801193237\n",
            "EPOCH 121 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.090777\n",
            "\tBEST VAL LOSS:    -2.187534\n",
            "\tBEST VAL EPOCH:   120\n",
            "EPOCH 122 3.8401613235473633\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 122 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.225837\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 123 4.396274566650391\n",
            "EPOCH 123 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.124221\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 124 5.181889533996582\n",
            "EPOCH 124 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.148211\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 125 3.884226083755493\n",
            "EPOCH 125 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.197517\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 126 3.9797582626342773\n",
            "EPOCH 126 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.106116\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 127 5.752371311187744\n",
            "EPOCH 127 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.142736\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 128 3.7875728607177734\n",
            "EPOCH 128 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.161519\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 129 3.7430813312530518\n",
            "EPOCH 129 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.203747\n",
            "\tBEST VAL LOSS:    -2.225837\n",
            "\tBEST VAL EPOCH:   122\n",
            "EPOCH 130 4.549472808837891\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 130 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.250311\n",
            "\tBEST VAL LOSS:    -2.250311\n",
            "\tBEST VAL EPOCH:   130\n",
            "EPOCH 131 4.9915783405303955\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 131 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.271699\n",
            "\tBEST VAL LOSS:    -2.271699\n",
            "\tBEST VAL EPOCH:   131\n",
            "EPOCH 132 3.897778034210205\n",
            "EPOCH 132 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.125689\n",
            "\tBEST VAL LOSS:    -2.271699\n",
            "\tBEST VAL EPOCH:   131\n",
            "EPOCH 133 3.843121290206909\n",
            "EPOCH 133 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.246785\n",
            "\tBEST VAL LOSS:    -2.271699\n",
            "\tBEST VAL EPOCH:   131\n",
            "EPOCH 134 5.600346088409424\n",
            "EPOCH 134 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.182323\n",
            "\tBEST VAL LOSS:    -2.271699\n",
            "\tBEST VAL EPOCH:   131\n",
            "EPOCH 135 3.8632848262786865\n",
            "EPOCH 135 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.950810\n",
            "\tBEST VAL LOSS:    -2.271699\n",
            "\tBEST VAL EPOCH:   131\n",
            "EPOCH 136 3.8841552734375\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 136 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.275569\n",
            "\tBEST VAL LOSS:    -2.275569\n",
            "\tBEST VAL EPOCH:   136\n",
            "EPOCH 137 5.126074552536011\n",
            "EPOCH 137 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.229369\n",
            "\tBEST VAL LOSS:    -2.275569\n",
            "\tBEST VAL EPOCH:   136\n",
            "EPOCH 138 4.4134931564331055\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 138 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.339538\n",
            "\tBEST VAL LOSS:    -2.339538\n",
            "\tBEST VAL EPOCH:   138\n",
            "EPOCH 139 4.018519639968872\n",
            "EPOCH 139 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.307250\n",
            "\tBEST VAL LOSS:    -2.339538\n",
            "\tBEST VAL EPOCH:   138\n",
            "EPOCH 140 4.342984199523926\n",
            "EPOCH 140 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.248669\n",
            "\tBEST VAL LOSS:    -2.339538\n",
            "\tBEST VAL EPOCH:   138\n",
            "EPOCH 141 5.26155686378479\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 141 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.378640\n",
            "\tBEST VAL LOSS:    -2.378640\n",
            "\tBEST VAL EPOCH:   141\n",
            "EPOCH 142 3.9072303771972656\n",
            "EPOCH 142 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.256908\n",
            "\tBEST VAL LOSS:    -2.378640\n",
            "\tBEST VAL EPOCH:   141\n",
            "EPOCH 143 3.8261308670043945\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 143 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.425255\n",
            "\tBEST VAL LOSS:    -2.425255\n",
            "\tBEST VAL EPOCH:   143\n",
            "EPOCH 144 5.63084602355957\n",
            "EPOCH 144 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.391274\n",
            "\tBEST VAL LOSS:    -2.425255\n",
            "\tBEST VAL EPOCH:   143\n",
            "EPOCH 145 3.8160417079925537\n",
            "EPOCH 145 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.423232\n",
            "\tBEST VAL LOSS:    -2.425255\n",
            "\tBEST VAL EPOCH:   143\n",
            "EPOCH 146 3.803872585296631\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 146 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.449218\n",
            "\tBEST VAL LOSS:    -2.449218\n",
            "\tBEST VAL EPOCH:   146\n",
            "EPOCH 147 4.790913105010986\n",
            "EPOCH 147 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.310564\n",
            "\tBEST VAL LOSS:    -2.449218\n",
            "\tBEST VAL EPOCH:   146\n",
            "EPOCH 148 4.712689399719238\n",
            "EPOCH 148 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.385044\n",
            "\tBEST VAL LOSS:    -2.449218\n",
            "\tBEST VAL EPOCH:   146\n",
            "EPOCH 149 3.9591140747070312\n",
            "EPOCH 149 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.307457\n",
            "\tBEST VAL LOSS:    -2.449218\n",
            "\tBEST VAL EPOCH:   146\n",
            "EPOCH 150 6.370012998580933\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 150 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.453416\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 151 4.939806699752808\n",
            "EPOCH 151 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.357791\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 152 3.740438938140869\n",
            "EPOCH 152 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.284093\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 153 3.808429002761841\n",
            "EPOCH 153 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.324686\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 154 5.680163860321045\n",
            "EPOCH 154 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.362895\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 155 3.7759740352630615\n",
            "EPOCH 155 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.371792\n",
            "\tBEST VAL LOSS:    -2.453416\n",
            "\tBEST VAL EPOCH:   150\n",
            "EPOCH 156 3.8258094787597656\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 156 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.458073\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 157 4.705295562744141\n",
            "EPOCH 157 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.150590\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 158 4.741230249404907\n",
            "EPOCH 158 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.439478\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 159 3.813640594482422\n",
            "EPOCH 159 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.406535\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 160 3.7541449069976807\n",
            "EPOCH 160 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.372183\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 161 5.5854878425598145\n",
            "EPOCH 161 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.337362\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 162 3.7425506114959717\n",
            "EPOCH 162 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.324968\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 163 3.7692902088165283\n",
            "EPOCH 163 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.362011\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 164 4.79823637008667\n",
            "EPOCH 164 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.450516\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 165 4.565023183822632\n",
            "EPOCH 165 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.431878\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 166 3.7627341747283936\n",
            "EPOCH 166 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.452632\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 167 3.874655246734619\n",
            "EPOCH 167 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.371488\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 168 5.573113918304443\n",
            "EPOCH 168 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.358881\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 169 3.763585329055786\n",
            "EPOCH 169 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.329352\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 170 3.878145694732666\n",
            "EPOCH 170 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.193132\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 171 4.954452276229858\n",
            "EPOCH 171 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.237831\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 172 4.436082363128662\n",
            "EPOCH 172 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.277160\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 173 3.847203016281128\n",
            "EPOCH 173 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.150285\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 174 4.04419207572937\n",
            "EPOCH 174 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.179295\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 175 5.392385482788086\n",
            "EPOCH 175 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.410984\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 176 3.845036029815674\n",
            "EPOCH 176 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.230307\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 177 3.8704679012298584\n",
            "EPOCH 177 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.231595\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 178 5.368297576904297\n",
            "EPOCH 178 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.342281\n",
            "\tBEST VAL LOSS:    -2.458073\n",
            "\tBEST VAL EPOCH:   156\n",
            "EPOCH 179 4.051515102386475\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 179 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.484159\n",
            "\tBEST VAL LOSS:    -2.484159\n",
            "\tBEST VAL EPOCH:   179\n",
            "EPOCH 180 3.895684003829956\n",
            "EPOCH 180 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.455598\n",
            "\tBEST VAL LOSS:    -2.484159\n",
            "\tBEST VAL EPOCH:   179\n",
            "EPOCH 181 4.45640230178833\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 181 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.504594\n",
            "\tBEST VAL LOSS:    -2.504594\n",
            "\tBEST VAL EPOCH:   181\n",
            "EPOCH 182 5.086893081665039\n",
            "EPOCH 182 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.408567\n",
            "\tBEST VAL LOSS:    -2.504594\n",
            "\tBEST VAL EPOCH:   181\n",
            "EPOCH 183 3.9088780879974365\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 183 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.545750\n",
            "\tBEST VAL LOSS:    -2.545750\n",
            "\tBEST VAL EPOCH:   183\n",
            "EPOCH 184 5.968158483505249\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 184 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.591308\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 185 5.579296588897705\n",
            "EPOCH 185 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.463900\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 186 3.85575795173645\n",
            "EPOCH 186 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.504639\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 187 10.183049201965332\n",
            "EPOCH 187 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.496096\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 188 4.7811315059661865\n",
            "EPOCH 188 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.392907\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 189 3.852640390396118\n",
            "EPOCH 189 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.379023\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 190 3.946661949157715\n",
            "EPOCH 190 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.349631\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 191 5.490296840667725\n",
            "EPOCH 191 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.321392\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 192 3.871340751647949\n",
            "EPOCH 192 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.540409\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 193 3.8590829372406006\n",
            "EPOCH 193 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.504068\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 194 5.284724950790405\n",
            "EPOCH 194 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.452758\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 195 4.145752668380737\n",
            "EPOCH 195 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.222949\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 196 3.83697772026062\n",
            "EPOCH 196 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.951707\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 197 5.2550413608551025\n",
            "EPOCH 197 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.033344\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 198 17.642892599105835\n",
            "EPOCH 198 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.059094\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 199 20.584441423416138\n",
            "EPOCH 199 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.400366\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n",
            "EPOCH 200 22.86868381500244\n",
            "EPOCH 200 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.268720\n",
            "\tBEST VAL LOSS:    -2.591308\n",
            "\tBEST VAL EPOCH:   184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if args.mode == 'eval':\n",
        "test_data = SmallSynthData(args.data_path, 'test', params)\n",
        "forward_pred = 50 - args.test_burn_in_steps\n",
        "test_mse  = evaluate.eval_forward_prediction(model, test_data, args.test_burn_in_steps, forward_pred, params)\n",
        "path = os.path.join(args.working_dir, args.error_out_name%args.test_burn_in_steps)\n",
        "np.save(path, test_mse.cpu().numpy())\n",
        "test_mse_1  = test_mse[0].item()\n",
        "test_mse_15 = test_mse[14].item()\n",
        "test_mse_25 = test_mse[24].item()\n",
        "print(\"FORWARD PRED RESULTS:\")\n",
        "print(\"\\t1 STEP: \", test_mse_1)\n",
        "print(\"\\t15 STEP: \",test_mse_15)\n",
        "print(\"\\t25 STEP: \",test_mse_25)\n",
        "\n",
        "\n",
        "f1, all_acc, acc_0, acc_1, edges = eval_edges(model, val_data, params)\n",
        "print(\"Val Edge results:\")\n",
        "print(\"\\tF1: \",f1)\n",
        "print(\"\\tAll predicted edge accuracy: \",all_acc)\n",
        "print(\"\\tFirst Edge Acc: \",acc_0)\n",
        "print(\"\\tSecond Edge Acc: \",acc_1)\n",
        "out_dir = os.path.join(args.working_dir, 'preds')\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "out_path = os.path.join(out_dir, 'encoder_edges.npy')\n",
        "np.save(out_path, edges.numpy())\n",
        "\n",
        "plot_sample(model, test_data, args.test_burn_in_steps, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtC4OX3_PMky",
        "outputId": "38ea3721-c4e6-496b-f60b-4be53fbde455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FORWARD PRED RESULTS:\n",
            "\t1 STEP:  0.00010658917017281055\n",
            "\t15 STEP:  0.02248053066432476\n",
            "\t25 STEP:  0.03390005975961685\n",
            "Val Edge results:\n",
            "\tF1:  0.28491291520204165\n",
            "\tAll predicted edge accuracy:  0.9064666666663644\n",
            "\tFirst Edge Acc:  0.9272088003895679\n",
            "\tSecond Edge Acc:  0.4387755102006376\n",
            "MSE:  0.025221118703484535\n",
            "MSE:  0.03121432289481163\n",
            "MSE:  0.026759441941976547\n",
            "MSE:  0.05606646463274956\n",
            "MSE:  0.031724411994218826\n",
            "MSE:  0.025016406551003456\n",
            "MSE:  0.029462609440088272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! --test_burn_in_steps 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSEVCZwoQBKZ",
        "outputId": "bdf04f2d-751d-4b2e-dfb6-3c31106a4348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: --test_burn_in_steps: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args(f'--mode eval --test_burn_in_steps 25 --load_best_model --data_path {DATA_PATH} --working_dir {WORKING_DIR} {MODEL_ARGS} {TRAINING_ARGS}'.split()); args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98w3AVwlQMw7",
        "outputId": "f05d2751-92e9-4838-8b79-3ff7b3a2b10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(working_dir='/content/drive/MyDrive/results/nri/seed_1/', gpu=False, seed=1, mode='eval', load_model=None, load_best_model=True, continue_training=False, model_type='nri', num_epochs=200, lr=0.0005, mom=0, batch_size=16, sub_batch_size=None, val_batch_size=None, val_interval=5, test=False, use_adam=True, lr_decay_factor=0.5, lr_decay_steps=200, clip_grad_norm=None, verbose=False, tune_on_nll=True, val_teacher_forcing=True, accumulate_steps=1, max_burn_in_count=-1, no_prior=False, avg_prior=False, add_uniform_prior=False, prior_num_layers=1, prior_hidden_size=256, use_learned_prior=False, graph_type='static', avg_encoder_inputs=False, use_dynamic_graph=False, use_static_encoder=False, decoder_type=None, encoder_rnn_type='lstm', decoder_rnn_type='gru', encoder_hidden=256, encoder_rnn_hidden=None, num_edge_types=2, encoder_dropout=0.0, encoder_unidirectional=False, encoder_bidirectional=False, encoder_no_factor=False, decoder_hidden=256, decoder_msg_hidden=256, decoder_dropout=0.0, skip_first=True, uniform_prior=False, no_edge_prior=0.9, teacher_forcing_steps=-1, gumbel_temp=0.5, train_hard_sample=False, normalize_kl=True, normalize_kl_per_var=False, normalize_nll=True, normalize_nll_per_var=False, kl_coef=1.0, no_encoder_bn=False, encoder_mlp_hidden=256, encoder_mlp_num_layers=3, rnn_hidden=64, teacher_forcing_prior=False, decoder_rnn_hidden=None, encoder_save_eval_memory=False, encoder_normalize_mode=None, normalize_inputs=False, data_path='/content/drive/MyDrive/datos/', same_data_norm=False, no_data_norm=False, error_out_name='prediction_errors_%dstep.npy', prior_variance=5e-05, test_burn_in_steps=25, error_suffix=None, subject_ind=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = vars(args);params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5LIniXDQN6h",
        "outputId": "12d1f0b4-3eb5-4960-a9e3-5ee254e8086d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'working_dir': '/content/drive/MyDrive/results/nri/seed_1/',\n",
              " 'gpu': False,\n",
              " 'seed': 1,\n",
              " 'mode': 'eval',\n",
              " 'load_model': None,\n",
              " 'load_best_model': True,\n",
              " 'continue_training': False,\n",
              " 'model_type': 'nri',\n",
              " 'num_epochs': 200,\n",
              " 'lr': 0.0005,\n",
              " 'mom': 0,\n",
              " 'batch_size': 16,\n",
              " 'sub_batch_size': None,\n",
              " 'val_batch_size': None,\n",
              " 'val_interval': 5,\n",
              " 'test': False,\n",
              " 'use_adam': True,\n",
              " 'lr_decay_factor': 0.5,\n",
              " 'lr_decay_steps': 200,\n",
              " 'clip_grad_norm': None,\n",
              " 'verbose': False,\n",
              " 'tune_on_nll': True,\n",
              " 'val_teacher_forcing': True,\n",
              " 'accumulate_steps': 1,\n",
              " 'max_burn_in_count': -1,\n",
              " 'no_prior': False,\n",
              " 'avg_prior': False,\n",
              " 'add_uniform_prior': False,\n",
              " 'prior_num_layers': 1,\n",
              " 'prior_hidden_size': 256,\n",
              " 'use_learned_prior': False,\n",
              " 'graph_type': 'static',\n",
              " 'avg_encoder_inputs': False,\n",
              " 'use_dynamic_graph': False,\n",
              " 'use_static_encoder': False,\n",
              " 'decoder_type': None,\n",
              " 'encoder_rnn_type': 'lstm',\n",
              " 'decoder_rnn_type': 'gru',\n",
              " 'encoder_hidden': 256,\n",
              " 'encoder_rnn_hidden': None,\n",
              " 'num_edge_types': 2,\n",
              " 'encoder_dropout': 0.0,\n",
              " 'encoder_unidirectional': False,\n",
              " 'encoder_bidirectional': False,\n",
              " 'encoder_no_factor': False,\n",
              " 'decoder_hidden': 256,\n",
              " 'decoder_msg_hidden': 256,\n",
              " 'decoder_dropout': 0.0,\n",
              " 'skip_first': True,\n",
              " 'uniform_prior': False,\n",
              " 'no_edge_prior': 0.9,\n",
              " 'teacher_forcing_steps': -1,\n",
              " 'gumbel_temp': 0.5,\n",
              " 'train_hard_sample': False,\n",
              " 'normalize_kl': True,\n",
              " 'normalize_kl_per_var': False,\n",
              " 'normalize_nll': True,\n",
              " 'normalize_nll_per_var': False,\n",
              " 'kl_coef': 1.0,\n",
              " 'no_encoder_bn': False,\n",
              " 'encoder_mlp_hidden': 256,\n",
              " 'encoder_mlp_num_layers': 3,\n",
              " 'rnn_hidden': 64,\n",
              " 'teacher_forcing_prior': False,\n",
              " 'decoder_rnn_hidden': None,\n",
              " 'encoder_save_eval_memory': False,\n",
              " 'encoder_normalize_mode': None,\n",
              " 'normalize_inputs': False,\n",
              " 'data_path': '/content/drive/MyDrive/datos/',\n",
              " 'same_data_norm': False,\n",
              " 'no_data_norm': False,\n",
              " 'error_out_name': 'prediction_errors_%dstep.npy',\n",
              " 'prior_variance': 5e-05,\n",
              " 'test_burn_in_steps': 25,\n",
              " 'error_suffix': None,\n",
              " 'subject_ind': -1}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misc.seed(args.seed) # set seed for numpy torch cuda and random libs"
      ],
      "metadata": {
        "id": "oPFS34pcQUGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params['num_vars'] = 3\n",
        "params['input_size'] = 4\n",
        "params['input_time_steps'] = 50\n",
        "params['nll_loss_type'] = 'gaussian'\n",
        "train_data = SmallSynthData(args.data_path, 'train', params)\n",
        "val_data   = SmallSynthData(args.data_path, 'val', params)"
      ],
      "metadata": {
        "id": "EUNb57HYQWtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_builder.build_model(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQYmn_2bQYvj",
        "outputId": "aec89366-c6bb-4379-ed6d-a53cf8b69c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using factor graph MLP encoder.\n",
            "ENCODER:  RefMLPEncoder(\n",
            "  (mlp1): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=200, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp2): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp3): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (mlp4): RefNRIMLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Dropout(p=0.0, inplace=False)\n",
            "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "      (4): ELU(alpha=1.0, inplace=True)\n",
            "    )\n",
            "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ELU(alpha=1.0, inplace=True)\n",
            "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (3): ELU(alpha=1.0, inplace=True)\n",
            "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Using learned recurrent interaction net decoder.\n",
            "DECODER:  GraphRNNDecoder(\n",
            "  (msg_fc1): ModuleList(\n",
            "    (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n",
            "  )\n",
            "  (msg_fc2): ModuleList(\n",
            "    (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (hidden_r): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (hidden_i): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (hidden_h): Linear(in_features=256, out_features=256, bias=False)\n",
            "  (input_r): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (input_i): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (input_n): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (out_fc1): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (out_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (out_fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n",
            "USING NO EDGE PRIOR:  tensor([[[-0.1054, -2.3026]]])\n",
            "LOADING BEST MODEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.mode == 'eval':\n",
        "    test_data = SmallSynthData(args.data_path, 'test', params)\n",
        "    forward_pred = 50 - args.test_burn_in_steps\n",
        "    test_mse  = evaluate.eval_forward_prediction(model, test_data, args.test_burn_in_steps, forward_pred, params)\n",
        "    path = os.path.join(args.working_dir, args.error_out_name%args.test_burn_in_steps)\n",
        "    np.save(path, test_mse.cpu().numpy())\n",
        "    test_mse_1  = test_mse[0].item()\n",
        "    test_mse_15 = test_mse[14].item()\n",
        "    test_mse_25 = test_mse[24].item()\n",
        "    print(\"FORWARD PRED RESULTS:\")\n",
        "    print(\"\\t1 STEP: \", test_mse_1)\n",
        "    print(\"\\t15 STEP: \",test_mse_15)\n",
        "    print(\"\\t25 STEP: \",test_mse_25)\n",
        "\n",
        "\n",
        "    f1, all_acc, acc_0, acc_1, edges = eval_edges(model, val_data, params)\n",
        "    print(\"Val Edge results:\")\n",
        "    print(\"\\tF1: \",f1)\n",
        "    print(\"\\tAll predicted edge accuracy: \",all_acc)\n",
        "    print(\"\\tFirst Edge Acc: \",acc_0)\n",
        "    print(\"\\tSecond Edge Acc: \",acc_1)\n",
        "    out_dir = os.path.join(args.working_dir, 'preds_eval_best_25')\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    out_path = os.path.join(out_dir, 'encoder_edges.npy')\n",
        "    np.save(out_path, edges.numpy())\n",
        "\n",
        "    plot_sample(model, test_data, args.test_burn_in_steps, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUb_BJx3Qd07",
        "outputId": "cc27bf59-1ff5-4de2-f426-a29a1aac32c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FORWARD PRED RESULTS:\n",
            "\t1 STEP:  2.123346166627016e-05\n",
            "\t15 STEP:  0.0019051118288189173\n",
            "\t25 STEP:  0.004750142805278301\n",
            "Val Edge results:\n",
            "\tF1:  0.26911271132856346\n",
            "\tAll predicted edge accuracy:  0.9043999999996986\n",
            "\tFirst Edge Acc:  0.9261296386545547\n",
            "\tSecond Edge Acc:  0.4144427001537328\n",
            "MSE:  0.01916656084358692\n",
            "MSE:  0.024596022441983223\n",
            "MSE:  0.016913970932364464\n",
            "MSE:  0.030282994732260704\n",
            "MSE:  0.021424787119030952\n",
            "MSE:  0.016158444806933403\n",
            "MSE:  0.03058360330760479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.mode == 'record_predictions':\n",
        "    model.eval()\n",
        "    burn_in = args.test_burn_in_steps\n",
        "    forward_pred = 50 - args.test_burn_in_steps\n",
        "    test_data = SmallSynthData(args.data_path, 'test', params)\n",
        "    if args.subject_ind == -1:\n",
        "        val_data_loader = DataLoader(test_data, batch_size=params['batch_size'])\n",
        "        all_predictions = []\n",
        "        all_edges = []\n",
        "        for batch_ind,batch in enumerate(val_data_loader):\n",
        "            print(\"BATCH %d of %d\"%(batch_ind+1, len(val_data_loader)))\n",
        "            inputs = batch['inputs']\n",
        "            if args.gpu:\n",
        "                inputs = inputs.cuda(non_blocking=True)\n",
        "            with torch.no_grad():\n",
        "                predictions, edges = model.predict_future(inputs[:, :burn_in], forward_pred, return_edges=True, return_everything=True)\n",
        "                all_predictions.append(predictions)\n",
        "                all_edges.append(edges)\n",
        "        if args.error_suffix is not None:\n",
        "            out_path = os.path.join(args.working_dir, 'preds/', 'all_test_subjects_%s.npy'%args.error_suffix)\n",
        "        else:\n",
        "            out_path = os.path.join(args.working_dir, 'preds/', 'all_test_subjects.npy')\n",
        "\n",
        "        predictions = torch.cat(all_predictions, dim=0)\n",
        "        edges = torch.cat(all_edges, dim=0)\n",
        "\n",
        "    else:\n",
        "        data = test_data[args.subject_ind]\n",
        "        inputs = data['inputs'].unsqueeze(0)\n",
        "        if args.gpu:\n",
        "            inputs = inputs.cuda(non_blocking=True)\n",
        "        with torch.no_grad():\n",
        "            predictions, edges = model.predict_future(inputs[:, :burn_in], forward_pred, return_edges=True, return_everything=True)\n",
        "            predictions = predictions.squeeze(0)\n",
        "            edges = edges.squeeze(0)\n",
        "        out_path = os.path.join(args.working_dir, 'preds/', 'subject_%d.npy'%args.subject_ind)\n",
        "    tmp_dir = os.path.join(args.working_dir, 'preds/')\n",
        "    if not os.path.exists(tmp_dir):\n",
        "        os.makedirs(tmp_dir)\n",
        "    torch.save([predictions.cpu(), edges.cpu()], out_path)"
      ],
      "metadata": {
        "id": "LUrAuHb2Qkp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DNRI**\n"
      ],
      "metadata": {
        "id": "4djeovdRQlfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_DIR=f\"{BASE_RESULTS_DIR}dnri/seed_{SEED}/\"\n",
        "ENCODER_ARGS=\"--encoder_hidden 256 --encoder_mlp_num_layers 3 --encoder_mlp_hidden 128 --encoder_rnn_hidden 64\"\n",
        "DECODER_ARGS=\"--decoder_hidden 256 --decoder_type ref_mlp\"\n",
        "HIDDEN_ARGS=\"--rnn_hidden 64\"\n",
        "PRIOR_ARGS=\"--use_learned_prior --prior_num_layers 3 --prior_hidden_size 128\"\n",
        "MODEL_ARGS=f\"--model_type dnri --graph_type dynamic --skip_first --num_edge_types 2 {ENCODER_ARGS} {DECODER_ARGS} {HIDDEN_ARGS} {PRIOR_ARGS} --seed {SEED}\"\n",
        "TRAINING_ARGS='--add_uniform_prior --no_edge_prior 0.9 --batch_size 16 --lr 5e-4 --use_adam --num_epochs 200 --lr_decay_factor 0.5 --lr_decay_steps 200 --normalize_kl --normalize_nll --tune_on_nll --val_teacher_forcing --teacher_forcing_steps -1'"
      ],
      "metadata": {
        "id": "1I99u5UVQz5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args(f'--mode train --data_path {DATA_PATH} --working_dir {WORKING_DIR} {MODEL_ARGS} {TRAINING_ARGS}'.split()); args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo4toTBiQ3P_",
        "outputId": "369c8020-2733-4af4-8153-3ac575ba3f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(working_dir='/content/drive/MyDrive/results/dnri/seed_1/', gpu=False, seed=1, mode='train', load_model=None, load_best_model=False, continue_training=False, model_type='dnri', num_epochs=200, lr=0.0005, mom=0, batch_size=16, sub_batch_size=None, val_batch_size=None, val_interval=5, test=False, use_adam=True, lr_decay_factor=0.5, lr_decay_steps=200, clip_grad_norm=None, verbose=False, tune_on_nll=True, val_teacher_forcing=True, accumulate_steps=1, max_burn_in_count=-1, no_prior=False, avg_prior=False, add_uniform_prior=True, prior_num_layers=3, prior_hidden_size=128, use_learned_prior=True, graph_type='dynamic', avg_encoder_inputs=False, use_dynamic_graph=False, use_static_encoder=False, decoder_type='ref_mlp', encoder_rnn_type='lstm', decoder_rnn_type='gru', encoder_hidden=256, encoder_rnn_hidden=64, num_edge_types=2, encoder_dropout=0.0, encoder_unidirectional=False, encoder_bidirectional=False, encoder_no_factor=False, decoder_hidden=256, decoder_msg_hidden=256, decoder_dropout=0.0, skip_first=True, uniform_prior=False, no_edge_prior=0.9, teacher_forcing_steps=-1, gumbel_temp=0.5, train_hard_sample=False, normalize_kl=True, normalize_kl_per_var=False, normalize_nll=True, normalize_nll_per_var=False, kl_coef=1.0, no_encoder_bn=False, encoder_mlp_hidden=128, encoder_mlp_num_layers=3, rnn_hidden=64, teacher_forcing_prior=False, decoder_rnn_hidden=None, encoder_save_eval_memory=False, encoder_normalize_mode=None, normalize_inputs=False, data_path='/content/drive/MyDrive/datos/', same_data_norm=False, no_data_norm=False, error_out_name='prediction_errors_%dstep.npy', prior_variance=5e-05, test_burn_in_steps=10, error_suffix=None, subject_ind=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = vars(args);params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBa4JLSoQ6Eh",
        "outputId": "32dc3a98-19b6-4302-ebd2-f6b87aaf6598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'working_dir': '/content/drive/MyDrive/results/dnri/seed_1/',\n",
              " 'gpu': False,\n",
              " 'seed': 1,\n",
              " 'mode': 'train',\n",
              " 'load_model': None,\n",
              " 'load_best_model': False,\n",
              " 'continue_training': False,\n",
              " 'model_type': 'dnri',\n",
              " 'num_epochs': 200,\n",
              " 'lr': 0.0005,\n",
              " 'mom': 0,\n",
              " 'batch_size': 16,\n",
              " 'sub_batch_size': None,\n",
              " 'val_batch_size': None,\n",
              " 'val_interval': 5,\n",
              " 'test': False,\n",
              " 'use_adam': True,\n",
              " 'lr_decay_factor': 0.5,\n",
              " 'lr_decay_steps': 200,\n",
              " 'clip_grad_norm': None,\n",
              " 'verbose': False,\n",
              " 'tune_on_nll': True,\n",
              " 'val_teacher_forcing': True,\n",
              " 'accumulate_steps': 1,\n",
              " 'max_burn_in_count': -1,\n",
              " 'no_prior': False,\n",
              " 'avg_prior': False,\n",
              " 'add_uniform_prior': True,\n",
              " 'prior_num_layers': 3,\n",
              " 'prior_hidden_size': 128,\n",
              " 'use_learned_prior': True,\n",
              " 'graph_type': 'dynamic',\n",
              " 'avg_encoder_inputs': False,\n",
              " 'use_dynamic_graph': False,\n",
              " 'use_static_encoder': False,\n",
              " 'decoder_type': 'ref_mlp',\n",
              " 'encoder_rnn_type': 'lstm',\n",
              " 'decoder_rnn_type': 'gru',\n",
              " 'encoder_hidden': 256,\n",
              " 'encoder_rnn_hidden': 64,\n",
              " 'num_edge_types': 2,\n",
              " 'encoder_dropout': 0.0,\n",
              " 'encoder_unidirectional': False,\n",
              " 'encoder_bidirectional': False,\n",
              " 'encoder_no_factor': False,\n",
              " 'decoder_hidden': 256,\n",
              " 'decoder_msg_hidden': 256,\n",
              " 'decoder_dropout': 0.0,\n",
              " 'skip_first': True,\n",
              " 'uniform_prior': False,\n",
              " 'no_edge_prior': 0.9,\n",
              " 'teacher_forcing_steps': -1,\n",
              " 'gumbel_temp': 0.5,\n",
              " 'train_hard_sample': False,\n",
              " 'normalize_kl': True,\n",
              " 'normalize_kl_per_var': False,\n",
              " 'normalize_nll': True,\n",
              " 'normalize_nll_per_var': False,\n",
              " 'kl_coef': 1.0,\n",
              " 'no_encoder_bn': False,\n",
              " 'encoder_mlp_hidden': 128,\n",
              " 'encoder_mlp_num_layers': 3,\n",
              " 'rnn_hidden': 64,\n",
              " 'teacher_forcing_prior': False,\n",
              " 'decoder_rnn_hidden': None,\n",
              " 'encoder_save_eval_memory': False,\n",
              " 'encoder_normalize_mode': None,\n",
              " 'normalize_inputs': False,\n",
              " 'data_path': '/content/drive/MyDrive/datos/',\n",
              " 'same_data_norm': False,\n",
              " 'no_data_norm': False,\n",
              " 'error_out_name': 'prediction_errors_%dstep.npy',\n",
              " 'prior_variance': 5e-05,\n",
              " 'test_burn_in_steps': 10,\n",
              " 'error_suffix': None,\n",
              " 'subject_ind': -1}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misc.seed(args.seed) # set seed for numpy torch cuda and random libs"
      ],
      "metadata": {
        "id": "8wJBk-9yQ9DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params['num_vars'] = 3\n",
        "params['input_size'] = 4\n",
        "params['input_time_steps'] = 50\n",
        "params['nll_loss_type'] = 'gaussian'\n",
        "train_data = SmallSynthData(args.data_path, 'train', params)\n",
        "val_data   = SmallSynthData(args.data_path, 'val', params)"
      ],
      "metadata": {
        "id": "3dt2LEErQ__A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_builder.build_model(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5apZWR-RApq",
        "outputId": "dfa00dcf-51fc-410f-8909-689363e8fd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using learned interaction net decoder.\n",
            "USING NO EDGE PRIOR:  tensor([[[-0.1054, -2.3026]]])\n",
            "dNRI MODEL:  DNRI(\n",
            "  (encoder): DNRI_Encoder(\n",
            "    (mlp1): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp2): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp3): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp4): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (forward_rnn): LSTM(256, 64, batch_first=True)\n",
            "    (reverse_rnn): LSTM(256, 64, batch_first=True)\n",
            "    (encoder_fc_out): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): ELU(alpha=1.0, inplace=True)\n",
            "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
            "    )\n",
            "    (prior_fc_out): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): ELU(alpha=1.0, inplace=True)\n",
            "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): DNRI_MLP_Decoder(\n",
            "    (msg_fc1): ModuleList(\n",
            "      (0-1): 2 x Linear(in_features=8, out_features=256, bias=True)\n",
            "    )\n",
            "    (msg_fc2): ModuleList(\n",
            "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (out_fc1): Linear(in_features=260, out_features=256, bias=True)\n",
            "    (out_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (out_fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.mode == 'train':\n",
        "    with train_utils.build_writers(args.working_dir) as (train_writer, val_writer):\n",
        "        train.train(model, train_data, val_data, params, train_writer, val_writer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnO37YLiRGtg",
        "outputId": "45f5660c-5857-4a75-dce3-406fd02e165a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1 0\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 1 EVAL: \n",
            "\tCURRENT VAL LOSS: 1.383601\n",
            "\tBEST VAL LOSS:    1.383601\n",
            "\tBEST VAL EPOCH:   1\n",
            "EPOCH 2 8.701972961425781\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 2 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.283971\n",
            "\tBEST VAL LOSS:    -0.283971\n",
            "\tBEST VAL EPOCH:   2\n",
            "EPOCH 3 6.496497631072998\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 3 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.305005\n",
            "\tBEST VAL LOSS:    -0.305005\n",
            "\tBEST VAL EPOCH:   3\n",
            "EPOCH 4 8.156956672668457\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 4 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.313773\n",
            "\tBEST VAL LOSS:    -0.313773\n",
            "\tBEST VAL EPOCH:   4\n",
            "EPOCH 5 6.677062749862671\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 5 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.585780\n",
            "\tBEST VAL LOSS:    -0.585780\n",
            "\tBEST VAL EPOCH:   5\n",
            "EPOCH 6 8.408665418624878\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 6 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.700710\n",
            "\tBEST VAL LOSS:    -0.700710\n",
            "\tBEST VAL EPOCH:   6\n",
            "EPOCH 7 6.600669622421265\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 7 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.717699\n",
            "\tBEST VAL LOSS:    -0.717699\n",
            "\tBEST VAL EPOCH:   7\n",
            "EPOCH 8 8.413633346557617\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 8 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.824970\n",
            "\tBEST VAL LOSS:    -0.824970\n",
            "\tBEST VAL EPOCH:   8\n",
            "EPOCH 9 6.508795738220215\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 9 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.882275\n",
            "\tBEST VAL LOSS:    -0.882275\n",
            "\tBEST VAL EPOCH:   9\n",
            "EPOCH 10 8.328554391860962\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 10 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.892692\n",
            "\tBEST VAL LOSS:    -0.892692\n",
            "\tBEST VAL EPOCH:   10\n",
            "EPOCH 11 6.700505495071411\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 11 EVAL: \n",
            "\tCURRENT VAL LOSS: -0.981177\n",
            "\tBEST VAL LOSS:    -0.981177\n",
            "\tBEST VAL EPOCH:   11\n",
            "EPOCH 12 8.369615077972412\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 12 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.051706\n",
            "\tBEST VAL LOSS:    -1.051706\n",
            "\tBEST VAL EPOCH:   12\n",
            "EPOCH 13 6.637763500213623\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 13 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.093206\n",
            "\tBEST VAL LOSS:    -1.093206\n",
            "\tBEST VAL EPOCH:   13\n",
            "EPOCH 14 8.374591588973999\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 14 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.217011\n",
            "\tBEST VAL LOSS:    -1.217011\n",
            "\tBEST VAL EPOCH:   14\n",
            "EPOCH 15 6.61391019821167\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 15 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.282657\n",
            "\tBEST VAL LOSS:    -1.282657\n",
            "\tBEST VAL EPOCH:   15\n",
            "EPOCH 16 8.374554872512817\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 16 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.408506\n",
            "\tBEST VAL LOSS:    -1.408506\n",
            "\tBEST VAL EPOCH:   16\n",
            "EPOCH 17 6.658583164215088\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 17 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.419746\n",
            "\tBEST VAL LOSS:    -1.419746\n",
            "\tBEST VAL EPOCH:   17\n",
            "EPOCH 18 8.247309684753418\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 18 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.502084\n",
            "\tBEST VAL LOSS:    -1.502084\n",
            "\tBEST VAL EPOCH:   18\n",
            "EPOCH 19 6.5860276222229\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 19 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.611015\n",
            "\tBEST VAL LOSS:    -1.611015\n",
            "\tBEST VAL EPOCH:   19\n",
            "EPOCH 20 10.285687446594238\n",
            "EPOCH 20 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.556945\n",
            "\tBEST VAL LOSS:    -1.611015\n",
            "\tBEST VAL EPOCH:   19\n",
            "EPOCH 21 6.585794925689697\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 21 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.675194\n",
            "\tBEST VAL LOSS:    -1.675194\n",
            "\tBEST VAL EPOCH:   21\n",
            "EPOCH 22 8.479460000991821\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 22 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.847799\n",
            "\tBEST VAL LOSS:    -1.847799\n",
            "\tBEST VAL EPOCH:   22\n",
            "EPOCH 23 6.631011247634888\n",
            "EPOCH 23 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.761182\n",
            "\tBEST VAL LOSS:    -1.847799\n",
            "\tBEST VAL EPOCH:   22\n",
            "EPOCH 24 8.198103904724121\n",
            "EPOCH 24 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.844052\n",
            "\tBEST VAL LOSS:    -1.847799\n",
            "\tBEST VAL EPOCH:   22\n",
            "EPOCH 25 6.516914367675781\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 25 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.909432\n",
            "\tBEST VAL LOSS:    -1.909432\n",
            "\tBEST VAL EPOCH:   25\n",
            "EPOCH 26 8.252138614654541\n",
            "EPOCH 26 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.889805\n",
            "\tBEST VAL LOSS:    -1.909432\n",
            "\tBEST VAL EPOCH:   25\n",
            "EPOCH 27 6.484626770019531\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 27 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.049723\n",
            "\tBEST VAL LOSS:    -2.049723\n",
            "\tBEST VAL EPOCH:   27\n",
            "EPOCH 28 8.370604515075684\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 28 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.087755\n",
            "\tBEST VAL LOSS:    -2.087755\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 29 6.576344013214111\n",
            "EPOCH 29 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.016949\n",
            "\tBEST VAL LOSS:    -2.087755\n",
            "\tBEST VAL EPOCH:   28\n",
            "EPOCH 30 8.282270193099976\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 30 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.170194\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 31 6.622525215148926\n",
            "EPOCH 31 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.955016\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 32 8.38599443435669\n",
            "EPOCH 32 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.115845\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 33 6.5684003829956055\n",
            "EPOCH 33 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.049151\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 34 8.41103458404541\n",
            "EPOCH 34 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.963924\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 35 6.3609514236450195\n",
            "EPOCH 35 EVAL: \n",
            "\tCURRENT VAL LOSS: -1.993721\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 36 8.259253978729248\n",
            "EPOCH 36 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.133859\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 37 6.501177549362183\n",
            "EPOCH 37 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.150503\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 38 8.241358518600464\n",
            "EPOCH 38 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.165018\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 39 6.546560287475586\n",
            "EPOCH 39 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.111660\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 40 10.17007303237915\n",
            "EPOCH 40 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.135639\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 41 6.572642087936401\n",
            "EPOCH 41 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.168180\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 42 8.02209997177124\n",
            "EPOCH 42 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.167234\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 43 6.553321599960327\n",
            "EPOCH 43 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.111258\n",
            "\tBEST VAL LOSS:    -2.170194\n",
            "\tBEST VAL EPOCH:   30\n",
            "EPOCH 44 7.9426867961883545\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 44 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.218516\n",
            "\tBEST VAL LOSS:    -2.218516\n",
            "\tBEST VAL EPOCH:   44\n",
            "EPOCH 45 6.6641316413879395\n",
            "EPOCH 45 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.125904\n",
            "\tBEST VAL LOSS:    -2.218516\n",
            "\tBEST VAL EPOCH:   44\n",
            "EPOCH 46 7.951300144195557\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 46 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.313467\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 47 6.789196491241455\n",
            "EPOCH 47 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.142849\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 48 7.850695610046387\n",
            "EPOCH 48 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.225638\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 49 6.891330242156982\n",
            "EPOCH 49 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.144693\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 50 7.7911248207092285\n",
            "EPOCH 50 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.140914\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 51 6.7872865200042725\n",
            "EPOCH 51 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.192723\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 52 7.657129287719727\n",
            "EPOCH 52 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.236651\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 53 7.1436192989349365\n",
            "EPOCH 53 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.081199\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 54 7.408291578292847\n",
            "EPOCH 54 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.169773\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 55 7.359595537185669\n",
            "EPOCH 55 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.170126\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 56 7.344302654266357\n",
            "EPOCH 56 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.256751\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 57 7.306778907775879\n",
            "EPOCH 57 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.139047\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 58 7.8792335987091064\n",
            "EPOCH 58 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.273805\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 59 12.954468727111816\n",
            "EPOCH 59 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.231677\n",
            "\tBEST VAL LOSS:    -2.313467\n",
            "\tBEST VAL EPOCH:   46\n",
            "EPOCH 60 7.681830644607544\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 60 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.325862\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 61 7.086700677871704\n",
            "EPOCH 61 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.280798\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 62 7.600801467895508\n",
            "EPOCH 62 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.266245\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 63 7.249879837036133\n",
            "EPOCH 63 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.170816\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 64 7.407329082489014\n",
            "EPOCH 64 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.278767\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 65 7.3788535594940186\n",
            "EPOCH 65 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.294244\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 66 7.358967304229736\n",
            "EPOCH 66 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.308586\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 67 7.299886703491211\n",
            "EPOCH 67 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.318478\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 68 7.131575107574463\n",
            "EPOCH 68 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.301854\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 69 7.687686204910278\n",
            "EPOCH 69 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.289053\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 70 6.867568254470825\n",
            "EPOCH 70 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.324158\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 71 7.79596734046936\n",
            "EPOCH 71 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.279698\n",
            "\tBEST VAL LOSS:    -2.325862\n",
            "\tBEST VAL EPOCH:   60\n",
            "EPOCH 72 6.896683216094971\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 72 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.340316\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 73 8.038454294204712\n",
            "EPOCH 73 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.161404\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 74 6.8604207038879395\n",
            "EPOCH 74 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.237787\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 75 8.023755311965942\n",
            "EPOCH 75 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.267305\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 76 6.61436915397644\n",
            "EPOCH 76 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.311283\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 77 8.043535232543945\n",
            "EPOCH 77 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.325677\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 78 6.529835224151611\n",
            "EPOCH 78 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.319559\n",
            "\tBEST VAL LOSS:    -2.340316\n",
            "\tBEST VAL EPOCH:   72\n",
            "EPOCH 79 10.235151529312134\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 79 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.414844\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 80 6.578021049499512\n",
            "EPOCH 80 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.302311\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 81 8.30630898475647\n",
            "EPOCH 81 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.260542\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 82 6.490748643875122\n",
            "EPOCH 82 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.354505\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 83 8.212669372558594\n",
            "EPOCH 83 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.279657\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 84 6.485982179641724\n",
            "EPOCH 84 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.223698\n",
            "\tBEST VAL LOSS:    -2.414844\n",
            "\tBEST VAL EPOCH:   79\n",
            "EPOCH 85 8.317205667495728\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 85 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.441352\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 86 6.600079774856567\n",
            "EPOCH 86 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.269559\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 87 8.18609094619751\n",
            "EPOCH 87 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.364982\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 88 6.524199485778809\n",
            "EPOCH 88 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.350623\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 89 8.138192653656006\n",
            "EPOCH 89 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.353178\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 90 6.411625862121582\n",
            "EPOCH 90 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.369654\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 91 8.254823207855225\n",
            "EPOCH 91 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.322390\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 92 6.490015745162964\n",
            "EPOCH 92 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.366164\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 93 8.279267311096191\n",
            "EPOCH 93 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.407909\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 94 6.5536110401153564\n",
            "EPOCH 94 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.204110\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 95 8.27936840057373\n",
            "EPOCH 95 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.410243\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 96 6.440430641174316\n",
            "EPOCH 96 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.429899\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 97 8.272019147872925\n",
            "EPOCH 97 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.351283\n",
            "\tBEST VAL LOSS:    -2.441352\n",
            "\tBEST VAL EPOCH:   85\n",
            "EPOCH 98 6.831329107284546\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 98 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.450862\n",
            "\tBEST VAL LOSS:    -2.450862\n",
            "\tBEST VAL EPOCH:   98\n",
            "EPOCH 99 9.789832353591919\n",
            "EPOCH 99 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.419725\n",
            "\tBEST VAL LOSS:    -2.450862\n",
            "\tBEST VAL EPOCH:   98\n",
            "EPOCH 100 6.507584571838379\n",
            "EPOCH 100 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.350389\n",
            "\tBEST VAL LOSS:    -2.450862\n",
            "\tBEST VAL EPOCH:   98\n",
            "EPOCH 101 8.384317398071289\n",
            "EPOCH 101 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.317144\n",
            "\tBEST VAL LOSS:    -2.450862\n",
            "\tBEST VAL EPOCH:   98\n",
            "EPOCH 102 6.5834128856658936\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 102 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.511842\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 103 8.3731050491333\n",
            "EPOCH 103 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.409673\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 104 6.477142095565796\n",
            "EPOCH 104 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.284462\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 105 8.390896320343018\n",
            "EPOCH 105 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.423621\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 106 6.493527412414551\n",
            "EPOCH 106 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.325499\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 107 8.063984394073486\n",
            "EPOCH 107 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.258895\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 108 6.530820846557617\n",
            "EPOCH 108 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.503646\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 109 8.163137435913086\n",
            "EPOCH 109 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.443869\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 110 6.396115779876709\n",
            "EPOCH 110 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.296886\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 111 8.273685932159424\n",
            "EPOCH 111 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.464132\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 112 6.508659601211548\n",
            "EPOCH 112 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.458074\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 113 8.145751476287842\n",
            "EPOCH 113 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.390779\n",
            "\tBEST VAL LOSS:    -2.511842\n",
            "\tBEST VAL EPOCH:   102\n",
            "EPOCH 114 6.581352710723877\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 114 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.515944\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 115 8.370574951171875\n",
            "EPOCH 115 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.392495\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 116 6.490498304367065\n",
            "EPOCH 116 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.382051\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 117 8.312219142913818\n",
            "EPOCH 117 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.475269\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 118 7.982717990875244\n",
            "EPOCH 118 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.354674\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 119 8.684658288955688\n",
            "EPOCH 119 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.343568\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 120 6.5734171867370605\n",
            "EPOCH 120 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.436501\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 121 8.218948364257812\n",
            "EPOCH 121 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.462972\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 122 6.503283739089966\n",
            "EPOCH 122 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.425740\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 123 8.28943943977356\n",
            "EPOCH 123 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.402565\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 124 6.431745767593384\n",
            "EPOCH 124 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.249021\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 125 8.369905710220337\n",
            "EPOCH 125 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.362032\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 126 6.525142431259155\n",
            "EPOCH 126 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.376545\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 127 8.292677879333496\n",
            "EPOCH 127 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.267643\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 128 6.781473159790039\n",
            "EPOCH 128 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.347297\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 129 8.35809326171875\n",
            "EPOCH 129 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.416819\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 130 6.4843504428863525\n",
            "EPOCH 130 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.414066\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 131 8.239208698272705\n",
            "EPOCH 131 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.477146\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 132 6.481537580490112\n",
            "EPOCH 132 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.353150\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 133 8.217978239059448\n",
            "EPOCH 133 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.389552\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 134 6.490765333175659\n",
            "EPOCH 134 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.436363\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 135 8.31473445892334\n",
            "EPOCH 135 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.282792\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 136 6.40566086769104\n",
            "EPOCH 136 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.380331\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 137 8.225636005401611\n",
            "EPOCH 137 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.242780\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 138 8.2738356590271\n",
            "EPOCH 138 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.317038\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 139 10.087892532348633\n",
            "EPOCH 139 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.355811\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 140 7.350520372390747\n",
            "EPOCH 140 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.302115\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 141 8.119002342224121\n",
            "EPOCH 141 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.261874\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 142 6.535748720169067\n",
            "EPOCH 142 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.421730\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 143 8.022159814834595\n",
            "EPOCH 143 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.491278\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 144 6.440663576126099\n",
            "EPOCH 144 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.338990\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 145 8.289798021316528\n",
            "EPOCH 145 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.485427\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 146 6.385286092758179\n",
            "EPOCH 146 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.351863\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 147 8.23761510848999\n",
            "EPOCH 147 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.262806\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 148 6.527934789657593\n",
            "EPOCH 148 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.432107\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 149 8.132479190826416\n",
            "EPOCH 149 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.473219\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 150 6.5790696144104\n",
            "EPOCH 150 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.383276\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 151 8.211732625961304\n",
            "EPOCH 151 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.485835\n",
            "\tBEST VAL LOSS:    -2.515944\n",
            "\tBEST VAL EPOCH:   114\n",
            "EPOCH 152 6.476814270019531\n",
            "BEST VAL RESULT. SAVING MODEL...\n",
            "EPOCH 152 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.525607\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 153 8.399747133255005\n",
            "EPOCH 153 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.506933\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 154 6.537344455718994\n",
            "EPOCH 154 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.261205\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 155 8.256168127059937\n",
            "EPOCH 155 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.446790\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 156 6.592222213745117\n",
            "EPOCH 156 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.477450\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 157 10.070778608322144\n",
            "EPOCH 157 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.483654\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 158 6.997621774673462\n",
            "EPOCH 158 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.449791\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 159 8.469788789749146\n",
            "EPOCH 159 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.342869\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 160 6.552529335021973\n",
            "EPOCH 160 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.238494\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 161 8.169256448745728\n",
            "EPOCH 161 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.144209\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 162 6.587919473648071\n",
            "EPOCH 162 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.126123\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 163 8.300283432006836\n",
            "EPOCH 163 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.260449\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 164 6.41614294052124\n",
            "EPOCH 164 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.230590\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 165 8.153619050979614\n",
            "EPOCH 165 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.149566\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 166 6.645022392272949\n",
            "EPOCH 166 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.187034\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 167 8.221701860427856\n",
            "EPOCH 167 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.327848\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 168 6.596974849700928\n",
            "EPOCH 168 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.075206\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 169 8.302765130996704\n",
            "EPOCH 169 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.305638\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 170 6.451660633087158\n",
            "EPOCH 170 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.326051\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 171 8.185380697250366\n",
            "EPOCH 171 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.299640\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 172 6.528574466705322\n",
            "EPOCH 172 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.423463\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 173 8.118586778640747\n",
            "EPOCH 173 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.348959\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 174 6.557905435562134\n",
            "EPOCH 174 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.348787\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 175 8.303209781646729\n",
            "EPOCH 175 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.420013\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 176 6.437387943267822\n",
            "EPOCH 176 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.420865\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 177 10.395643711090088\n",
            "EPOCH 177 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.141986\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 178 6.605849027633667\n",
            "EPOCH 178 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.391669\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 179 8.026031255722046\n",
            "EPOCH 179 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.328228\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 180 6.581295967102051\n",
            "EPOCH 180 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.243395\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 181 8.26525616645813\n",
            "EPOCH 181 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.379446\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 182 6.539748668670654\n",
            "EPOCH 182 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.382474\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 183 8.32042121887207\n",
            "EPOCH 183 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.361551\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 184 6.653873682022095\n",
            "EPOCH 184 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.427060\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 185 8.195573806762695\n",
            "EPOCH 185 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.286835\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 186 6.560684442520142\n",
            "EPOCH 186 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.356064\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 187 8.354651927947998\n",
            "EPOCH 187 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.315950\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 188 6.4645185470581055\n",
            "EPOCH 188 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.387186\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 189 8.327186822891235\n",
            "EPOCH 189 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.359697\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 190 6.563112258911133\n",
            "EPOCH 190 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.247971\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 191 8.319488286972046\n",
            "EPOCH 191 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.342119\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 192 6.523785352706909\n",
            "EPOCH 192 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.403032\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 193 8.395667552947998\n",
            "EPOCH 193 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.236904\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 194 6.473965406417847\n",
            "EPOCH 194 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.284197\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 195 8.222853660583496\n",
            "EPOCH 195 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.318537\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 196 8.327599287033081\n",
            "EPOCH 196 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.347056\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 197 8.355748414993286\n",
            "EPOCH 197 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.373186\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 198 6.624444484710693\n",
            "EPOCH 198 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.292960\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 199 8.469534158706665\n",
            "EPOCH 199 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.385282\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n",
            "EPOCH 200 6.631179332733154\n",
            "EPOCH 200 EVAL: \n",
            "\tCURRENT VAL LOSS: -2.334642\n",
            "\tBEST VAL LOSS:    -2.525607\n",
            "\tBEST VAL EPOCH:   152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if args.mode == 'eval':\n",
        "test_data = SmallSynthData(args.data_path, 'test', params)\n",
        "forward_pred = 50 - args.test_burn_in_steps\n",
        "test_mse  = evaluate.eval_forward_prediction(model, test_data, args.test_burn_in_steps, forward_pred, params)\n",
        "path = os.path.join(args.working_dir, args.error_out_name%args.test_burn_in_steps)\n",
        "np.save(path, test_mse.cpu().numpy())\n",
        "test_mse_1  = test_mse[0].item()\n",
        "test_mse_15 = test_mse[14].item()\n",
        "test_mse_25 = test_mse[24].item()\n",
        "print(\"FORWARD PRED RESULTS:\")\n",
        "print(\"\\t1 STEP: \", test_mse_1)\n",
        "print(\"\\t15 STEP: \",test_mse_15)\n",
        "print(\"\\t25 STEP: \",test_mse_25)\n",
        "\n",
        "\n",
        "f1, all_acc, acc_0, acc_1, edges = eval_edges(model, val_data, params)\n",
        "print(\"Val Edge results:\")\n",
        "print(\"\\tF1: \",f1)\n",
        "print(\"\\tAll predicted edge accuracy: \",all_acc)\n",
        "print(\"\\tFirst Edge Acc: \",acc_0)\n",
        "print(\"\\tSecond Edge Acc: \",acc_1)\n",
        "out_dir = os.path.join(args.working_dir, 'preds')\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "out_path = os.path.join(out_dir, 'encoder_edges.npy')\n",
        "np.save(out_path, edges.numpy())\n",
        "\n",
        "plot_sample(model, test_data, args.test_burn_in_steps, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8D9NvoqRLnw",
        "outputId": "29543c51-5fc6-411a-857f-928d9f51312e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FORWARD PRED RESULTS:\n",
            "\t1 STEP:  0.00012391609197948128\n",
            "\t15 STEP:  0.026994721964001656\n",
            "\t25 STEP:  0.03485015034675598\n",
            "Val Edge results:\n",
            "\tF1:  0.6159691038095376\n",
            "\tAll predicted edge accuracy:  0.9725170068023903\n",
            "\tFirst Edge Acc:  0.9934937959963759\n",
            "\tSecond Edge Acc:  0.5090337784720422\n",
            "MSE:  0.03326752036809921\n",
            "MSE:  0.030963309109210968\n",
            "MSE:  0.01859257183969021\n",
            "MSE:  0.027361633256077766\n",
            "MSE:  0.012471616268157959\n",
            "MSE:  0.018234288319945335\n",
            "MSE:  0.019530829042196274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-30936ab70676>:97: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig, ax = plt.subplots()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Eval best**"
      ],
      "metadata": {
        "id": "Jq5Jtc2UV4pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args(f'--mode eval --load_best_model --data_path {DATA_PATH} --working_dir {WORKING_DIR} {MODEL_ARGS} {TRAINING_ARGS}'.split()); args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQW7yIH1V7WA",
        "outputId": "51ac70c7-6161-4ebb-a65e-d5610b29828e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Namespace(working_dir='/content/drive/MyDrive/results/dnri/seed_1/', gpu=False, seed=1, mode='eval', load_model=None, load_best_model=True, continue_training=False, model_type='dnri', num_epochs=200, lr=0.0005, mom=0, batch_size=16, sub_batch_size=None, val_batch_size=None, val_interval=5, test=False, use_adam=True, lr_decay_factor=0.5, lr_decay_steps=200, clip_grad_norm=None, verbose=False, tune_on_nll=True, val_teacher_forcing=True, accumulate_steps=1, max_burn_in_count=-1, no_prior=False, avg_prior=False, add_uniform_prior=True, prior_num_layers=3, prior_hidden_size=128, use_learned_prior=True, graph_type='dynamic', avg_encoder_inputs=False, use_dynamic_graph=False, use_static_encoder=False, decoder_type='ref_mlp', encoder_rnn_type='lstm', decoder_rnn_type='gru', encoder_hidden=256, encoder_rnn_hidden=64, num_edge_types=2, encoder_dropout=0.0, encoder_unidirectional=False, encoder_bidirectional=False, encoder_no_factor=False, decoder_hidden=256, decoder_msg_hidden=256, decoder_dropout=0.0, skip_first=True, uniform_prior=False, no_edge_prior=0.9, teacher_forcing_steps=-1, gumbel_temp=0.5, train_hard_sample=False, normalize_kl=True, normalize_kl_per_var=False, normalize_nll=True, normalize_nll_per_var=False, kl_coef=1.0, no_encoder_bn=False, encoder_mlp_hidden=128, encoder_mlp_num_layers=3, rnn_hidden=64, teacher_forcing_prior=False, decoder_rnn_hidden=None, encoder_save_eval_memory=False, encoder_normalize_mode=None, normalize_inputs=False, data_path='/content/drive/MyDrive/datos/', same_data_norm=False, no_data_norm=False, error_out_name='prediction_errors_%dstep.npy', prior_variance=5e-05, test_burn_in_steps=10, error_suffix=None, subject_ind=-1)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = vars(args);params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dyIh-MyWAaE",
        "outputId": "66e6c66c-6ff7-4c1e-ab7d-1b5d13df2d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'working_dir': '/content/drive/MyDrive/results/dnri/seed_1/',\n",
              " 'gpu': False,\n",
              " 'seed': 1,\n",
              " 'mode': 'eval',\n",
              " 'load_model': None,\n",
              " 'load_best_model': True,\n",
              " 'continue_training': False,\n",
              " 'model_type': 'dnri',\n",
              " 'num_epochs': 200,\n",
              " 'lr': 0.0005,\n",
              " 'mom': 0,\n",
              " 'batch_size': 16,\n",
              " 'sub_batch_size': None,\n",
              " 'val_batch_size': None,\n",
              " 'val_interval': 5,\n",
              " 'test': False,\n",
              " 'use_adam': True,\n",
              " 'lr_decay_factor': 0.5,\n",
              " 'lr_decay_steps': 200,\n",
              " 'clip_grad_norm': None,\n",
              " 'verbose': False,\n",
              " 'tune_on_nll': True,\n",
              " 'val_teacher_forcing': True,\n",
              " 'accumulate_steps': 1,\n",
              " 'max_burn_in_count': -1,\n",
              " 'no_prior': False,\n",
              " 'avg_prior': False,\n",
              " 'add_uniform_prior': True,\n",
              " 'prior_num_layers': 3,\n",
              " 'prior_hidden_size': 128,\n",
              " 'use_learned_prior': True,\n",
              " 'graph_type': 'dynamic',\n",
              " 'avg_encoder_inputs': False,\n",
              " 'use_dynamic_graph': False,\n",
              " 'use_static_encoder': False,\n",
              " 'decoder_type': 'ref_mlp',\n",
              " 'encoder_rnn_type': 'lstm',\n",
              " 'decoder_rnn_type': 'gru',\n",
              " 'encoder_hidden': 256,\n",
              " 'encoder_rnn_hidden': 64,\n",
              " 'num_edge_types': 2,\n",
              " 'encoder_dropout': 0.0,\n",
              " 'encoder_unidirectional': False,\n",
              " 'encoder_bidirectional': False,\n",
              " 'encoder_no_factor': False,\n",
              " 'decoder_hidden': 256,\n",
              " 'decoder_msg_hidden': 256,\n",
              " 'decoder_dropout': 0.0,\n",
              " 'skip_first': True,\n",
              " 'uniform_prior': False,\n",
              " 'no_edge_prior': 0.9,\n",
              " 'teacher_forcing_steps': -1,\n",
              " 'gumbel_temp': 0.5,\n",
              " 'train_hard_sample': False,\n",
              " 'normalize_kl': True,\n",
              " 'normalize_kl_per_var': False,\n",
              " 'normalize_nll': True,\n",
              " 'normalize_nll_per_var': False,\n",
              " 'kl_coef': 1.0,\n",
              " 'no_encoder_bn': False,\n",
              " 'encoder_mlp_hidden': 128,\n",
              " 'encoder_mlp_num_layers': 3,\n",
              " 'rnn_hidden': 64,\n",
              " 'teacher_forcing_prior': False,\n",
              " 'decoder_rnn_hidden': None,\n",
              " 'encoder_save_eval_memory': False,\n",
              " 'encoder_normalize_mode': None,\n",
              " 'normalize_inputs': False,\n",
              " 'data_path': '/content/drive/MyDrive/datos/',\n",
              " 'same_data_norm': False,\n",
              " 'no_data_norm': False,\n",
              " 'error_out_name': 'prediction_errors_%dstep.npy',\n",
              " 'prior_variance': 5e-05,\n",
              " 'test_burn_in_steps': 10,\n",
              " 'error_suffix': None,\n",
              " 'subject_ind': -1}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "misc.seed(args.seed) # set seed for numpy torch cuda and random libs"
      ],
      "metadata": {
        "id": "SOe9D8a2WINd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params['num_vars'] = 3\n",
        "params['input_size'] = 4\n",
        "params['input_time_steps'] = 50\n",
        "params['nll_loss_type'] = 'gaussian'\n",
        "train_data = SmallSynthData(args.data_path, 'train', params)\n",
        "val_data   = SmallSynthData(args.data_path, 'val', params)"
      ],
      "metadata": {
        "id": "SlJjrwsdWLms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_builder.build_model(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLaZ2kK_WPPl",
        "outputId": "6c59a111-c781-4884-d7c1-2674465a4df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using learned interaction net decoder.\n",
            "USING NO EDGE PRIOR:  tensor([[[-0.1054, -2.3026]]])\n",
            "dNRI MODEL:  DNRI(\n",
            "  (encoder): DNRI_Encoder(\n",
            "    (mlp1): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp2): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp3): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (mlp4): RefNRIMLP(\n",
            "      (model): Sequential(\n",
            "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "        (1): ELU(alpha=1.0, inplace=True)\n",
            "        (2): Dropout(p=0.0, inplace=False)\n",
            "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (4): ELU(alpha=1.0, inplace=True)\n",
            "      )\n",
            "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (forward_rnn): LSTM(256, 64, batch_first=True)\n",
            "    (reverse_rnn): LSTM(256, 64, batch_first=True)\n",
            "    (encoder_fc_out): Sequential(\n",
            "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): ELU(alpha=1.0, inplace=True)\n",
            "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
            "    )\n",
            "    (prior_fc_out): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): ELU(alpha=1.0, inplace=True)\n",
            "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "      (3): ELU(alpha=1.0, inplace=True)\n",
            "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): DNRI_MLP_Decoder(\n",
            "    (msg_fc1): ModuleList(\n",
            "      (0-1): 2 x Linear(in_features=8, out_features=256, bias=True)\n",
            "    )\n",
            "    (msg_fc2): ModuleList(\n",
            "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "    )\n",
            "    (out_fc1): Linear(in_features=260, out_features=256, bias=True)\n",
            "    (out_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (out_fc3): Linear(in_features=256, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "LOADING BEST MODEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.mode == 'eval':\n",
        "    test_data = SmallSynthData(args.data_path, 'test', params)\n",
        "    forward_pred = 50 - args.test_burn_in_steps\n",
        "    test_mse  = evaluate.eval_forward_prediction(model, test_data, args.test_burn_in_steps, forward_pred, params)\n",
        "    path = os.path.join(args.working_dir, args.error_out_name%args.test_burn_in_steps)\n",
        "    np.save(path, test_mse.cpu().numpy())\n",
        "    test_mse_1  = test_mse[0].item()\n",
        "    test_mse_15 = test_mse[14].item()\n",
        "    test_mse_25 = test_mse[24].item()\n",
        "    print(\"FORWARD PRED RESULTS:\")\n",
        "    print(\"\\t1 STEP: \", test_mse_1)\n",
        "    print(\"\\t15 STEP: \",test_mse_15)\n",
        "    print(\"\\t25 STEP: \",test_mse_25)\n",
        "\n",
        "\n",
        "    f1, all_acc, acc_0, acc_1, edges = eval_edges(model, val_data, params)\n",
        "    print(\"Val Edge results:\")\n",
        "    print(\"\\tF1: \",f1)\n",
        "    print(\"\\tAll predicted edge accuracy: \",all_acc)\n",
        "    print(\"\\tFirst Edge Acc: \",acc_0)\n",
        "    print(\"\\tSecond Edge Acc: \",acc_1)\n",
        "    out_dir = os.path.join(args.working_dir, 'preds')\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    out_path = os.path.join(out_dir, 'encoder_edges.npy')\n",
        "    np.save(out_path, edges.numpy())\n",
        "\n",
        "    plot_sample(model, test_data, args.test_burn_in_steps, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZN1lXNDWUfz",
        "outputId": "731106be-d018-4c26-d35c-5b9f32ccff0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FORWARD PRED RESULTS:\n",
            "\t1 STEP:  0.00010179352102568373\n",
            "\t15 STEP:  0.025014378130435944\n",
            "\t25 STEP:  0.032938361167907715\n",
            "Val Edge results:\n",
            "\tF1:  0.5909762324539021\n",
            "\tAll predicted edge accuracy:  0.9719387755098735\n",
            "\tFirst Edge Acc:  0.9947381519532852\n",
            "\tSecond Edge Acc:  0.4681853888415697\n",
            "MSE:  0.024372613057494164\n",
            "MSE:  0.036975909024477005\n",
            "MSE:  0.02883647009730339\n",
            "MSE:  0.03650031238794327\n",
            "MSE:  0.025631828233599663\n",
            "MSE:  0.021745318546891212\n",
            "MSE:  0.015820661559700966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual inspection of best model DNRI**"
      ],
      "metadata": {
        "id": "oVxuY4TPWbeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate.eval_forward_prediction??"
      ],
      "metadata": {
        "id": "2bsRNiv5Wc11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_edges??"
      ],
      "metadata": {
        "id": "ZvSIShDhW9dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dnri  # Make sure the dnri package is installed\n",
        "\n",
        "import dnri  # Import the dnri module"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTHBdibCXSeM",
        "outputId": "27049f80-564b-457a-c360-bb2529322bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dnri in /content/cvpr_dNRI (1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = dnri.models.dnri.DNRI(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JR5sYhXXCCb",
        "outputId": "06398773-9f35-4bd1-e281-4343ca282d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using learned interaction net decoder.\n",
            "USING NO EDGE PRIOR:  tensor([[[-0.1054, -2.3026]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6ua7666nXbZd",
        "outputId": "9dd151a7-a2ff-4d35-f6ae-52b83f177a66",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-012ab3a6b187>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LOADING BEST MODEL\")\n",
        "path = os.path.join(\"/content/drive/MyDrive/results/dnri/seed_1/\", 'best_model')\n",
        "print(path)\n",
        "model.load(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YJGfcXnXfWQ",
        "outputId": "eaebd893-4c55-42a2-af82-b2dca0d0cf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING BEST MODEL\n",
            "/content/drive/MyDrive/results/dnri/seed_1/best_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71_558iKXx-I",
        "outputId": "77f0e0b2-5688-4beb-f630-702985fb01c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DNRI(\n",
              "  (encoder): DNRI_Encoder(\n",
              "    (mlp1): RefNRIMLP(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=4, out_features=256, bias=True)\n",
              "        (1): ELU(alpha=1.0, inplace=True)\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (4): ELU(alpha=1.0, inplace=True)\n",
              "      )\n",
              "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (mlp2): RefNRIMLP(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (1): ELU(alpha=1.0, inplace=True)\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (4): ELU(alpha=1.0, inplace=True)\n",
              "      )\n",
              "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (mlp3): RefNRIMLP(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (1): ELU(alpha=1.0, inplace=True)\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (4): ELU(alpha=1.0, inplace=True)\n",
              "      )\n",
              "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (mlp4): RefNRIMLP(\n",
              "      (model): Sequential(\n",
              "        (0): Linear(in_features=768, out_features=256, bias=True)\n",
              "        (1): ELU(alpha=1.0, inplace=True)\n",
              "        (2): Dropout(p=0.0, inplace=False)\n",
              "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (4): ELU(alpha=1.0, inplace=True)\n",
              "      )\n",
              "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (forward_rnn): LSTM(256, 64, batch_first=True)\n",
              "    (reverse_rnn): LSTM(256, 64, batch_first=True)\n",
              "    (encoder_fc_out): Sequential(\n",
              "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (1): ELU(alpha=1.0, inplace=True)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (3): ELU(alpha=1.0, inplace=True)\n",
              "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
              "    )\n",
              "    (prior_fc_out): Sequential(\n",
              "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
              "      (1): ELU(alpha=1.0, inplace=True)\n",
              "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (3): ELU(alpha=1.0, inplace=True)\n",
              "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): DNRI_MLP_Decoder(\n",
              "    (msg_fc1): ModuleList(\n",
              "      (0-1): 2 x Linear(in_features=8, out_features=256, bias=True)\n",
              "    )\n",
              "    (msg_fc2): ModuleList(\n",
              "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (out_fc1): Linear(in_features=260, out_features=256, bias=True)\n",
              "    (out_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (out_fc3): Linear(in_features=256, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SmallSynthData??"
      ],
      "metadata": {
        "id": "Csd8pGs5Ydic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.data_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RJ9yzy-zYiy8",
        "outputId": "2e84b04b-1f92-4da7-ede2-c0c8272ad8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/datos/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_data = SmallSynthData(args.data_path, 'test', params)"
      ],
      "metadata": {
        "id": "6gngor6gyavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsCT8xqy2fBL",
        "outputId": "55db98e1-6a93-4a97-ee64-27cea4d49ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<dnri.datasets.small_synth_data.SmallSynthData at 0x79ccb1355360>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true = torch.load(args.data_path+'test_edges')"
      ],
      "metadata": {
        "id": "h9TWrYGByjQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true = torch.load(args.data_path+'val_edges')"
      ],
      "metadata": {
        "id": "KIf9baaLym96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwsxMAMP2jmi",
        "outputId": "993427ae-6515-45cd-da50-9e614676dd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats = torch.load(args.data_path+'test_feats')"
      ],
      "metadata": {
        "id": "tGE-y4iyzvQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats = torch.load(args.data_path+'val_feats')"
      ],
      "metadata": {
        "id": "F4keXzTSzyhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SwNuKPF2qXn",
        "outputId": "e9ee0373-aa56-4b08-cb84-98a49af3ae84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.0803e+00, -1.2923e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [ 1.1310e+00,  1.3297e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3765e-01,  9.5263e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-9.8252e-01, -1.2128e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [ 1.0642e+00,  1.2301e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3577e-01,  9.1986e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-8.8471e-01, -1.1333e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [ 9.9732e-01,  1.1305e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3389e-01,  8.8709e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-7.8689e-01, -1.0538e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [ 9.3048e-01,  1.0309e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3201e-01,  8.5431e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-6.8908e-01, -9.7428e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 8.6365e-01,  9.3134e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3012e-01,  8.2154e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-5.9126e-01, -8.9478e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 7.9681e-01,  8.3175e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.2824e-01,  7.8877e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-4.9345e-01, -8.1529e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 7.2997e-01,  7.3215e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.2636e-01,  7.5600e-01,  1.8804e-03, -3.2772e-02]],\n",
              "\n",
              "        [[-3.9563e-01, -7.3579e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 6.6313e-01,  6.3255e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.2665e-01,  7.2328e-01, -2.8725e-04, -3.2718e-02]],\n",
              "\n",
              "        [[-2.9781e-01, -6.5630e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 5.9630e-01,  5.3296e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3219e-01,  6.9110e-01, -5.5401e-03, -3.2182e-02]],\n",
              "\n",
              "        [[-2.0000e-01, -5.7680e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 5.2946e-01,  4.3336e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.4542e-01,  6.6038e-01, -1.3229e-02, -3.0715e-02]],\n",
              "\n",
              "        [[-1.0218e-01, -4.9730e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 4.6262e-01,  3.3376e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.6789e-01,  6.3237e-01, -2.2468e-02, -2.8008e-02]],\n",
              "\n",
              "        [[-4.3697e-03, -4.1781e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 3.9579e-01,  2.3416e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-3.0011e-01,  6.0836e-01, -3.2225e-02, -2.4019e-02]],\n",
              "\n",
              "        [[ 9.3445e-02, -3.3831e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 3.2895e-01,  1.3457e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-3.4158e-01,  5.8931e-01, -4.1467e-02, -1.9050e-02]],\n",
              "\n",
              "        [[ 1.9126e-01, -2.5881e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 2.6211e-01,  3.4969e-02, -6.6837e-02, -9.9597e-02],\n",
              "         [-3.9090e-01,  5.7558e-01, -4.9322e-02, -1.3723e-02]],\n",
              "\n",
              "        [[ 2.8908e-01, -1.7932e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [ 1.9527e-01, -6.4628e-02, -6.6837e-02, -9.9597e-02],\n",
              "         [-4.4609e-01,  5.6671e-01, -5.5186e-02, -8.8684e-03]],\n",
              "\n",
              "        [[ 3.8689e-01, -9.9822e-02,  9.7815e-02,  7.9496e-02],\n",
              "         [ 1.2844e-01, -1.6423e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-5.0484e-01,  5.6135e-01, -5.8750e-02, -5.3596e-03]],\n",
              "\n",
              "        [[ 4.8471e-01, -2.0326e-02,  9.7815e-02,  7.9496e-02],\n",
              "         [ 6.1599e-02, -2.6382e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-5.6480e-01,  5.5739e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 5.8252e-01,  5.9170e-02,  9.7815e-02,  7.9496e-02],\n",
              "         [-5.2386e-03, -3.6342e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-6.2477e-01,  5.5342e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 6.8034e-01,  1.3867e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-7.2076e-02, -4.6302e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-6.8473e-01,  5.4945e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 7.7815e-01,  2.1816e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.3891e-01, -5.6261e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-7.4470e-01,  5.4548e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 8.7597e-01,  2.9766e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-2.0575e-01, -6.6221e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-8.0466e-01,  5.4151e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 9.7378e-01,  3.7716e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-2.7259e-01, -7.6181e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-8.6463e-01,  5.3754e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.0716e+00,  4.5665e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-3.3943e-01, -8.6141e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-9.2459e-01,  5.3358e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.1694e+00,  5.3615e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-4.0626e-01, -9.6100e-01, -6.6837e-02, -9.9597e-02],\n",
              "         [-9.8455e-01,  5.2961e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.2672e+00,  6.1564e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-4.7310e-01, -1.0606e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.0445e+00,  5.2564e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.3650e+00,  6.9514e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-5.3994e-01, -1.1602e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.1045e+00,  5.2167e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.4629e+00,  7.7464e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-6.0677e-01, -1.2598e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.1644e+00,  5.1770e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.5607e+00,  8.5413e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-6.7361e-01, -1.3594e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.2244e+00,  5.1373e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.6585e+00,  9.3363e-01,  9.7815e-02,  7.9496e-02],\n",
              "         [-7.4045e-01, -1.4590e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.2844e+00,  5.0977e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.7563e+00,  1.0131e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-8.0729e-01, -1.5586e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.3443e+00,  5.0580e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.8541e+00,  1.0926e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-8.7412e-01, -1.6582e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.4043e+00,  5.0183e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 1.9519e+00,  1.1721e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-9.4096e-01, -1.7578e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.4643e+00,  4.9786e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.0497e+00,  1.2516e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.0078e+00, -1.8574e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.5242e+00,  4.9389e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.1476e+00,  1.3311e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.0746e+00, -1.9570e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.5842e+00,  4.8992e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.2454e+00,  1.4106e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.1415e+00, -2.0566e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.6442e+00,  4.8595e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.3432e+00,  1.4901e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.2083e+00, -2.1562e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.7041e+00,  4.8199e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.4410e+00,  1.5696e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.2751e+00, -2.2558e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.7641e+00,  4.7802e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.5388e+00,  1.6491e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.3420e+00, -2.3554e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.8241e+00,  4.7405e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.6366e+00,  1.7286e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.4088e+00, -2.4550e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.8840e+00,  4.7008e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.7345e+00,  1.8081e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.4757e+00, -2.5546e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-1.9440e+00,  4.6611e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.8323e+00,  1.8876e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.5425e+00, -2.6542e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.0040e+00,  4.6214e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 2.9301e+00,  1.9671e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.6093e+00, -2.7538e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.0639e+00,  4.5818e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.0279e+00,  2.0466e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.6762e+00, -2.8534e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.1239e+00,  4.5421e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.1257e+00,  2.1261e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.7430e+00, -2.9529e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.1838e+00,  4.5024e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.2235e+00,  2.2056e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.8098e+00, -3.0525e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.2438e+00,  4.4627e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.3213e+00,  2.2851e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.8767e+00, -3.1521e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3038e+00,  4.4230e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.4192e+00,  2.3646e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-1.9435e+00, -3.2517e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.3637e+00,  4.3833e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.5170e+00,  2.4441e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-2.0104e+00, -3.3513e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.4237e+00,  4.3437e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.6148e+00,  2.5236e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-2.0772e+00, -3.4509e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.4837e+00,  4.3040e-01, -5.9965e-02, -3.9684e-03]],\n",
              "\n",
              "        [[ 3.7126e+00,  2.6030e+00,  9.7815e-02,  7.9496e-02],\n",
              "         [-2.1440e+00, -3.5505e+00, -6.6837e-02, -9.9597e-02],\n",
              "         [-2.5436e+00,  4.2643e-01, -5.9965e-02, -3.9684e-03]]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loc_max = feats[:, :, :, :2].max()\n",
        "loc_min = feats[:, :, :, :2].min()\n",
        "vel_max = feats[:, :, :, 2:].max()\n",
        "vel_min = feats[:, :, :, 2:].min()\n",
        "feats[:,:,:, :2] = (feats[:,:,:,:2]-loc_min)*2/(loc_max - loc_min) - 1\n",
        "feats[:,:,:,2:] = (feats[:,:,:,2:]-vel_min)*2/(vel_max-vel_min)-1\n"
      ],
      "metadata": {
        "id": "8a8ei-Mu1HzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7MSRH312vgG",
        "outputId": "ed623e1f-1588-4b6b-c56a-13889df79b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.5806e-01, -1.7931e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 6.3576e-02,  8.3496e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.3602e-02,  4.5699e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-1.4826e-01, -1.7134e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 5.6877e-02,  7.3513e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.3413e-02,  4.2414e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-1.3846e-01, -1.6337e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 5.0178e-02,  6.3531e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.3225e-02,  3.9129e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-1.2865e-01, -1.5540e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 4.3479e-02,  5.3548e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.3036e-02,  3.5845e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-1.1885e-01, -1.4743e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 3.6780e-02,  4.3565e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.2848e-02,  3.2560e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-1.0904e-01, -1.3947e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 3.0081e-02,  3.3583e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.2659e-02,  2.9275e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-9.9240e-02, -1.3150e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 2.3382e-02,  2.3600e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.2471e-02,  2.5991e-02,  3.7932e-02, -8.3844e-02]],\n",
              "\n",
              "        [[-8.9436e-02, -1.2353e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 1.6683e-02,  1.3618e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.2500e-02,  2.2711e-02,  3.0314e-02, -8.3654e-02]],\n",
              "\n",
              "        [[-7.9632e-02, -1.1556e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 9.9840e-03,  3.6353e-03, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.3055e-02,  1.9486e-02,  1.1855e-02, -8.1772e-02]],\n",
              "\n",
              "        [[-6.9828e-02, -1.0759e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [ 3.2849e-03, -6.3473e-03, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.4381e-02,  1.6407e-02, -1.5166e-02, -7.6614e-02]],\n",
              "\n",
              "        [[-6.0024e-02, -9.9627e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-3.4142e-03, -1.6330e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.6633e-02,  1.3600e-02, -4.7633e-02, -6.7102e-02]],\n",
              "\n",
              "        [[-5.0220e-02, -9.1659e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.0113e-02, -2.6312e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-7.9863e-02,  1.1193e-02, -8.1922e-02, -5.3086e-02]],\n",
              "\n",
              "        [[-4.0416e-02, -8.3691e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.6812e-02, -3.6295e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-8.4019e-02,  9.2832e-03, -1.1440e-01, -3.5621e-02]],\n",
              "\n",
              "        [[-3.0613e-02, -7.5723e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.3511e-02, -4.6278e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-8.8962e-02,  7.9077e-03, -1.4201e-01, -1.6901e-02]],\n",
              "\n",
              "        [[-2.0809e-02, -6.7755e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-3.0210e-02, -5.6260e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-9.4494e-02,  7.0189e-03, -1.6261e-01,  1.5843e-04]],\n",
              "\n",
              "        [[-1.1005e-02, -5.9788e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-3.6909e-02, -6.6243e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.0038e-01,  6.4816e-03, -1.7514e-01,  1.2489e-02]],\n",
              "\n",
              "        [[-1.2007e-03, -5.1820e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-4.3608e-02, -7.6225e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.0639e-01,  6.0840e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 8.6031e-03, -4.3852e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-5.0308e-02, -8.6208e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.1240e-01,  5.6862e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.8407e-02, -3.5884e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-5.7007e-02, -9.6190e-02, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.1841e-01,  5.2884e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.8211e-02, -2.7916e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-6.3706e-02, -1.0617e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.2442e-01,  4.8907e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 3.8015e-02, -1.9948e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-7.0405e-02, -1.1616e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.3043e-01,  4.4929e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 4.7819e-02, -1.1980e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-7.7104e-02, -1.2614e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.3644e-01,  4.0952e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 5.7623e-02, -4.0127e-03,  3.7507e-01,  3.1069e-01],\n",
              "         [-8.3803e-02, -1.3612e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.4245e-01,  3.6975e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 6.7427e-02,  3.9551e-03,  3.7507e-01,  3.1069e-01],\n",
              "         [-9.0502e-02, -1.4610e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.4846e-01,  3.2997e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 7.7231e-02,  1.1923e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-9.7201e-02, -1.5609e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.5447e-01,  2.9019e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 8.7035e-02,  1.9891e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.0390e-01, -1.6607e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.6048e-01,  2.5041e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 9.6838e-02,  2.7859e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.1060e-01, -1.7605e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.6649e-01,  2.1064e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.0664e-01,  3.5827e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.1730e-01, -1.8603e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.7250e-01,  1.7087e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.1645e-01,  4.3794e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.2400e-01, -1.9602e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.7851e-01,  1.3109e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.2625e-01,  5.1762e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.3070e-01, -2.0600e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.8452e-01,  9.1326e-04, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.3605e-01,  5.9730e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.3740e-01, -2.1598e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.9054e-01,  5.1534e-04, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.4586e-01,  6.7698e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.4409e-01, -2.2596e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-1.9655e-01,  1.1766e-04, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.5566e-01,  7.5666e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.5079e-01, -2.3595e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.0256e-01, -2.8008e-04, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.6547e-01,  8.3634e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.5749e-01, -2.4593e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.0857e-01, -6.7782e-04, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.7527e-01,  9.1601e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.6419e-01, -2.5591e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.1458e-01, -1.0755e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.8507e-01,  9.9569e-02,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.7089e-01, -2.6589e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.2059e-01, -1.4734e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 1.9488e-01,  1.0754e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.7759e-01, -2.7588e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.2660e-01, -1.8711e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.0468e-01,  1.1550e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.8429e-01, -2.8586e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.3261e-01, -2.2689e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.1449e-01,  1.2347e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.9099e-01, -2.9584e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.3862e-01, -2.6666e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.2429e-01,  1.3144e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-1.9769e-01, -3.0582e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.4463e-01, -3.0643e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.3409e-01,  1.3941e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.0439e-01, -3.1581e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.5064e-01, -3.4621e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.4390e-01,  1.4738e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.1108e-01, -3.2579e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.5665e-01, -3.8599e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.5370e-01,  1.5534e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.1778e-01, -3.3577e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.6266e-01, -4.2576e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.6351e-01,  1.6331e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.2448e-01, -3.4575e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.6867e-01, -4.6553e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.7331e-01,  1.7128e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.3118e-01, -3.5574e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.7468e-01, -5.0530e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.8311e-01,  1.7925e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.3788e-01, -3.6572e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.8069e-01, -5.4508e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 2.9292e-01,  1.8722e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.4458e-01, -3.7570e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.8670e-01, -5.8486e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 3.0272e-01,  1.9518e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.5128e-01, -3.8568e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.9271e-01, -6.2463e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 3.1253e-01,  2.0315e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.5798e-01, -3.9567e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-2.9872e-01, -6.6441e-03, -1.7940e-01,  1.7378e-02]],\n",
              "\n",
              "        [[ 3.2233e-01,  2.1112e-01,  3.7507e-01,  3.1069e-01],\n",
              "         [-2.6468e-01, -4.0565e-01, -2.0356e-01, -3.1868e-01],\n",
              "         [-3.0473e-01, -7.0418e-03, -1.7940e-01,  1.7378e-02]]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DNRI??"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvSom0ZT1Kgp",
        "outputId": "d641ae44-a53d-40dc-e310-50889801281a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object `DNRI` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Ea-7z29D1hXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    inputs = feats # B, T, N, F\n",
        "\n",
        "    # Encode input = predict prior and edge class\n",
        "    sinputs = inputs[:, :-1]   # B,T-1,N,F\n",
        "    prior_logits, posterior_logits, prior_state = model.encoder(sinputs)\n",
        "\n",
        "    # Decode = predict next timestep features\n",
        "    hidden = model.decoder.get_initial_hidden(inputs) # zeros B,N,H\n",
        "    timesteps = inputs.size(1)  # T\n",
        "    edges, predictions = [], []\n",
        "\n",
        "    for step in range(timesteps - 1): # T-1\n",
        "        dinput = inputs[:, step]  # B, N, F\n",
        "        prediction, hidden, edge = model.single_step_forward(\n",
        "            dinput,  # B, N, F\n",
        "            hidden,  # B, N, H\n",
        "            posterior_logits[:, step],  # B, E, C\n",
        "            hard_sample=not model.training,\n",
        "        )\n",
        "        predictions.append(prediction)  # B, N, F\n",
        "        edges.append(edge)              # B, E, C\n",
        "\n",
        "    predictions = torch.stack(predictions, dim=1) # B, T-1, N, F\n",
        "    target = inputs[:, 1:]  # [B:batch, T:time-1, N:num_object, F:num_feats] target the future\n",
        "\n",
        "    # get loss\n",
        "    # reconstruction\n",
        "    loss_nll = model.nll_gaussian(predictions, target)\n",
        "\n",
        "    # priors\n",
        "    # learned prior\n",
        "    prob = F.softmax(posterior_logits, dim=-1)\n",
        "    loss_kl_learned = model.kl_categorical_learned(prob, prior_logits)\n",
        "    # uniform (or guessed) prior\n",
        "    loss_kl_uniform = model.kl_categorical_avg(prob)\n",
        "\n",
        "    loss_kl = 0.5 * loss_kl_learned + 0.5 * loss_kl_uniform\n",
        "\n",
        "    loss = loss_nll + model.kl_coef * loss_kl\n",
        "    loss = loss.mean().item()"
      ],
      "metadata": {
        "id": "MVUkN1WL1mlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, loss_nll.mean(), loss_kl.mean(), loss_kl_uniform.mean(), loss_kl_learned.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvLy6p4s1r2Q",
        "outputId": "e5c354c9-cdcc-4b88-ccc6-a31659e9f427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.154784679412842,\n",
              " tensor(-2.2162),\n",
              " tensor(0.0614),\n",
              " tensor(0.0964),\n",
              " tensor(0.0265))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.mse_loss(predictions, target).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbaj95c12Xo",
        "outputId": "44b98b37-a8cc-4c40-8112-d256daca0af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5415225031320006e-05"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "qp_3LP-N15OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges = torch.stack(edges).transpose(1,0)"
      ],
      "metadata": {
        "id": "hNlPDbga18PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY_nv2aC3ENQ",
        "outputId": "f182655e-7c6c-405c-ca13-24c9663b1596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000e+00, 8.1347e-24],\n",
              "         [1.0000e+00, 3.6132e-29],\n",
              "         [1.0000e+00, 1.2934e-15],\n",
              "         [1.0000e+00, 2.1181e-21],\n",
              "         [1.0000e+00, 3.3391e-13],\n",
              "         [1.0000e+00, 5.6062e-18]],\n",
              "\n",
              "        [[1.0000e+00, 9.6799e-22],\n",
              "         [1.0000e+00, 2.1281e-25],\n",
              "         [1.0000e+00, 6.3079e-19],\n",
              "         [1.0000e+00, 1.0249e-23],\n",
              "         [1.0000e+00, 5.7036e-23],\n",
              "         [1.0000e+00, 1.9238e-19]],\n",
              "\n",
              "        [[1.0000e+00, 3.0310e-24],\n",
              "         [1.0000e+00, 1.7661e-20],\n",
              "         [1.0000e+00, 8.7027e-24],\n",
              "         [1.0000e+00, 3.4158e-19],\n",
              "         [1.0000e+00, 4.2350e-23],\n",
              "         [1.0000e+00, 1.9687e-19]],\n",
              "\n",
              "        [[1.0000e+00, 1.1083e-18],\n",
              "         [1.0000e+00, 5.1644e-21],\n",
              "         [1.0000e+00, 2.5862e-25],\n",
              "         [1.0000e+00, 2.5035e-17],\n",
              "         [1.0000e+00, 9.5698e-23],\n",
              "         [1.0000e+00, 1.7137e-18]],\n",
              "\n",
              "        [[1.0000e+00, 2.9951e-19],\n",
              "         [1.0000e+00, 9.7461e-17],\n",
              "         [1.0000e+00, 3.4888e-27],\n",
              "         [1.0000e+00, 1.0200e-20],\n",
              "         [1.0000e+00, 1.9720e-26],\n",
              "         [1.0000e+00, 5.1131e-20]],\n",
              "\n",
              "        [[1.0000e+00, 1.2200e-17],\n",
              "         [1.0000e+00, 1.7101e-14],\n",
              "         [1.0000e+00, 4.8087e-27],\n",
              "         [1.0000e+00, 2.3636e-18],\n",
              "         [1.0000e+00, 1.3418e-27],\n",
              "         [1.0000e+00, 9.0063e-21]],\n",
              "\n",
              "        [[1.0000e+00, 6.0485e-17],\n",
              "         [1.0000e+00, 3.9424e-12],\n",
              "         [1.0000e+00, 8.3409e-25],\n",
              "         [1.0000e+00, 5.6088e-20],\n",
              "         [1.0000e+00, 1.2365e-27],\n",
              "         [1.0000e+00, 6.1786e-22]],\n",
              "\n",
              "        [[1.0000e+00, 8.8505e-17],\n",
              "         [1.0000e+00, 9.7205e-15],\n",
              "         [1.0000e+00, 4.8884e-25],\n",
              "         [1.0000e+00, 2.2956e-23],\n",
              "         [1.0000e+00, 1.7379e-23],\n",
              "         [1.0000e+00, 3.5989e-20]],\n",
              "\n",
              "        [[1.0000e+00, 3.2266e-16],\n",
              "         [1.0000e+00, 1.6873e-12],\n",
              "         [1.0000e+00, 1.4630e-24],\n",
              "         [1.0000e+00, 5.4533e-16],\n",
              "         [1.0000e+00, 3.8678e-29],\n",
              "         [1.0000e+00, 4.0286e-23]],\n",
              "\n",
              "        [[1.0000e+00, 3.1925e-18],\n",
              "         [1.0000e+00, 1.0566e-09],\n",
              "         [1.0000e+00, 2.4447e-25],\n",
              "         [1.0000e+00, 9.8816e-13],\n",
              "         [1.0000e+00, 1.2274e-26],\n",
              "         [1.0000e+00, 1.6371e-19]],\n",
              "\n",
              "        [[1.0000e+00, 4.6340e-15],\n",
              "         [1.0000e+00, 2.4706e-09],\n",
              "         [1.0000e+00, 4.1548e-23],\n",
              "         [1.0000e+00, 7.7326e-11],\n",
              "         [1.0000e+00, 1.5579e-28],\n",
              "         [1.0000e+00, 3.3574e-20]],\n",
              "\n",
              "        [[1.0000e+00, 4.2641e-14],\n",
              "         [1.0000e+00, 3.9483e-11],\n",
              "         [1.0000e+00, 1.3672e-23],\n",
              "         [1.0000e+00, 1.4375e-08],\n",
              "         [1.0000e+00, 2.8104e-26],\n",
              "         [1.0000e+00, 1.3219e-18]],\n",
              "\n",
              "        [[1.0000e+00, 5.4475e-14],\n",
              "         [1.0000e+00, 7.4261e-08],\n",
              "         [1.0000e+00, 9.6585e-22],\n",
              "         [1.0000e+00, 3.9935e-07],\n",
              "         [1.0000e+00, 3.7764e-24],\n",
              "         [1.0000e+00, 2.7702e-17]],\n",
              "\n",
              "        [[1.0000e+00, 4.8670e-13],\n",
              "         [1.0000e+00, 1.5916e-07],\n",
              "         [1.0000e+00, 7.2439e-24],\n",
              "         [1.0000e+00, 2.0603e-09],\n",
              "         [1.0000e+00, 6.7627e-26],\n",
              "         [1.0000e+00, 3.2110e-18]],\n",
              "\n",
              "        [[1.0000e+00, 3.2363e-13],\n",
              "         [1.0000e+00, 3.0379e-09],\n",
              "         [1.0000e+00, 4.8517e-22],\n",
              "         [9.9977e-01, 2.2697e-04],\n",
              "         [1.0000e+00, 9.9836e-27],\n",
              "         [1.0000e+00, 3.4775e-16]],\n",
              "\n",
              "        [[1.0000e+00, 1.9808e-12],\n",
              "         [1.0000e+00, 7.6553e-07],\n",
              "         [1.0000e+00, 3.9006e-22],\n",
              "         [1.0000e+00, 4.4327e-10],\n",
              "         [1.0000e+00, 1.3935e-24],\n",
              "         [1.0000e+00, 9.5566e-18]],\n",
              "\n",
              "        [[1.0000e+00, 1.5698e-11],\n",
              "         [1.0000e+00, 6.4815e-08],\n",
              "         [1.0000e+00, 1.6971e-22],\n",
              "         [1.0000e+00, 4.5044e-09],\n",
              "         [1.0000e+00, 1.0019e-24],\n",
              "         [1.0000e+00, 1.7337e-18]],\n",
              "\n",
              "        [[1.0000e+00, 1.5029e-10],\n",
              "         [1.0000e+00, 2.5846e-11],\n",
              "         [1.0000e+00, 7.4144e-23],\n",
              "         [1.0000e+00, 3.0983e-08],\n",
              "         [1.0000e+00, 1.1440e-22],\n",
              "         [1.0000e+00, 8.0190e-18]],\n",
              "\n",
              "        [[1.0000e+00, 2.0886e-10],\n",
              "         [1.0000e+00, 1.6682e-10],\n",
              "         [1.0000e+00, 5.2570e-21],\n",
              "         [9.9940e-01, 6.0195e-04],\n",
              "         [1.0000e+00, 6.9099e-24],\n",
              "         [1.0000e+00, 2.8041e-16]],\n",
              "\n",
              "        [[1.0000e+00, 4.4928e-12],\n",
              "         [1.0000e+00, 5.2709e-11],\n",
              "         [1.0000e+00, 9.8250e-23],\n",
              "         [1.0000e+00, 2.8082e-08],\n",
              "         [1.0000e+00, 1.4872e-22],\n",
              "         [1.0000e+00, 8.5872e-17]],\n",
              "\n",
              "        [[1.0000e+00, 9.8730e-13],\n",
              "         [1.0000e+00, 7.9219e-10],\n",
              "         [1.0000e+00, 5.1697e-20],\n",
              "         [1.0000e+00, 7.9479e-10],\n",
              "         [1.0000e+00, 5.1192e-22],\n",
              "         [1.0000e+00, 8.2366e-17]],\n",
              "\n",
              "        [[1.0000e+00, 1.7396e-09],\n",
              "         [1.0000e+00, 5.3783e-12],\n",
              "         [1.0000e+00, 2.9964e-21],\n",
              "         [1.0000e+00, 5.6488e-09],\n",
              "         [1.0000e+00, 9.1553e-20],\n",
              "         [1.0000e+00, 1.6622e-16]],\n",
              "\n",
              "        [[1.0000e+00, 1.6986e-09],\n",
              "         [1.0000e+00, 1.1503e-10],\n",
              "         [1.0000e+00, 6.0996e-19],\n",
              "         [1.0000e+00, 4.7689e-07],\n",
              "         [1.0000e+00, 5.5945e-22],\n",
              "         [1.0000e+00, 2.1917e-16]],\n",
              "\n",
              "        [[1.0000e+00, 6.4534e-08],\n",
              "         [1.0000e+00, 6.5622e-17],\n",
              "         [1.0000e+00, 1.7753e-18],\n",
              "         [1.0000e+00, 7.7297e-08],\n",
              "         [1.0000e+00, 2.2864e-22],\n",
              "         [1.0000e+00, 1.9057e-18]],\n",
              "\n",
              "        [[1.0000e+00, 8.8306e-10],\n",
              "         [1.0000e+00, 1.2179e-15],\n",
              "         [1.0000e+00, 1.8861e-19],\n",
              "         [1.0000e+00, 1.3442e-06],\n",
              "         [1.0000e+00, 1.5825e-22],\n",
              "         [1.0000e+00, 5.7087e-16]],\n",
              "\n",
              "        [[1.0000e+00, 1.4100e-10],\n",
              "         [1.0000e+00, 4.4418e-16],\n",
              "         [1.0000e+00, 3.3146e-19],\n",
              "         [1.0000e+00, 1.6651e-08],\n",
              "         [1.0000e+00, 1.4711e-20],\n",
              "         [1.0000e+00, 2.6932e-16]],\n",
              "\n",
              "        [[1.0000e+00, 9.3691e-12],\n",
              "         [1.0000e+00, 6.9921e-16],\n",
              "         [1.0000e+00, 2.4872e-13],\n",
              "         [1.0000e+00, 6.6578e-08],\n",
              "         [1.0000e+00, 1.2809e-23],\n",
              "         [1.0000e+00, 2.8580e-17]],\n",
              "\n",
              "        [[1.0000e+00, 8.8524e-15],\n",
              "         [1.0000e+00, 5.5622e-19],\n",
              "         [1.0000e+00, 3.0595e-18],\n",
              "         [1.0000e+00, 2.8304e-13],\n",
              "         [1.0000e+00, 1.4650e-19],\n",
              "         [1.0000e+00, 3.0983e-17]],\n",
              "\n",
              "        [[1.0000e+00, 5.3677e-15],\n",
              "         [1.0000e+00, 2.5202e-17],\n",
              "         [1.0000e+00, 3.8041e-23],\n",
              "         [1.0000e+00, 1.5022e-12],\n",
              "         [1.0000e+00, 1.5836e-21],\n",
              "         [1.0000e+00, 5.5333e-18]],\n",
              "\n",
              "        [[1.0000e+00, 4.3433e-19],\n",
              "         [1.0000e+00, 1.8520e-18],\n",
              "         [1.0000e+00, 3.1275e-19],\n",
              "         [1.0000e+00, 1.6085e-10],\n",
              "         [1.0000e+00, 1.1159e-19],\n",
              "         [1.0000e+00, 5.3129e-19]],\n",
              "\n",
              "        [[1.0000e+00, 4.8311e-18],\n",
              "         [1.0000e+00, 2.4221e-15],\n",
              "         [1.0000e+00, 6.1408e-17],\n",
              "         [1.0000e+00, 1.6035e-11],\n",
              "         [1.0000e+00, 1.4598e-18],\n",
              "         [1.0000e+00, 7.4730e-16]],\n",
              "\n",
              "        [[1.0000e+00, 4.7221e-16],\n",
              "         [1.0000e+00, 5.7985e-21],\n",
              "         [1.0000e+00, 1.5880e-20],\n",
              "         [1.0000e+00, 3.3061e-13],\n",
              "         [1.0000e+00, 8.6098e-19],\n",
              "         [1.0000e+00, 4.4883e-19]],\n",
              "\n",
              "        [[1.0000e+00, 7.0542e-21],\n",
              "         [1.0000e+00, 9.7774e-20],\n",
              "         [1.0000e+00, 1.2200e-17],\n",
              "         [1.0000e+00, 5.9563e-13],\n",
              "         [1.0000e+00, 1.2666e-13],\n",
              "         [1.0000e+00, 8.4596e-18]],\n",
              "\n",
              "        [[1.0000e+00, 9.8037e-19],\n",
              "         [1.0000e+00, 4.8971e-21],\n",
              "         [1.0000e+00, 1.3585e-23],\n",
              "         [1.0000e+00, 6.9761e-12],\n",
              "         [1.0000e+00, 7.1762e-14],\n",
              "         [1.0000e+00, 1.7423e-17]],\n",
              "\n",
              "        [[1.0000e+00, 2.1581e-21],\n",
              "         [1.0000e+00, 6.1664e-23],\n",
              "         [1.0000e+00, 3.0920e-18],\n",
              "         [1.0000e+00, 2.1679e-11],\n",
              "         [1.0000e+00, 4.0143e-17],\n",
              "         [1.0000e+00, 2.8638e-21]],\n",
              "\n",
              "        [[1.0000e+00, 5.8823e-23],\n",
              "         [1.0000e+00, 3.0007e-22],\n",
              "         [1.0000e+00, 6.0085e-20],\n",
              "         [1.0000e+00, 3.6283e-12],\n",
              "         [1.0000e+00, 7.9014e-15],\n",
              "         [1.0000e+00, 3.2077e-20]],\n",
              "\n",
              "        [[1.0000e+00, 2.0181e-21],\n",
              "         [1.0000e+00, 1.2942e-20],\n",
              "         [1.0000e+00, 6.8777e-16],\n",
              "         [1.0000e+00, 2.9147e-12],\n",
              "         [1.0000e+00, 2.9954e-17],\n",
              "         [1.0000e+00, 8.8300e-22]],\n",
              "\n",
              "        [[1.0000e+00, 1.3005e-20],\n",
              "         [1.0000e+00, 7.3779e-17],\n",
              "         [1.0000e+00, 6.3708e-20],\n",
              "         [1.0000e+00, 7.0693e-11],\n",
              "         [1.0000e+00, 7.1254e-16],\n",
              "         [1.0000e+00, 4.2513e-19]],\n",
              "\n",
              "        [[1.0000e+00, 1.9958e-21],\n",
              "         [1.0000e+00, 9.7469e-21],\n",
              "         [1.0000e+00, 9.9801e-20],\n",
              "         [1.0000e+00, 4.6898e-12],\n",
              "         [1.0000e+00, 5.0256e-14],\n",
              "         [1.0000e+00, 2.2840e-17]],\n",
              "\n",
              "        [[1.0000e+00, 2.3405e-20],\n",
              "         [1.0000e+00, 2.6427e-19],\n",
              "         [1.0000e+00, 1.3630e-20],\n",
              "         [1.0000e+00, 2.3421e-11],\n",
              "         [1.0000e+00, 1.2196e-16],\n",
              "         [1.0000e+00, 3.1223e-17]],\n",
              "\n",
              "        [[1.0000e+00, 1.8872e-22],\n",
              "         [1.0000e+00, 3.4762e-19],\n",
              "         [1.0000e+00, 1.3652e-20],\n",
              "         [1.0000e+00, 3.8785e-12],\n",
              "         [1.0000e+00, 2.1767e-16],\n",
              "         [1.0000e+00, 3.5036e-19]],\n",
              "\n",
              "        [[1.0000e+00, 4.4642e-21],\n",
              "         [1.0000e+00, 4.5479e-22],\n",
              "         [1.0000e+00, 1.8495e-22],\n",
              "         [1.0000e+00, 2.4596e-12],\n",
              "         [1.0000e+00, 1.0941e-15],\n",
              "         [1.0000e+00, 1.8241e-19]],\n",
              "\n",
              "        [[1.0000e+00, 3.4423e-21],\n",
              "         [1.0000e+00, 4.8878e-20],\n",
              "         [1.0000e+00, 2.0397e-20],\n",
              "         [1.0000e+00, 3.2841e-11],\n",
              "         [1.0000e+00, 5.0583e-13],\n",
              "         [1.0000e+00, 1.0715e-19]],\n",
              "\n",
              "        [[1.0000e+00, 3.1641e-19],\n",
              "         [1.0000e+00, 1.3452e-19],\n",
              "         [1.0000e+00, 3.2072e-21],\n",
              "         [1.0000e+00, 1.2302e-09],\n",
              "         [1.0000e+00, 2.8881e-15],\n",
              "         [1.0000e+00, 1.3449e-17]],\n",
              "\n",
              "        [[1.0000e+00, 3.6214e-21],\n",
              "         [1.0000e+00, 1.6700e-19],\n",
              "         [1.0000e+00, 6.7333e-18],\n",
              "         [1.0000e+00, 2.1213e-13],\n",
              "         [1.0000e+00, 9.1204e-16],\n",
              "         [1.0000e+00, 6.1291e-18]],\n",
              "\n",
              "        [[1.0000e+00, 1.6313e-18],\n",
              "         [1.0000e+00, 1.3518e-19],\n",
              "         [1.0000e+00, 1.6479e-18],\n",
              "         [1.0000e+00, 4.8601e-14],\n",
              "         [1.0000e+00, 2.3668e-15],\n",
              "         [1.0000e+00, 6.8994e-17]],\n",
              "\n",
              "        [[1.0000e+00, 1.5974e-19],\n",
              "         [1.0000e+00, 1.1321e-23],\n",
              "         [1.0000e+00, 1.9640e-17],\n",
              "         [1.0000e+00, 2.7428e-11],\n",
              "         [1.0000e+00, 6.6151e-15],\n",
              "         [1.0000e+00, 2.5984e-14]],\n",
              "\n",
              "        [[1.0000e+00, 3.2063e-18],\n",
              "         [1.0000e+00, 6.7276e-20],\n",
              "         [1.0000e+00, 1.6698e-15],\n",
              "         [1.0000e+00, 2.9400e-14],\n",
              "         [1.0000e+00, 2.1530e-15],\n",
              "         [1.0000e+00, 4.1997e-15]],\n",
              "\n",
              "        [[1.0000e+00, 5.3283e-16],\n",
              "         [1.0000e+00, 8.7715e-18],\n",
              "         [1.0000e+00, 6.2280e-16],\n",
              "         [1.0000e+00, 1.0378e-10],\n",
              "         [1.0000e+00, 6.2930e-18],\n",
              "         [1.0000e+00, 7.0139e-13]]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(torch.argmax(edges,dim=-1).to(int).numpy().reshape(-1,6)==1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyqeII4E1_w2",
        "outputId": "dc6bb714-707f-4fea-b8f0-4f79e302717a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 187,   0, 571,   1,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(edges_true[:,1:].to(bool).numpy().reshape(-1,6)==1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne76JIII2Snx",
        "outputId": "e41d3997-6ab0-41f2-f381-9c2ad6bba074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 602,   0, 666,   0,   0])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true_ = edges_true[:,1:].long()\n",
        "edges_pred  = edges.argmax(dim=-1).long()"
      ],
      "metadata": {
        "id": "Xvqn1WCM2V4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGw51Mjo3NWF",
        "outputId": "d0b2b90e-1743-4af9-ee27-00ad5ece6525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_pred[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhxrIdzc3RT0",
        "outputId": "e3a8c46d-dc7a-43c3-eae7-2a84db36f48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_true_.shape, edges_pred.shape, edges_true_.dtype, edges_pred.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unOJajtQ2YcN",
        "outputId": "86cabedf-7757-4f6a-9622-3dd24671ca7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 49, 6]), torch.Size([100, 49, 6]), torch.int64, torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(f1_score(edges_true_.numpy().reshape(-1,6)[:,1],edges_pred.numpy().reshape(-1,6)[:,1],)\n",
        " +\n",
        " f1_score(edges_true_.numpy().reshape(-1,6)[:,3],edges_pred.numpy().reshape(-1,6)[:,3],)\n",
        ")/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRdUTjze2bBJ",
        "outputId": "7c2601d2-87ba-4c20-8b86-c31ec66073e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.580013381243513"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_count = edges_pred.numel()\n",
        "full_edge_count = edges_true_.numel()\n",
        "correct_edges = ((edges_pred == edges_true_)).sum().item()\n",
        "edge_0_count = (edges_true_ == 0).sum().item()\n",
        "edge_1_count = (edges_true_ == 1).sum().item()\n",
        "correct_0_edges = ((edges_pred == edges_true_)*(edges_true_ == 0)).sum().item()\n",
        "correct_1_edges = ((edges_pred == edges_true_)*(edges_true_ == 1)).sum().item()\n",
        "correct = (edges_pred*edges_true_).sum().item()\n",
        "num_predicted = edges_pred.sum().item()\n",
        "num_gt = edges_true_.sum().item()\n",
        "prec = correct / (num_predicted + 1e-8)\n",
        "rec = correct / (num_gt + 1e-8)\n",
        "f1 = 2*prec*rec / (prec+rec+1e-6)\n",
        "print(f1, correct_edges / (full_edge_count + 1e-8), correct_0_edges / (edge_0_count + 1e-8), correct_1_edges / (edge_1_count + 1e-8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJxfULW62eyJ",
        "outputId": "21ed6112-de28-411c-defe-ddc7e50560b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6048342626522633 0.9727551020404854 0.9948101805769249 0.48343848580060383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see accuracy compared with data class percentages\n",
        "1-edges_true.mean(), correct_edges / (full_edge_count + 1e-8)\n",
        "# total accuracy is slightly better than percentage of edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXtda7Nk2iBF",
        "outputId": "b4135250-7815-4061-8fcf-1d567694590f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.9575), 0.9727551020404854)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prior**"
      ],
      "metadata": {
        "id": "1sKGcDur274N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prior_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypcH40de27Lj",
        "outputId": "6012b7d3-f7f1-4c6e-edd9-024d0064daaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 49, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_prior  = prior_logits.cpu().detach()\n",
        "edges_logits = posterior_logits.cpu().detach()"
      ],
      "metadata": {
        "id": "aTicInVs3K28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_prior[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyH0DFdn3r6W",
        "outputId": "f8be05bf-1ac9-49e2-d8d4-b141b8e29dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 6.1125, -5.4729],\n",
              "         [ 5.7644, -4.8734],\n",
              "         [ 3.6876, -2.9060],\n",
              "         [ 2.6849, -2.4115],\n",
              "         [ 4.1094, -3.3937],\n",
              "         [ 4.1447, -3.9342]],\n",
              "\n",
              "        [[ 6.6808, -6.0281],\n",
              "         [ 4.7721, -3.8887],\n",
              "         [ 5.4189, -4.3171],\n",
              "         [ 2.9722, -2.6285],\n",
              "         [ 5.8479, -4.8753],\n",
              "         [ 5.6949, -5.6019]],\n",
              "\n",
              "        [[ 6.8471, -6.2130],\n",
              "         [ 3.7649, -3.0543],\n",
              "         [ 6.2189, -4.9743],\n",
              "         [ 2.9154, -2.5099],\n",
              "         [ 6.7702, -5.6686],\n",
              "         [ 6.6179, -6.5407]],\n",
              "\n",
              "        [[ 6.8110, -6.2033],\n",
              "         [ 3.0460, -2.5061],\n",
              "         [ 6.5764, -5.2680],\n",
              "         [ 2.7103, -2.2793],\n",
              "         [ 7.2703, -6.0961],\n",
              "         [ 7.1441, -7.0375]],\n",
              "\n",
              "        [[ 6.5614, -5.9914],\n",
              "         [ 2.5855, -2.1727],\n",
              "         [ 6.7609, -5.4225],\n",
              "         [ 2.4276, -2.0005],\n",
              "         [ 7.5855, -6.3669],\n",
              "         [ 7.4517, -7.3107]],\n",
              "\n",
              "        [[ 6.1491, -5.6265],\n",
              "         [ 2.2776, -1.9550],\n",
              "         [ 6.8812, -5.5261],\n",
              "         [ 2.1111, -1.7027],\n",
              "         [ 7.8056, -6.5590],\n",
              "         [ 7.6340, -7.4648]],\n",
              "\n",
              "        [[ 5.6429, -5.1754],\n",
              "         [ 2.0476, -1.7888],\n",
              "         [ 6.9829, -5.6162],\n",
              "         [ 1.7947, -1.4071],\n",
              "         [ 7.9632, -6.7005],\n",
              "         [ 7.7399, -7.5497]],\n",
              "\n",
              "        [[ 5.0997, -4.6918],\n",
              "         [ 1.8938, -1.6716],\n",
              "         [ 7.0924, -5.7159],\n",
              "         [ 1.6237, -1.2237],\n",
              "         [ 8.0796, -6.8083],\n",
              "         [ 7.7931, -7.5917]],\n",
              "\n",
              "        [[ 4.5714, -4.2246],\n",
              "         [ 1.7852, -1.5811],\n",
              "         [ 7.2174, -5.8325],\n",
              "         [ 1.6230, -1.1517],\n",
              "         [ 8.1709, -6.8959],\n",
              "         [ 7.7913, -7.5875]],\n",
              "\n",
              "        [[ 4.1049, -3.8187],\n",
              "         [ 1.6731, -1.4945],\n",
              "         [ 7.3506, -5.9570],\n",
              "         [ 1.5544, -1.0038],\n",
              "         [ 8.2437, -6.9696],\n",
              "         [ 7.7290, -7.5286]],\n",
              "\n",
              "        [[ 3.7266, -3.4991],\n",
              "         [ 1.5429, -1.4088],\n",
              "         [ 7.4688, -6.0642],\n",
              "         [ 1.1048, -0.5322],\n",
              "         [ 8.2956, -7.0271],\n",
              "         [ 7.6321, -7.4375]],\n",
              "\n",
              "        [[ 3.4317, -3.2579],\n",
              "         [ 1.4129, -1.3261],\n",
              "         [ 7.5510, -6.1336],\n",
              "         [ 0.3398,  0.2029],\n",
              "         [ 8.3188, -7.0624],\n",
              "         [ 7.5412, -7.3490]],\n",
              "\n",
              "        [[ 3.2039, -3.0766],\n",
              "         [ 1.3131, -1.2587],\n",
              "         [ 7.6298, -6.2060],\n",
              "         [-0.0273,  0.5699],\n",
              "         [ 8.3112, -7.0719],\n",
              "         [ 7.5073, -7.3070]],\n",
              "\n",
              "        [[ 3.0283, -2.9390],\n",
              "         [ 1.2525, -1.2166],\n",
              "         [ 7.7366, -6.3155],\n",
              "         [-0.2899,  0.8278],\n",
              "         [ 8.2720, -7.0515],\n",
              "         [ 7.5369, -7.3199]],\n",
              "\n",
              "        [[ 2.8953, -2.8367],\n",
              "         [ 1.2309, -1.2107],\n",
              "         [ 7.8343, -6.4209],\n",
              "         [-0.4846,  1.0025],\n",
              "         [ 8.1861, -6.9834],\n",
              "         [ 7.6079, -7.3714]],\n",
              "\n",
              "        [[ 2.7928, -2.7583],\n",
              "         [ 1.2679, -1.2642],\n",
              "         [ 7.8834, -6.4872],\n",
              "         [-0.5700,  1.0640],\n",
              "         [ 8.0401, -6.8581],\n",
              "         [ 7.6926, -7.4383]],\n",
              "\n",
              "        [[ 2.7091, -2.6937],\n",
              "         [ 1.3950, -1.4042],\n",
              "         [ 7.8391, -6.4931],\n",
              "         [-0.5553,  1.0300],\n",
              "         [ 7.8488, -6.7050],\n",
              "         [ 7.7658, -7.4993]],\n",
              "\n",
              "        [[ 2.6364, -2.6388],\n",
              "         [ 1.6097, -1.6258],\n",
              "         [ 7.6165, -6.4075],\n",
              "         [-0.4736,  0.9314],\n",
              "         [ 7.6342, -6.5712],\n",
              "         [ 7.8205, -7.5481]],\n",
              "\n",
              "        [[ 2.5615, -2.5894],\n",
              "         [ 1.9391, -1.9605],\n",
              "         [ 7.1654, -6.1821],\n",
              "         [-0.3423,  0.7772],\n",
              "         [ 7.4149, -6.4793],\n",
              "         [ 7.8617, -7.5887]],\n",
              "\n",
              "        [[ 2.4763, -2.5461],\n",
              "         [ 2.4881, -2.5197],\n",
              "         [ 6.6419, -5.8766],\n",
              "         [-0.1535,  0.5548],\n",
              "         [ 7.2343, -6.4303],\n",
              "         [ 7.8907, -7.6212]],\n",
              "\n",
              "        [[ 2.3984, -2.5210],\n",
              "         [ 3.1627, -3.2147],\n",
              "         [ 6.2358, -5.6402],\n",
              "         [ 0.1011,  0.2525],\n",
              "         [ 7.1202, -6.4287],\n",
              "         [ 7.9096, -7.6471]],\n",
              "\n",
              "        [[ 2.3661, -2.5366],\n",
              "         [ 3.7223, -3.8078],\n",
              "         [ 5.9760, -5.5065],\n",
              "         [ 0.4294, -0.1314],\n",
              "         [ 7.0496, -6.4564],\n",
              "         [ 7.9202, -7.6673]],\n",
              "\n",
              "        [[ 2.4176, -2.6205],\n",
              "         [ 4.0872, -4.2266],\n",
              "         [ 5.8035, -5.4349],\n",
              "         [ 0.8346, -0.5883],\n",
              "         [ 6.9909, -6.4841],\n",
              "         [ 7.9249, -7.6839]],\n",
              "\n",
              "        [[ 2.5803, -2.8008],\n",
              "         [ 4.3557, -4.5702],\n",
              "         [ 5.7022, -5.4144],\n",
              "         [ 1.3230, -1.1192],\n",
              "         [ 6.9233, -6.4918],\n",
              "         [ 7.9248, -7.6974]],\n",
              "\n",
              "        [[ 2.8534, -3.0793],\n",
              "         [ 4.7327, -5.0229],\n",
              "         [ 5.6715, -5.4451],\n",
              "         [ 1.8957, -1.7277],\n",
              "         [ 6.8364, -6.4728],\n",
              "         [ 7.9196, -7.7079]],\n",
              "\n",
              "        [[ 3.2046, -3.4133],\n",
              "         [ 5.3824, -5.6888],\n",
              "         [ 5.6988, -5.5170],\n",
              "         [ 2.5236, -2.3916],\n",
              "         [ 6.7263, -6.4265],\n",
              "         [ 7.9097, -7.7152]],\n",
              "\n",
              "        [[ 3.6206, -3.7776],\n",
              "         [ 6.2095, -6.4287],\n",
              "         [ 5.7598, -5.6126],\n",
              "         [ 3.1452, -3.0493],\n",
              "         [ 6.6021, -6.3611],\n",
              "         [ 7.8947, -7.7189]],\n",
              "\n",
              "        [[ 4.2181, -4.2822],\n",
              "         [ 7.0169, -7.0900],\n",
              "         [ 5.8355, -5.7231],\n",
              "         [ 3.6981, -3.6377],\n",
              "         [ 6.4775, -6.2882],\n",
              "         [ 7.8749, -7.7191]],\n",
              "\n",
              "        [[ 5.2427, -5.1824],\n",
              "         [ 7.6803, -7.6073],\n",
              "         [ 5.9215, -5.8544],\n",
              "         [ 4.1503, -4.1265],\n",
              "         [ 6.3665, -6.2193],\n",
              "         [ 7.8498, -7.7150]],\n",
              "\n",
              "        [[ 6.4899, -6.2947],\n",
              "         [ 8.1672, -7.9731],\n",
              "         [ 6.0323, -6.0190],\n",
              "         [ 4.4997, -4.5127],\n",
              "         [ 6.2811, -6.1650],\n",
              "         [ 7.8179, -7.7053]],\n",
              "\n",
              "        [[ 7.5082, -7.1622],\n",
              "         [ 8.5000, -8.2106],\n",
              "         [ 6.1823, -6.2104],\n",
              "         [ 4.7630, -4.8120],\n",
              "         [ 6.2290, -6.1328],\n",
              "         [ 7.7778, -7.6882]],\n",
              "\n",
              "        [[ 8.2863, -7.7773],\n",
              "         [ 8.7189, -8.3533],\n",
              "         [ 6.3615, -6.4039],\n",
              "         [ 4.9631, -5.0453],\n",
              "         [ 6.2127, -6.1267],\n",
              "         [ 7.7284, -7.6622]],\n",
              "\n",
              "        [[ 8.9237, -8.2406],\n",
              "         [ 8.8614, -8.4322],\n",
              "         [ 6.5402, -6.5758],\n",
              "         [ 5.1194, -5.2306],\n",
              "         [ 6.2318, -6.1478],\n",
              "         [ 7.6693, -7.6268]],\n",
              "\n",
              "        [[ 9.4423, -8.6095],\n",
              "         [ 8.9517, -8.4701],\n",
              "         [ 6.6976, -6.7154],\n",
              "         [ 5.2447, -5.3809],\n",
              "         [ 6.2862, -6.1976],\n",
              "         [ 7.6013, -7.5819]],\n",
              "\n",
              "        [[ 9.7633, -8.8489],\n",
              "         [ 9.0119, -8.4898],\n",
              "         [ 6.8231, -6.8190],\n",
              "         [ 5.3480, -5.5061],\n",
              "         [ 6.3797, -6.2818],\n",
              "         [ 7.5217, -7.5245]],\n",
              "\n",
              "        [[ 9.8232, -8.9053],\n",
              "         [ 9.0569, -8.5082],\n",
              "         [ 6.9183, -6.8910],\n",
              "         [ 5.4352, -5.6133],\n",
              "         [ 6.5244, -6.4141],\n",
              "         [ 7.4243, -7.4478]],\n",
              "\n",
              "        [[ 9.7584, -8.8753],\n",
              "         [ 9.0919, -8.5336],\n",
              "         [ 6.9899, -6.9399],\n",
              "         [ 5.5106, -5.7068],\n",
              "         [ 6.7372, -6.6128],\n",
              "         [ 7.3034, -7.3453]],\n",
              "\n",
              "        [[ 9.7441, -8.8820],\n",
              "         [ 9.1151, -8.5646],\n",
              "         [ 7.0509, -6.9809],\n",
              "         [ 5.5820, -5.7936],\n",
              "         [ 7.0184, -6.8797],\n",
              "         [ 7.1553, -7.2146]],\n",
              "\n",
              "        [[ 9.7849, -8.9274],\n",
              "         [ 9.1121, -8.5795],\n",
              "         [ 7.1387, -7.0605],\n",
              "         [ 5.6589, -5.8806],\n",
              "         [ 7.3305, -7.1782],\n",
              "         [ 6.9844, -7.0617]],\n",
              "\n",
              "        [[ 9.8417, -8.9837],\n",
              "         [ 9.0491, -8.5375],\n",
              "         [ 7.3307, -7.2777],\n",
              "         [ 5.7514, -5.9762],\n",
              "         [ 7.6215, -7.4561],\n",
              "         [ 6.8057, -6.9027]],\n",
              "\n",
              "        [[ 9.8956, -9.0371],\n",
              "         [ 8.8952, -8.4056],\n",
              "         [ 7.5637, -7.5529],\n",
              "         [ 5.8652, -6.0852],\n",
              "         [ 7.8646, -7.6866],\n",
              "         [ 6.6380, -6.7546]],\n",
              "\n",
              "        [[ 9.9393, -9.0817],\n",
              "         [ 8.6639, -8.1936],\n",
              "         [ 7.6615, -7.6615],\n",
              "         [ 5.9962, -6.2057],\n",
              "         [ 8.0622, -7.8713],\n",
              "         [ 6.4921, -6.6239]],\n",
              "\n",
              "        [[ 9.9687, -9.1137],\n",
              "         [ 8.4060, -7.9474],\n",
              "         [ 7.6611, -7.6531],\n",
              "         [ 6.1407, -6.3371],\n",
              "         [ 8.2273, -8.0233],\n",
              "         [ 6.3668, -6.5041]],\n",
              "\n",
              "        [[ 9.9796, -9.1288],\n",
              "         [ 8.1630, -7.7091],\n",
              "         [ 7.6160, -7.5977],\n",
              "         [ 6.3028, -6.4843],\n",
              "         [ 8.3727, -8.1554],\n",
              "         [ 6.2562, -6.3857]],\n",
              "\n",
              "        [[ 9.9684, -9.1231],\n",
              "         [ 7.9486, -7.4958],\n",
              "         [ 7.5432, -7.5167],\n",
              "         [ 6.4907, -6.6541],\n",
              "         [ 8.5080, -8.2772],\n",
              "         [ 6.1573, -6.2674]],\n",
              "\n",
              "        [[ 9.9345, -9.0955],\n",
              "         [ 7.7682, -7.3164],\n",
              "         [ 7.4480, -7.4154],\n",
              "         [ 6.7064, -6.8454],\n",
              "         [ 8.6389, -8.3944],\n",
              "         [ 6.0750, -6.1625]],\n",
              "\n",
              "        [[ 9.8819, -9.0502],\n",
              "         [ 7.6292, -7.1800],\n",
              "         [ 7.3344, -7.2974],\n",
              "         [ 6.9403, -7.0451],\n",
              "         [ 8.7677, -8.5096],\n",
              "         [ 6.0184, -6.0870]],\n",
              "\n",
              "        [[ 9.8152, -8.9913],\n",
              "         [ 7.5604, -7.1111],\n",
              "         [ 7.2058, -7.1656],\n",
              "         [ 7.1837, -7.2379],\n",
              "         [ 8.8946, -8.6231],\n",
              "         [ 5.9895, -6.0462]],\n",
              "\n",
              "        [[ 9.7391, -8.9228],\n",
              "         [ 7.6211, -7.1576],\n",
              "         [ 7.0656, -7.0232],\n",
              "         [ 7.4632, -7.4350],\n",
              "         [ 9.0187, -8.7340],\n",
              "         [ 5.9844, -6.0375]]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_logits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prg7FMzJ32u_",
        "outputId": "5e368e18-df60-4699-820c-df91de7ccd89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 13.3790, -14.6843],\n",
              "         [ 13.6014, -14.8915],\n",
              "         [  8.7523, -10.5359],\n",
              "         [ 11.1843, -12.9371],\n",
              "         [  7.2392,  -8.6253],\n",
              "         [  9.6848, -10.8599]],\n",
              "\n",
              "        [[ 12.4564, -13.3799],\n",
              "         [ 12.0721, -12.8575],\n",
              "         [ 11.3971, -13.6147],\n",
              "         [ 10.6036, -12.2012],\n",
              "         [ 10.7372, -12.7225],\n",
              "         [ 10.6722, -11.9837]],\n",
              "\n",
              "        [[ 11.7539, -12.4410],\n",
              "         [ 10.9703, -11.4578],\n",
              "         [ 12.4171, -14.7908],\n",
              "         [ 10.0686, -11.6174],\n",
              "         [ 12.2821, -14.5651],\n",
              "         [ 11.0382, -12.3706]],\n",
              "\n",
              "        [[ 11.2123, -11.7430],\n",
              "         [ 10.1626, -10.4446],\n",
              "         [ 12.8468, -15.2798],\n",
              "         [  9.7027, -11.2710],\n",
              "         [ 13.0808, -15.5248],\n",
              "         [ 11.1292, -12.4421]],\n",
              "\n",
              "        [[ 10.7374, -11.1560],\n",
              "         [  9.5087,  -9.6158],\n",
              "         [ 13.0267, -15.4760],\n",
              "         [  9.5223, -11.1433],\n",
              "         [ 13.5489, -16.0851],\n",
              "         [ 11.1029, -12.3817]],\n",
              "\n",
              "        [[ 10.2857, -10.6149],\n",
              "         [  8.9056,  -8.8403],\n",
              "         [ 13.0927, -15.5357],\n",
              "         [  9.5143, -11.2114],\n",
              "         [ 13.8394, -16.4288],\n",
              "         [ 11.0144, -12.2521]],\n",
              "\n",
              "        [[  9.8406, -10.0903],\n",
              "         [  8.2892,  -8.0428],\n",
              "         [ 13.1048, -15.5293],\n",
              "         [  9.5587, -11.3322],\n",
              "         [ 14.0186, -16.6372],\n",
              "         [ 10.8780, -12.0691]],\n",
              "\n",
              "        [[  9.3921,  -9.5659],\n",
              "         [  7.5903,  -7.1535],\n",
              "         [ 13.0925, -15.4914],\n",
              "         [  9.1612, -10.9028],\n",
              "         [ 14.1228, -16.7540],\n",
              "         [ 10.6604, -11.7954]],\n",
              "\n",
              "        [[  8.9420,  -9.0415],\n",
              "         [  6.7962,  -6.1740],\n",
              "         [ 13.0642, -15.4315],\n",
              "         [  8.3643,  -9.9758],\n",
              "         [ 14.1615, -16.7905],\n",
              "         [ 10.3659, -11.4357]],\n",
              "\n",
              "        [[  8.5038,  -8.5332],\n",
              "         [  6.0272,  -5.2574],\n",
              "         [ 13.0111, -15.3396],\n",
              "         [  7.3656,  -8.7977],\n",
              "         [ 14.1381, -16.7517],\n",
              "         [ 10.0407, -11.0419]],\n",
              "\n",
              "        [[  8.0867,  -8.0513],\n",
              "         [  5.4435,  -4.5875],\n",
              "         [ 12.9035, -15.1816],\n",
              "         [  6.1162,  -7.3122],\n",
              "         [ 14.0590, -16.6467],\n",
              "         [  9.7434, -10.6774]],\n",
              "\n",
              "        [[  7.7067,  -7.6145],\n",
              "         [  5.1131,  -4.2276],\n",
              "         [ 12.7189, -14.9318],\n",
              "         [  4.5713,  -5.4573],\n",
              "         [ 13.9318, -16.4862],\n",
              "         [  9.5367, -10.4133]],\n",
              "\n",
              "        [[  7.3651,  -7.2232],\n",
              "         [  5.0052,  -4.1286],\n",
              "         [ 12.5296, -14.6738],\n",
              "         [  3.7172,  -4.4953],\n",
              "         [ 13.7713, -16.2904],\n",
              "         [  9.4404, -10.2755]],\n",
              "\n",
              "        [[  7.0551,  -6.8667],\n",
              "         [  5.0364,  -4.1872],\n",
              "         [ 12.4034, -14.4839],\n",
              "         [  3.3790,  -4.1865],\n",
              "         [ 13.5801, -16.0647],\n",
              "         [  9.4094, -10.2123]],\n",
              "\n",
              "        [[  6.7712,  -6.5363],\n",
              "         [  5.1219,  -4.3054],\n",
              "         [ 12.2872, -14.2983],\n",
              "         [  3.3030,  -4.1977],\n",
              "         [ 13.3433, -15.7914],\n",
              "         [  9.3936, -10.1652]],\n",
              "\n",
              "        [[  6.5016,  -6.2185],\n",
              "         [  5.2186,  -4.4353],\n",
              "         [ 12.1189, -14.0432],\n",
              "         [  3.3835,  -4.3892],\n",
              "         [ 13.0506, -15.4561],\n",
              "         [  9.3694, -10.1069]],\n",
              "\n",
              "        [[  6.2350,  -5.9020],\n",
              "         [  5.2891,  -4.5333],\n",
              "         [ 11.8457, -13.6591],\n",
              "         [  3.5082,  -4.6118],\n",
              "         [ 12.7197, -15.0706],\n",
              "         [  9.3256, -10.0239]],\n",
              "\n",
              "        [[  5.9723,  -5.5909],\n",
              "         [  5.3618,  -4.6336],\n",
              "         [ 11.3475, -13.0081],\n",
              "         [  3.6224,  -4.7963],\n",
              "         [ 12.3389, -14.6148],\n",
              "         [  9.2692,  -9.9256]],\n",
              "\n",
              "        [[  5.7122,  -5.2861],\n",
              "         [  5.5425,  -4.8621],\n",
              "         [ 10.7536, -12.2452],\n",
              "         [  3.7356,  -4.9564],\n",
              "         [ 11.9322, -14.1203],\n",
              "         [  9.2145,  -9.8307]],\n",
              "\n",
              "        [[  5.4570,  -4.9937],\n",
              "         [  5.8869,  -5.2905],\n",
              "         [ 10.4745, -11.8503],\n",
              "         [  3.8458,  -5.0970],\n",
              "         [ 11.6513, -13.7630],\n",
              "         [  9.1628,  -9.7407]],\n",
              "\n",
              "        [[  5.2458,  -4.7575],\n",
              "         [  6.2791,  -5.7991],\n",
              "         [ 10.4153, -11.7237],\n",
              "         [  3.9575,  -5.2276],\n",
              "         [ 11.5629, -13.6147],\n",
              "         [  9.1167,  -9.6587]],\n",
              "\n",
              "        [[  5.1555,  -4.6613],\n",
              "         [  6.5248,  -6.1655],\n",
              "         [ 10.3601, -11.6233],\n",
              "         [  4.0803,  -5.3606],\n",
              "         [ 11.5145, -13.5051],\n",
              "         [  9.0799,  -9.5902]],\n",
              "\n",
              "        [[  5.2653,  -4.7935],\n",
              "         [  6.6142,  -6.3736],\n",
              "         [ 10.2742, -11.5028],\n",
              "         [  4.2291,  -5.5119],\n",
              "         [ 11.4043, -13.3225],\n",
              "         [  9.0579,  -9.5427]],\n",
              "\n",
              "        [[  5.6168,  -5.2047],\n",
              "         [  6.7362,  -6.6332],\n",
              "         [ 10.1785, -11.3790],\n",
              "         [  4.4280,  -5.7061],\n",
              "         [ 11.2425, -13.0813],\n",
              "         [  9.0549,  -9.5230]],\n",
              "\n",
              "        [[  6.1635,  -5.8475],\n",
              "         [  7.1968,  -7.2966],\n",
              "         [ 10.0927, -11.2687],\n",
              "         [  4.7041,  -5.9736],\n",
              "         [ 11.0373, -12.7931],\n",
              "         [  9.0741,  -9.5371]],\n",
              "\n",
              "        [[  6.7652,  -6.5719],\n",
              "         [  7.9374,  -8.2965],\n",
              "         [ 10.0243, -11.1780],\n",
              "         [  5.0618,  -6.3262],\n",
              "         [ 10.7933, -12.4642],\n",
              "         [  9.1164,  -9.5879]],\n",
              "\n",
              "        [[  7.3049,  -7.2591],\n",
              "         [  8.5804,  -9.1833],\n",
              "         [  9.9753, -11.1092],\n",
              "         [  5.4685,  -6.7375],\n",
              "         [ 10.5242, -12.1114],\n",
              "         [  9.1799,  -9.6736]],\n",
              "\n",
              "        [[  7.8092,  -7.9697],\n",
              "         [  9.1674,  -9.9795],\n",
              "         [  9.9573, -11.0754],\n",
              "         [  5.8675,  -7.1479],\n",
              "         [ 10.2432, -11.7499],\n",
              "         [  9.2594,  -9.7853]],\n",
              "\n",
              "        [[  8.4233,  -8.9120],\n",
              "         [  9.6766, -10.6534],\n",
              "         [  9.9972, -11.1094],\n",
              "         [  6.1906,  -7.4759],\n",
              "         [  9.9600, -11.3907],\n",
              "         [  9.3496,  -9.9136]],\n",
              "\n",
              "        [[  9.0645,  -9.9107],\n",
              "         [  9.9861, -11.0616],\n",
              "         [ 10.1045, -11.2245],\n",
              "         [  6.3759,  -7.6405],\n",
              "         [  9.6846, -11.0451],\n",
              "         [  9.4540, -10.0610]],\n",
              "\n",
              "        [[  9.5610, -10.6448],\n",
              "         [ 10.0469, -11.1361],\n",
              "         [ 10.2266, -11.3637],\n",
              "         [  6.3837,  -7.5857],\n",
              "         [  9.4252, -10.7232],\n",
              "         [  9.5870, -10.2468]],\n",
              "\n",
              "        [[  9.9955, -11.2246],\n",
              "         [ 10.0383, -11.1156],\n",
              "         [ 10.3036, -11.4584],\n",
              "         [  6.2366,  -7.3391],\n",
              "         [  9.1862, -10.4295],\n",
              "         [  9.7575, -10.4842]],\n",
              "\n",
              "        [[ 10.4474, -11.7797],\n",
              "         [ 10.1625, -11.2846],\n",
              "         [ 10.3262, -11.4945],\n",
              "         [  6.0628,  -7.0693],\n",
              "         [  8.9701, -10.1666],\n",
              "         [  9.9473, -10.7486]],\n",
              "\n",
              "        [[ 10.8497, -12.2551],\n",
              "         [ 10.3436, -11.5454],\n",
              "         [ 10.3110, -11.4884],\n",
              "         [  5.9738,  -6.9223],\n",
              "         [  8.7808,  -9.9391],\n",
              "         [ 10.1130, -10.9787]],\n",
              "\n",
              "        [[ 11.1130, -12.5544],\n",
              "         [ 10.5039, -11.7932],\n",
              "         [ 10.2756, -11.4601],\n",
              "         [  5.9468,  -6.8704],\n",
              "         [  8.6194,  -9.7492],\n",
              "         [ 10.2226, -11.1282]],\n",
              "\n",
              "        [[ 11.2231, -12.6600],\n",
              "         [ 10.6214, -11.9998],\n",
              "         [ 10.2368, -11.4329],\n",
              "         [  5.9158,  -6.8300],\n",
              "         [  8.4874,  -9.5998],\n",
              "         [ 10.2708, -11.1896]],\n",
              "\n",
              "        [[ 11.2457, -12.6605],\n",
              "         [ 10.6868, -12.1552],\n",
              "         [ 10.2096, -11.4288],\n",
              "         [  5.8378,  -6.7492],\n",
              "         [  8.3854,  -9.4945],\n",
              "         [ 10.2651, -11.1737]],\n",
              "\n",
              "        [[ 11.2670, -12.6668],\n",
              "         [ 10.6896, -12.2470],\n",
              "         [ 10.1961, -11.4512],\n",
              "         [  5.7396,  -6.6523],\n",
              "         [  8.3115,  -9.4309],\n",
              "         [ 10.2138, -11.0936]],\n",
              "\n",
              "        [[ 11.2988, -12.6903],\n",
              "         [ 10.6184, -12.2583],\n",
              "         [ 10.1933, -11.4896],\n",
              "         [  5.6907,  -6.6060],\n",
              "         [  8.2600,  -9.3950],\n",
              "         [ 10.1268, -10.9673]],\n",
              "\n",
              "        [[ 11.3232, -12.7091],\n",
              "         [ 10.4727, -12.1825],\n",
              "         [ 10.2246, -11.5621],\n",
              "         [  5.7163,  -6.6352],\n",
              "         [  8.2248,  -9.3705],\n",
              "         [ 10.0186, -10.8191]],\n",
              "\n",
              "        [[ 11.3225, -12.7029],\n",
              "         [ 10.2750, -12.0423],\n",
              "         [ 10.2468, -11.6148],\n",
              "         [  5.8060,  -6.7344],\n",
              "         [  8.2019,  -9.3495],\n",
              "         [  9.9004, -10.6673]],\n",
              "\n",
              "        [[ 11.2793, -12.6518],\n",
              "         [ 10.0794, -11.8968],\n",
              "         [ 10.1704, -11.5455],\n",
              "         [  5.9378,  -6.8857],\n",
              "         [  8.1895,  -9.3314],\n",
              "         [  9.7720, -10.5137]],\n",
              "\n",
              "        [[ 11.1894, -12.5516],\n",
              "         [  9.9500, -11.8186],\n",
              "         [ 10.0272, -11.3980],\n",
              "         [  6.0884,  -7.0653],\n",
              "         [  8.1872,  -9.3187],\n",
              "         [  9.6207, -10.3432]],\n",
              "\n",
              "        [[ 11.0641, -12.4161],\n",
              "         [  9.9243, -11.8500],\n",
              "         [  9.8481, -11.2087],\n",
              "         [  6.2487,  -7.2597],\n",
              "         [  8.1936,  -9.3123],\n",
              "         [  9.4254, -10.1310]],\n",
              "\n",
              "        [[ 10.9044, -12.2447],\n",
              "         [ 10.0052, -11.9935],\n",
              "         [  9.6357, -10.9793],\n",
              "         [  6.4133,  -7.4543],\n",
              "         [  8.2052,  -9.3096],\n",
              "         [  9.1594,  -9.8442]],\n",
              "\n",
              "        [[ 10.6746, -11.9892],\n",
              "         [ 10.1627, -12.2108],\n",
              "         [  9.3760, -10.6925],\n",
              "         [  6.5602,  -7.6157],\n",
              "         [  8.2154,  -9.3037],\n",
              "         [  8.7919,  -9.4428]],\n",
              "\n",
              "        [[ 10.3039, -11.5541],\n",
              "         [ 10.3370, -12.4188],\n",
              "         [  9.0342, -10.3062],\n",
              "         [  6.6490,  -7.6976],\n",
              "         [  8.2153,  -9.2852],\n",
              "         [  8.2687,  -8.8600]],\n",
              "\n",
              "        [[  9.6530, -10.7611],\n",
              "         [ 10.4344, -12.4709],\n",
              "         [  8.5360,  -9.7297],\n",
              "         [  6.6276,  -7.6359],\n",
              "         [  8.2036,  -9.2550],\n",
              "         [  7.4325,  -7.9104]],\n",
              "\n",
              "        [[  8.2353,  -9.0200],\n",
              "         [ 10.2808, -12.0776],\n",
              "         [  7.7617,  -8.8234],\n",
              "         [  6.4589,  -7.3479],\n",
              "         [  8.2349,  -9.2824],\n",
              "         [  5.7950,  -6.0314]]])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ep = np.argmax(edges_prior, axis=-1)\n",
        "ec = np.argmax(edges_logits, axis=-1)"
      ],
      "metadata": {
        "id": "mZoCyVJN3NOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ep[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW2ttq3_38gT",
        "outputId": "cbf088b1-e595-4972-ce90-40600d2cd905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ec[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09EPacu44ARh",
        "outputId": "0dd02028-50b1-4c55-cda7-a67fc84b1acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "el = torch.nn.functional.gumbel_softmax(edges_logits, tau=model.gumbel_temp/2, hard=True,).cpu().numpy()"
      ],
      "metadata": {
        "id": "DnbOdWRw3TZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "el[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B81KZc94DiB",
        "outputId": "014dd691-dc45-4275-d06c-ca56ebebf125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_t = edges_true_"
      ],
      "metadata": {
        "id": "pGPRI8Gc3Vej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_t[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cviQrWrn4Hdu",
        "outputId": "154d6574-4f1f-40df-bff6-908d25c4e6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_t.shape, ep.shape, ec.shape, el.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq55Mqkz3YAx",
        "outputId": "1db264c3-e37c-4eac-9d51-109d3fb40954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 49, 6]),\n",
              " torch.Size([100, 49, 6]),\n",
              " torch.Size([100, 49, 6]),\n",
              " (100, 49, 6, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edges_t[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-TGCm4Q3agt",
        "outputId": "a947efdc-3e01-4632-bad3-684d6601b25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ep[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TPutohf4X6q",
        "outputId": "5ec9bf98-0299-4e14-bd53-531a585d9d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ec[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBPxzP-54a-D",
        "outputId": "e3d383fd-9417-4296-d2fe-c32dfee926a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "el[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAObXY_g4eA-",
        "outputId": "9941704b-2a3a-4ef8-8cd2-040a1e2dc8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [0., 1.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]],\n",
              "\n",
              "       [[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "uWGjb_zq3etf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=1\n",
        "plt.subplot(141)\n",
        "plt.imshow(edges_t[i].cpu().numpy())\n",
        "\n",
        "plt.subplot(142)\n",
        "plt.imshow(ep[i])\n",
        "\n",
        "plt.subplot(143)\n",
        "plt.imshow(ec[i]);\n",
        "plt.subplot(144)\n",
        "plt.imshow(el[i,...,1]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "rw7Ycfu_3hwZ",
        "outputId": "38ae660a-8d0d-416e-ffa9-514637424fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i=3 # timeseries sample\n",
        "fig, axs = plt.subplots(1,4, sharey=True,\n",
        "                        figsize=(12,12), gridspec_kw=dict(wspace=0))\n",
        "axs[0].imshow(edges_t[i,].cpu().numpy().T,aspect='auto', vmin=0,vmax=1)\n",
        "axs[0].set_xlabel('Time'); axs[0].set_ylabel('Edge\\npair', rotation=0, ha='right')\n",
        "axs[0].set_title('Ground truth')\n",
        "\n",
        "axs[1].imshow(ep[i].T,aspect='auto', vmin=0,vmax=1)\n",
        "axs[1].set_title('Prior')\n",
        "\n",
        "axs[2].imshow(ec[i].T,aspect='auto', vmin=0,vmax=1);\n",
        "axs[2].set_title('Edges argmax')\n",
        "\n",
        "axs[3].imshow(el[i,...,1].T,aspect='auto', vmin=0,vmax=1);\n",
        "axs[3].set_title('Edges Gumbel');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dTC5ZX8I3oig",
        "outputId": "b9e1990c-e988-48b2-f0be-cdd224b2ffac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAGiCAYAAADTBw0VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcOElEQVR4nO3df5BV9X3/8dcuxgV1d4OIirL8spkAJQkKSBtMiw0TzOSPYizJtJgJjENigr9iZiI0nZC2GTctpiUxjsF0hmTSWh1jMK2JSRgzYprqVyNJJyaAZRyHzQoIau5Soquy9/vHTrahIuwCd+/nwuMxc0b33HPveXNm9T4599x7m6rVajUAAAVorvcAAAC/JUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYtQ8TLq7u3PllVdmzJgxGTVqVN72trflJz/5Sa13CwA0oFNq+eAvvvhi5s2bl0svvTQPPPBAxo4dm//+7//O6NGja7lbAKBBNdXyS/xWrlyZH//4x/nRj35Uq10AACeQmobJ9OnTs3DhwvzqV7/Kpk2bcv755+fjH/94li9ffsjte3t709vbO/BzX19fXnjhhYwZMyZNTU21GhMAOI6q1Wr27duX8847L83NQ7xqpFpDLS0t1ZaWluqqVauqmzdvrq5bt646cuTI6te+9rVDbr969epqEovFYrFYLCfA0tXVNeR2qOkZk1NPPTWzZ8/Of/7nfw6su+666/L444/nkUceed32//eMSaVSyYQJE9LV1ZW2trZajQkAHEc9PT3p6OjIr3/967S3tw/pvjW9+HXcuHGZPn36QeumTZuWe++995Dbt7S0pKWl5XXr29rahAkANJijuQyjpm8XnjdvXrZt23bQuqeeeioTJ06s5W4BgAZV0zD5xCc+kUcffTQ333xztm/fnjvvvDN33HFHVqxYUcvdAgANqqZhMmfOnGzYsCH/+q//mhkzZuRv//Zvs3bt2ixZsqSWuwUAGlRNL349Vj09PWlvb0+lUnGNCVCEvmpfdlR2ZF/vvrS2tGZC+4Q0N/l2D/hdx/L8XdOLXwFOJFv2bMmGrRuyde/WvPzayxl5yshMPWtqLp96eaaNnVbv8eCEIEwABmHLni350v/7Uvb+Zm862jty+ptOz/5X9+enO3+arkpXrpt7nTiB48D5R4Aj6Kv2ZcPWDdn7m72ZPnZ62lraMqJ5RNpa2jJ97PTs/c3e3Lf1vvRV++o9KjQ8YQJwBDsqO7J179Z0tHe87nMZmpqaMr5tfLbs3ZIdlR11mhBOHMIE4Aj29e7Ly6+9nNPfdPohbz/91NPz8msvZ1/vvmGeDE48wgTgCFpbWjPylJHZ/+r+Q96+/5X9GXnKyLS2tA7zZHDiESYARzChfUKmnjU1XZWu/N9PWKhWq/lVz68y7axpmdA+oU4TwolDmAAcQXNTcy6fennOOu2s/HLPL1N5uZLX+l5L5eVKfrnnlznrtLOyaOoin2cCx4H/igAGYdrYablu7nW5cNyFef6l5/PU80/l+Zeez0XjLvJWYTiOfI4JwCBNGzstbz3rrT75FWpImAAMQXNTcya9eVK9x4ATlswHAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKMawhcnnP//5NDU15YYbbhiuXQIADWZYwuTxxx/PunXr8va3v304dgcANKiah8n//M//ZMmSJfnqV7+a0aNHH3bb3t7e9PT0HLQAACePmofJihUr8r73vS8LFiw44radnZ1pb28fWDo6Omo9HgBQkJqGyV133ZXNmzens7NzUNuvWrUqlUplYOnq6qrleABAYU6p1QN3dXXl+uuvz8aNGzNy5MhB3aelpSUtLS21GgkAKFxTtVqt1uKB77vvvlx++eUZMWLEwLoDBw6kqakpzc3N6e3tPei2Q+np6Ul7e3sqlUra2tpqMSYAcJwdy/N3zc6YvPvd787Pf/7zg9YtW7YsU6dOzU033XTEKAEATj41C5PW1tbMmDHjoHWnn356xowZ87r1AACJT34FAApSszMmh/LQQw8N5+4AgAbjjAkAUAxhAgAUQ5gAAMUQJgBAMYQJAFAMYQIAFEOYAADFECYAQDGECQBQDGECABRDmAAAxRAmAEAxhAkAUAxhAgAU45R6DwBwsuvrS3bsSPbtS1pbkwkTkmZ/beQkJUwA6mjLlmTDhmTr1uTll5ORI5OpU5PLL0+mTav3dDD8NDlAnWzZkqxdm3z/+8mePUlLS3LmmclPf5p86Uv9t8PJxhkTgDro60s+97n+syUvvfS/61tbk8suS/buTe67L3nrW72sw8nFrztAHdxxR3LnnQdHSdJ/nck99/S/rLNlS/+1J3AyESYAw+zAgeSznz38Nps29UfLvn3DMhIUQ5gADLMf/SjZvfvw2+zbl1Qq/S/twMlEmAAMs507B7fdmWf2v3UYTibCBGCYjRs3uO3e+14XvnLy8SsPMMze9a5k/PikqemNtzn33OTKK4dvJiiFMAEYZiNGJF/8Yv+/v1Gc3HZb/3ZwshEmAHXw/vcn3/xmcv75B6/v6Ejuvbf/djgZ+YA1gDp5//uTP/3T/nfp7NzZf+3Ju97lTAknN2ECUEcjRiTz59d7CiiHl3IAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYNQ2Tzs7OzJkzJ62trTn77LOzaNGibNu2rZa7BAAaWE3DZNOmTVmxYkUeffTRbNy4Ma+++mre8573ZP/+/bXcLQDQoJqq1Wp1uHa2Z8+enH322dm0aVP+6I/+6Ijb9/T0pL29PZVKJW1tbcMwIQBwrI7l+fuUGs10SJVKJUly5plnHvL23t7e9Pb2Dvzc09MzLHMBAGUYtotf+/r6csMNN2TevHmZMWPGIbfp7OxMe3v7wNLR0TFc4wEABRi2l3I+9rGP5YEHHsh//Md/ZPz48Yfc5lBnTDo6OryUAwANpPiXcq655prcf//9efjhh98wSpKkpaUlLS0twzESAFCgmoZJtVrNtddemw0bNuShhx7K5MmTa7k7AKDB1TRMVqxYkTvvvDPf/va309raml27diVJ2tvbM2rUqFruGgBoQDW9xqSpqemQ69evX5+lS5ce8f7eLgwAjafYa0yG8SNSAIATgO/KAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMMSJrfddlsmTZqUkSNHZu7cuXnssceGY7cAQIOpeZjcfffdufHGG7N69eps3rw573jHO7Jw4cI899xztd41ANBgmqrVarWWO5g7d27mzJmTL3/5y0mSvr6+dHR05Nprr83KlSsP2ra3tze9vb0DP/f09KSjoyOVSiVtbW21HBMAOE56enrS3t5+VM/fNT1j8sorr+SJJ57IggUL/neHzc1ZsGBBHnnkkddt39nZmfb29oGlo6OjluMBAIWpaZjs3bs3Bw4cyDnnnHPQ+nPOOSe7du163farVq1KpVIZWLq6umo5HgBQmFPqPcDvamlpSUtLS73HAADqpKZnTM4666yMGDEiu3fvPmj97t27c+6559Zy1wBAA6ppmJx66qmZNWtWHnzwwYF1fX19efDBB/OHf/iHtdw1ANCAav5Szo033pgPf/jDmT17di6++OKsXbs2+/fvz7Jly2q9awCgwdQ8TD74wQ9mz549+cxnPpNdu3Zl5syZ+d73vve6C2IBAGr+OSbH4ljeBw0A1Eexn2MCADAUwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKEbNwuSZZ57JVVddlcmTJ2fUqFG54IILsnr16rzyyiu12iUA0OBOqdUDb926NX19fVm3bl1+7/d+L08++WSWL1+e/fv355ZbbqnVbgGABtZUrVarw7WzNWvW5Pbbb8/TTz89qO17enrS3t6eSqWStra2Gk8HABwPx/L8XbMzJodSqVRy5plnvuHtvb296e3tHfi5p6dnOMYCAAoxbBe/bt++Pbfeems++tGPvuE2nZ2daW9vH1g6OjqGazwAoABDDpOVK1emqanpsMvWrVsPuk93d3cuu+yyLF68OMuXL3/Dx161alUqlcrA0tXVNfQ/EQDQsIZ8jcmePXvy/PPPH3abKVOm5NRTT02SPPvss5k/f37+4A/+IF/72tfS3Dz4FnKNCQA0nmG9xmTs2LEZO3bsoLbt7u7OpZdemlmzZmX9+vVDihIA4ORTs4tfu7u7M3/+/EycODG33HJL9uzZM3DbueeeW6vdAgANrGZhsnHjxmzfvj3bt2/P+PHjD7ptGN+hDAA0kJq9trJ06dJUq9VDLgAAh+KiDwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKIUwAgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKIYwAQCKcUq9B+AEceBA8qMfJTt3JuPGJe96VzJiRL2nAqDBCBOO3be+lVx/ffKrX/3vuvHjky9+MXn/++s3FwANx0s5HJtvfSu54oqDoyRJuruTP/uz/tsBYJCECUfvwIHk4x8/9G3Vav8/b7ihfzsAGARhwtH7539Odu9+49ur1aSrq//aEwAYBGHC0enrS7773cFtu3NnbWcB4IQhTDg6O3YkL744uG3HjavtLACcMIQJR2ffvqS9PTnjjMNvd845/W8dBoBBECYcndbWZNSoZP78w2/32c/6PBMABk2YcHQmTEimTk1GjkwWL+4Pld81alSyZEnykY/UZz4AGpIPWOPoNDcnl1/e/66bvXuTZcuSF17o//ff/CaZNq3/Q9eatS8AgydMOHrTpiXXXZds2JBs3Zr09iZjx/avX7So/58AMATChGMzbVry1rf2v0tn377+l3QmTHCmBICjIkw4ds3NyaRJ9Z4CgBOAv9YCAMUQJgBAMYYlTHp7ezNz5sw0NTXlZz/72XDsEgBoQMMSJp/61Kdy3nnnDceuAIAGVvMweeCBB/KDH/wgt9xyS613BQA0uJq+K2f37t1Zvnx57rvvvpx22mlH3L63tze9vb0DP/f09NRyPACgMDU7Y1KtVrN06dJcffXVmT179qDu09nZmfb29oGlo6OjVuMBAAUacpisXLkyTU1Nh122bt2aW2+9Nfv27cuqVasG/dirVq1KpVIZWLq6uoY6HgDQwJqq1Wp1KHfYs2dPnn/++cNuM2XKlHzgAx/Iv//7v6epqWlg/YEDBzJixIgsWbIkX//614+4r56enrS3t6dSqaStrW0oYwIAdXIsz99DDpPB2rFjx0HXiDz77LNZuHBhvvnNb2bu3LkZP378ER9DmABA4zmW5++aXfw6YcKEg34+44wzkiQXXHDBoKIEADj5+ORXAKAYw/YlfpMmTUqNXjUCAE4QzpgAAMUQJgBAMYQJAFAMYQIAFEOYAADFECYAQDGECQBQDGECABRDmAAAxRAmAEAxhAkAUAxhAgAUQ5gAAMUQJgBAMYQJAFAMYQIAFEOYAADFECYAQDGECQBQDGECABRDmAAAxRAmAEAxhAkAUAxhAgAUQ5gAAMUQJgBAMYQJAFAMYQIAFEOYAADFECYAQDGECQBQDGECABRDmAAAxRAmAEAxhAkAUAxhAgAUQ5gAAMUQJgBAMYQJAFAMYQIAFEOYAADFECYAQDGECQBQDGECABRDmAAAxRAmAEAxhAkAUAxhAgAUQ5gAAMUQJgBAMYQJAFCMmobJd77zncydOzejRo3K6NGjs2jRolruDgBocKfU6oHvvffeLF++PDfffHP+5E/+JK+99lqefPLJWu0OADgB1CRMXnvttVx//fVZs2ZNrrrqqoH106dPP+z9ent709vbO/BzpVJJkvT09NRiTACgBn77vF2tVod835qEyebNm9Pd3Z3m5uZceOGF2bVrV2bOnJk1a9ZkxowZb3i/zs7O/PVf//Xr1nd0dNRiTACghp5//vm0t7cP6T5N1aPJmSO466678ud//ueZMGFC/uEf/iGTJk3KF77whfzgBz/IU089lTPPPPOQ9/u/Z0x+/etfZ+LEidmxY8eQ/2AcrKenJx0dHenq6kpbW1u9x2lojuXx4TgeP47l8eNYHh+VSiUTJkzIiy++mDe/+c1Duu+QzpisXLkyf/d3f3fYbbZs2ZK+vr4kyac//elcccUVSZL169dn/Pjxueeee/LRj370kPdtaWlJS0vL69a3t7f7BTlO2traHMvjxLE8PhzH48exPH4cy+OjuXno77EZUph88pOfzNKlSw+7zZQpU7Jz584kB19T0tLSkilTpmTHjh1DHhIAODkMKUzGjh2bsWPHHnG7WbNmpaWlJdu2bcsll1ySJHn11VfzzDPPZOLEiUc3KQBwwqvJxa9tbW25+uqrs3r16nR0dGTixIlZs2ZNkmTx4sWDfpyWlpasXr36kC/vMDSO5fHjWB4fjuPx41geP47l8XEsx7EmF78m/WdIVq1alW984xt56aWXMnfu3Kxduza///u/X4vdAQAngJqFCQDAUPmuHACgGMIEACiGMAEAiiFMAIBiNFSYfOc738ncuXMzatSojB49OosWLar3SA2tt7c3M2fOTFNTU372s5/Ve5yG88wzz+Sqq67K5MmTM2rUqFxwwQVZvXp1XnnllXqP1hBuu+22TJo0KSNHjszcuXPz2GOP1XukhtPZ2Zk5c+aktbU1Z599dhYtWpRt27bVe6yG9/nPfz5NTU254YYb6j1KQ+ru7s6VV16ZMWPGZNSoUXnb296Wn/zkJ4O+f8OEyb333psPfehDWbZsWf7rv/4rP/7xj/MXf/EX9R6roX3qU5/KeeedV+8xGtbWrVvT19eXdevW5Re/+EX+8R//MV/5ylfyl3/5l/UerXh33313brzxxqxevTqbN2/OO97xjixcuDDPPfdcvUdrKJs2bcqKFSvy6KOPZuPGjXn11Vfznve8J/v376/3aA3r8ccfz7p16/L2t7+93qM0pBdffDHz5s3Lm970pjzwwAP55S9/mS984QsZPXr04B+k2gBeffXV6vnnn1/9p3/6p3qPcsL47ne/W506dWr1F7/4RTVJ9ac//Wm9Rzoh/P3f/3118uTJ9R6jeBdffHF1xYoVAz8fOHCget5551U7OzvrOFXje+6556pJqps2bar3KA1p37591be85S3VjRs3Vv/4j/+4ev3119d7pIZz0003VS+55JJjeoyGOGOyefPmdHd3p7m5ORdeeGHGjRuX9773vXnyySfrPVpD2r17d5YvX55vfOMbOe200+o9zgmlUqm84bdn0++VV17JE088kQULFgysa25uzoIFC/LII4/UcbLGV6lUksTv4FFasWJF3ve+9x30u8nQ/Nu//Vtmz56dxYsX5+yzz86FF16Yr371q0N6jIYIk6effjpJ8tnPfjZ/9Vd/lfvvvz+jR4/O/Pnz88ILL9R5usZSrVazdOnSXH311Zk9e3a9xzmhbN++Pbfeeusbfns2/fbu3ZsDBw7knHPOOWj9Oeeck127dtVpqsbX19eXG264IfPmzcuMGTPqPU7Dueuuu7J58+Z0dnbWe5SG9vTTT+f222/PW97ylnz/+9/Pxz72sVx33XX5+te/PujHqGuYrFy5Mk1NTYddfvs6fpJ8+tOfzhVXXJFZs2Zl/fr1aWpqyj333FPPP0IxBnssb7311uzbty+rVq2q98jFGuyx/F3d3d257LLLsnjx4ixfvrxOk3MyW7FiRZ588sncdddd9R6l4XR1deX666/Pv/zLv2TkyJH1Hqeh9fX15aKLLsrNN9+cCy+8MB/5yEeyfPnyfOUrXxn0Y9TkS/wG65Of/GSWLl162G2mTJmSnTt3JkmmT58+sL6lpSVTpkzJjh07ajliwxjssfzhD3+YRx555HVfrDR79uwsWbJkSFV7ohrssfytZ599Npdeemne+c535o477qjxdI3vrLPOyogRI7J79+6D1u/evTvnnntunaZqbNdcc03uv//+PPzwwxk/fny9x2k4TzzxRJ577rlcdNFFA+sOHDiQhx9+OF/+8pfT29ubESNG1HHCxjFu3LiDnquTZNq0abn33nsH/Rh1DZOxY8dm7NixR9xu1qxZaWlpybZt23LJJZck6f+SwGeeeSYTJ06s9ZgNYbDH8ktf+lI+97nPDfz87LPPZuHChbn77rszd+7cWo7YMAZ7LJP+MyWXXnrpwFm85uaGeHW0rk499dTMmjUrDz744MBb/vv6+vLggw/mmmuuqe9wDaZarebaa6/Nhg0b8tBDD2Xy5Mn1Hqkhvfvd787Pf/7zg9YtW7YsU6dOzU033SRKhmDevHmve8v6U089NaTn6rqGyWC1tbXl6quvzurVq9PR0ZGJEydmzZo1SZLFixfXebrGMmHChIN+PuOMM5IkF1xwgb9pDVF3d3fmz5+fiRMn5pZbbsmePXsGbvM3/8O78cYb8+EPfzizZ8/OxRdfnLVr12b//v1ZtmxZvUdrKCtWrMidd96Zb3/722ltbR24Rqe9vT2jRo2q83SNo7W19XXX5Zx++ukZM2aM63WG6BOf+ETe+c535uabb84HPvCBPPbYY7njjjuGdDa5IcIkSdasWZNTTjklH/rQh/LSSy9l7ty5+eEPfzi090bDcbRx48Zs374927dvf13UVX1p92F98IMfzJ49e/KZz3wmu3btysyZM/O9733vdRfEcni33357kmT+/PkHrV+/fv0RX46EWpgzZ042bNiQVatW5W/+5m8yefLkrF27NkuWLBn0YzRV/R8UACiEF8QBgGIIEwCgGMIEACiGMAEAiiFMAIBiCBMAoBjCBAAohjABAIohTACAYggTAKAYwgQAKMb/B5VG49sSXHYKAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1200 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}